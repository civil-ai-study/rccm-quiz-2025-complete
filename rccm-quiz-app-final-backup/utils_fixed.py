"""
RCCMå­¦ç¿’ã‚¢ãƒ—ãƒª - ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆ & é«˜æ€§èƒ½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
Render.com ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå¯¾å¿œç‰ˆ
"""

import csv
import os
import logging
import threading
import time
import hashlib
import functools
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Callable, Tuple
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('rccm_app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  ===

class LRUCache:
    """
    ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã‚¢ã‚¯ã‚»ã‚¹é€Ÿåº¦ã‚’ä¸¡ç«‹
    """
    
    def __init__(self, maxsize: int = 100, ttl: int = 3600):
        self.maxsize = maxsize
        self.ttl = ttl  # Time-To-Live (ç§’)
        self.cache = OrderedDict()
        self.timestamps = {}
        self.access_count = {}
        self.hit_count = 0
        self.miss_count = 0
        self.lock = threading.RLock()
        
    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key not in self.cache:
                self.miss_count += 1
                return None
            
            # TTLãƒã‚§ãƒƒã‚¯
            if self._is_expired(key):
                del self.cache[key]
                del self.timestamps[key]
                if key in self.access_count:
                    del self.access_count[key]
                self.miss_count += 1
                return None
            
            # LRUæ›´æ–°
            value = self.cache.pop(key)
            self.cache[key] = value
            self.access_count[key] = self.access_count.get(key, 0) + 1
            self.hit_count += 1
            
            return value
    
    def put(self, key: str, value: Any) -> None:
        with self.lock:
            if key in self.cache:
                self.cache.pop(key)
            elif len(self.cache) >= self.maxsize:
                # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
                oldest_key = next(iter(self.cache))
                del self.cache[oldest_key]
                del self.timestamps[oldest_key]
                if oldest_key in self.access_count:
                    del self.access_count[oldest_key]
            
            self.cache[key] = value
            self.timestamps[key] = time.time()
            self.access_count[key] = 0
    
    def _is_expired(self, key: str) -> bool:
        if key not in self.timestamps:
            return True
        return time.time() - self.timestamps[key] > self.ttl
    
    def clear(self) -> None:
        with self.lock:
            self.cache.clear()
            self.timestamps.clear()
            self.access_count.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def stats(self) -> Dict[str, Any]:
        with self.lock:
            total_requests = self.hit_count + self.miss_count
            hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
            
            return {
                'size': len(self.cache),
                'maxsize': self.maxsize,
                'hit_count': self.hit_count,
                'miss_count': self.miss_count,
                'hit_rate': hit_rate,
                'total_requests': total_requests,
                'most_accessed': sorted(
                    self.access_count.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )[:5]
            }

class CacheManager:
    """
    çµ±åˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    è¤‡æ•°ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç®¡ç†
    """
    
    def __init__(self):
        # ä¼æ¥­ç’°å¢ƒç”¨ã«æ‹¡å¼µã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.caches = {
            'questions': LRUCache(maxsize=50, ttl=7200),  # å•é¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆä¼æ¥­ç”¨æ‹¡å¼µï¼‰
            'validation': LRUCache(maxsize=100, ttl=3600),  # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ
            'csv_parsing': LRUCache(maxsize=50, ttl=7200),  # CSVè§£æçµæœ
            'file_metadata': LRUCache(maxsize=200, ttl=600),  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            'department_mapping': LRUCache(maxsize=500, ttl=14400),  # éƒ¨é–€ãƒãƒƒãƒ”ãƒ³ã‚°
            'user_sessions': LRUCache(maxsize=1000, ttl=1800),  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³
            'question_filters': LRUCache(maxsize=200, ttl=3600),  # å•é¡Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            'aggregated_stats': LRUCache(maxsize=100, ttl=900),  # é›†è¨ˆçµ±è¨ˆ
        }
        self.background_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix='cache_bg')
        
    def get_cache(self, cache_name: str) -> LRUCache:
        return self.caches.get(cache_name)
    
    def clear_all(self) -> None:
        for cache in self.caches.values():
            cache.clear()
        logger.info("å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def get_stats(self) -> Dict[str, Any]:
        stats = {}
        for name, cache in self.caches.items():
            stats[name] = cache.stats()
        return stats
    
    def log_stats(self) -> None:
        stats = self.get_stats()
        logger.info("=== ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ ===")
        for cache_name, cache_stats in stats.items():
            logger.info(f"{cache_name}: ã‚µã‚¤ã‚º={cache_stats['size']}/{cache_stats['maxsize']}, "
                       f"ãƒ’ãƒƒãƒˆç‡={cache_stats['hit_rate']:.2%}, "
                       f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ={cache_stats['total_requests']}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
cache_manager = CacheManager()

def cache_result(cache_name: str, ttl: Optional[int] = None):
    """
    é–¢æ•°çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            key_data = str(args) + str(sorted(kwargs.items()))
            cache_key = hashlib.md5(key_data.encode()).hexdigest()
            
            cache = cache_manager.get_cache(cache_name)
            if cache is None:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç›´æ¥å®Ÿè¡Œ
                return func(*args, **kwargs)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
            result = cache.get(cache_key)
            if result is not None:
                logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {func.__name__}")
                return result
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: é–¢æ•°å®Ÿè¡Œ
            logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {func.__name__}")
            result = func(*args, **kwargs)
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            cache.put(cache_key, result)
            
            return result
        return wrapper
    return decorator

def get_file_hash(filepath: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    if not os.path.exists(filepath):
        return ""
    
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def is_file_modified(filepath: str, cached_hash: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    current_hash = get_file_hash(filepath)
    return current_hash != cached_hash

class DataLoadError(Exception):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å°‚ç”¨ã‚¨ãƒ©ãƒ¼"""
    pass

class DataValidationError(Exception):
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å°‚ç”¨ã‚¨ãƒ©ãƒ¼"""
    pass

@cache_result('questions', ttl=3600)
def load_questions_improved(csv_path: str) -> List[Dict]:
    """
    æ”¹å–„ç‰ˆå•é¡Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨è©³ç´°ãƒ­ã‚°
    """
    logger.info(f"å•é¡Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {csv_path}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not os.path.exists(csv_path):
        logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        raise FileNotFoundError(f"å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
    file_size = os.path.getsize(csv_path)
    if file_size == 0:
        logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {csv_path}")
        raise DataLoadError("å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
    
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size} bytes")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ç”¨ï¼‰
    file_hash = get_file_hash(csv_path)
    cache_key = f"{csv_path}_{file_hash}"
    
    # CSVãƒ‘ãƒ¼ã‚¹çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºèª
    csv_cache = cache_manager.get_cache('csv_parsing')
    cached_df = csv_cache.get(cache_key) if csv_cache else None
    
    if cached_df is not None:
        logger.debug(f"CSVè§£æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {csv_path}")
        df, used_encoding = cached_df
    else:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ¥èª­ã¿è¾¼ã¿è©¦è¡Œ
        encodings = ['utf-8', 'shift_jis', 'cp932', 'utf-8-sig', 'iso-2022-jp']
        df = None
        used_encoding = None
        
        for encoding in encodings:
            try:
                logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è©¦è¡Œ: {encoding}")
                with open(csv_path, 'r', encoding=encoding, newline='') as f:
                    reader = csv.DictReader(f)
                    rows = list(reader)
                    if not rows:
                        logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        raise DataLoadError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    df = rows
                    used_encoding = encoding
                    logger.info(f"èª­ã¿è¾¼ã¿æˆåŠŸ: {encoding} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
                    break
            except UnicodeDecodeError as e:
                logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ {encoding}: {e}")
                continue
            except Exception as e:
                logger.error(f"CSVè§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # è§£æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if df is not None and csv_cache:
            csv_cache.put(cache_key, (df, used_encoding))
    
    if df is None:
        logger.error("ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        raise DataLoadError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç‰¹å®šã§ãã¾ã›ã‚“")
    
    if not df:
        logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        raise DataLoadError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {used_encoding}")
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ æ¤œè¨¼
    required_columns = [
        'id', 'category', 'question', 'option_a', 'option_b', 
        'option_c', 'option_d', 'correct_answer'
    ]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€åˆã®è¡Œã‹ã‚‰ã‚«ãƒ©ãƒ ã‚’å–å¾—
    if df:
        columns = list(df[0].keys())
    else:
        columns = []
    
    missing_columns = [col for col in required_columns if col not in columns]
    if missing_columns:
        error_msg = f"å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}"
        logger.error(error_msg)
        raise DataValidationError(error_msg)
    
    # ãƒ‡ãƒ¼ã‚¿å†…å®¹æ¤œè¨¼
    valid_questions = []
    validation_errors = []
    
    for index, row in enumerate(df):
        try:
            validated_question = validate_question_data(row, index)
            if validated_question:
                valid_questions.append(validated_question)
        except DataValidationError as e:
            validation_errors.append(f"è¡Œ{index + 2}: {e}")
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ è¡Œ{index + 2}: {e}")
    
    if not valid_questions:
        error_msg = "æœ‰åŠ¹ãªå•é¡Œãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
        logger.error(error_msg)
        if validation_errors:
            error_msg += f"\næ¤œè¨¼ã‚¨ãƒ©ãƒ¼:\n" + "\n".join(validation_errors[:5])
        raise DataValidationError(error_msg)
    
    if validation_errors:
        logger.warning(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã®ã‚ã‚‹è¡Œ: {len(validation_errors)}ä»¶")
        # æœ€åˆã®5ä»¶ã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
        for error in validation_errors[:5]:
            logger.warning(error)
    
    logger.info(f"æœ‰åŠ¹ãªå•é¡Œãƒ‡ãƒ¼ã‚¿: {len(valid_questions)}å•")
    return valid_questions

def validate_question_data(row: Dict[str, Any], index: int) -> Optional[Dict]:
    """
    å€‹åˆ¥å•é¡Œãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    """
    # IDæ¤œè¨¼
    if not row.get('id') or row.get('id') == '':
        raise DataValidationError("IDãŒç©ºã§ã™")
    
    try:
        question_id = int(float(row['id']))
    except (ValueError, TypeError):
        raise DataValidationError(f"IDãŒæ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {row.get('id')}")
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œè¨¼
    if not row.get('question') or row.get('question') == '':
        raise DataValidationError("å•é¡Œæ–‡ãŒç©ºã§ã™")
    
    if not row.get('correct_answer') or row.get('correct_answer') == '':
        raise DataValidationError("æ­£è§£ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # æ­£è§£é¸æŠè‚¢æ¤œè¨¼
    correct_answer = str(row['correct_answer']).strip().upper()
    if correct_answer not in ['A', 'B', 'C', 'D']:
        raise DataValidationError(f"æ­£è§£é¸æŠè‚¢ãŒç„¡åŠ¹: {correct_answer}")
    
    # é¸æŠè‚¢å­˜åœ¨ç¢ºèª
    options = {
        'A': row.get('option_a'),
        'B': row.get('option_b'),
        'C': row.get('option_c'),
        'D': row.get('option_d')
    }
    
    for option_key, option_text in options.items():
        if not option_text or option_text == '':
            raise DataValidationError(f"é¸æŠè‚¢{option_key}ãŒç©ºã§ã™")
    
    # æ­£è§£é¸æŠè‚¢ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    correct_option = options.get(correct_answer)
    if not correct_option or correct_option == '':
        raise DataValidationError(f"æ­£è§£é¸æŠè‚¢{correct_answer}ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
    question_data = {
        'id': question_id,
        'category': str(row.get('category', '')).strip(),
        'question': str(row['question']).strip(),
        'option_a': str(row['option_a']).strip(),
        'option_b': str(row['option_b']).strip(),
        'option_c': str(row['option_c']).strip(),
        'option_d': str(row['option_d']).strip(),
        'correct_answer': correct_answer,
        'explanation': str(row.get('explanation', '')).strip(),
        'reference': str(row.get('reference', '')).strip(),
        'difficulty': str(row.get('difficulty', 'æ¨™æº–')).strip(),
        'keywords': str(row.get('keywords', '')).strip(),
        'practical_tip': str(row.get('practical_tip', '')).strip()
    }
    
    return question_data

def load_rccm_data_files(data_dir: str) -> List[Dict]:
    """
    RCCMå°‚ç”¨ï¼š4-1åŸºç¤ãƒ»4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆèª­ã¿è¾¼ã¿
    ===== é‡è¦ï¼šã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªãå®Ÿéš›ã®3,883å•ã‚’ä½¿ç”¨ =====
    """
    global _data_already_loaded, _data_load_lock
    
    # é‡è¤‡èª­ã¿è¾¼ã¿é˜²æ­¢ãƒã‚§ãƒƒã‚¯
    with _data_load_lock:
        if _data_already_loaded:
            logger.info("ğŸš€ ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: ãƒ‡ãƒ¼ã‚¿æ—¢èª­ã¿è¾¼ã¿æ¸ˆã¿ - ã‚¹ã‚­ãƒƒãƒ—")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            if hasattr(cache_manager_instance, '_global_questions_cache'):
                return cache_manager_instance._global_questions_cache
            else:
                logger.warning("âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - èª­ã¿è¾¼ã¿ç¶šè¡Œ")
    
    logger.info(f"ğŸš€ RCCMçµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_dir}")
    
    # ğŸ” Render.com ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    logger.info(f"ğŸ” ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
    logger.info(f"ğŸ” çµ¶å¯¾ãƒ‘ã‚¹ç¢ºèª: {os.path.abspath(data_dir)}")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨ç¢ºèª
    if not os.path.exists(data_dir):
        error_msg = f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {data_dir}"
        logger.error(error_msg)
        raise DataLoadError(error_msg)
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹è¡¨ç¤º
    try:
        files_in_dir = os.listdir(data_dir)
        logger.info(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ ({len(files_in_dir)}ãƒ•ã‚¡ã‚¤ãƒ«): {files_in_dir}")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        csv_files = [f for f in files_in_dir if f.endswith('.csv')]
        logger.info(f"ğŸ“Š CSV ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ ({len(csv_files)}ãƒ•ã‚¡ã‚¤ãƒ«): {csv_files}")
        
    except Exception as e:
        error_msg = f"âŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªèª­ã¿å–ã‚Šå¤±æ•—: {e}"
        logger.error(error_msg)
        raise DataLoadError(error_msg)
    
    all_questions = []
    file_count = 0
    failed_files = []
    
    # 4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    basic_file = os.path.join(data_dir, '4-1.csv')
    logger.info(f"ğŸ” 4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: {basic_file}")
    
    if os.path.exists(basic_file):
        try:
            logger.info(f"ğŸ“– 4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
            basic_questions = load_questions_improved(basic_file)
            for q in basic_questions:
                q['question_type'] = 'basic'
                q['department'] = 'common'  # åŸºç¤ç§‘ç›®ã¯å…±é€š
                q['category'] = 'å…±é€š'  # ã‚«ãƒ†ã‚´ãƒªã‚‚çµ±ä¸€
                # åŸºç¤ç§‘ç›®ã«ã¯å¹´åº¦æƒ…å ±ã‚’è¨­å®šã—ãªã„ï¼ˆå¹´åº¦ä¸å•ï¼‰
                q['year'] = None
            all_questions.extend(basic_questions)
            file_count += 1
            logger.info(f"âœ… 4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(basic_questions)}å•")
        except Exception as e:
            error_msg = f"âŒ 4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {e}"
            logger.error(error_msg)
            logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {basic_file}")
            logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨: {os.path.exists(basic_file)}")
            if os.path.exists(basic_file):
                try:
                    file_stat = os.stat(basic_file)
                    logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_stat.st_size} bytes")
                    logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™: {oct(file_stat.st_mode)}")
                    logger.error(f"ğŸ” æœ€çµ‚æ›´æ–°: {datetime.fromtimestamp(file_stat.st_mtime)}")
                except Exception as stat_error:
                    logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {stat_error}")
            failed_files.append(f"4-1.csv: {e}")
    else:
        error_msg = f"âŒ 4-1.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {basic_file}"
        logger.error(error_msg)
        failed_files.append("4-1.csv: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
    
    # 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå¹´åº¦åˆ¥ï¼‰
    specialist_years = []
    target_years = list(range(2008, 2019))  # 2008-2018å¹´
    logger.info(f"ğŸ” 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿å¯¾è±¡å¹´åº¦: {target_years}")
    
    for year in target_years:
        specialist_file = os.path.join(data_dir, f'4-2_{year}.csv')
        logger.info(f"ğŸ” å°‚é–€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª: 4-2_{year}.csv")
        
        if os.path.exists(specialist_file):
            try:
                logger.info(f"ğŸ“– 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿{year}å¹´èª­ã¿è¾¼ã¿é–‹å§‹...")
                year_questions = load_questions_improved(specialist_file)
                for q in year_questions:
                    q['question_type'] = 'specialist'
                    q['year'] = year
                    # ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰éƒ¨é–€ã‚’æ¨å®š
                    q['department'] = map_category_to_department(q.get('category', ''))
                    # å°‚é–€ç§‘ç›®ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¢ºã«æ¨™è¨˜
                    if not q.get('category'):
                        q['category'] = 'å°‚é–€ç§‘ç›®'
                
                all_questions.extend(year_questions)
                specialist_years.append(year)
                file_count += 1
                logger.info(f"âœ… 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿{year}å¹´èª­ã¿è¾¼ã¿å®Œäº†: {len(year_questions)}å•")
            except Exception as e:
                error_msg = f"âŒ 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿{year}å¹´èª­ã¿è¾¼ã¿å¤±æ•—: {e}"
                logger.error(error_msg)
                logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {specialist_file}")
                if os.path.exists(specialist_file):
                    try:
                        file_stat = os.stat(specialist_file)
                        logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_stat.st_size} bytes")
                        logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™: {oct(file_stat.st_mode)}")
                        logger.error(f"ğŸ” æœ€çµ‚æ›´æ–°: {datetime.fromtimestamp(file_stat.st_mtime)}")
                    except Exception as stat_error:
                        logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {stat_error}")
                else:
                    logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {specialist_file}")
                failed_files.append(f"4-2_{year}.csv: {e}")
        else:
            logger.warning(f"âš ï¸ 4-2_{year}.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {specialist_file}")
    
    # IDã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»èª¿æ•´
    if all_questions:
        all_questions = resolve_id_conflicts(all_questions)
    
    # ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ•ãƒ©ã‚°ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    with _data_load_lock:
        _data_already_loaded = True
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        cache_manager_instance._global_questions_cache = all_questions
        logger.info("ğŸš€ ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† - æ¬¡å›èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–")
    
    # ğŸ” æœ€çµ‚çµæœã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    logger.info(f"ğŸ“Š RCCMçµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
    logger.info(f"   âœ… æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
    logger.info(f"   ğŸ“ˆ ç·å•é¡Œæ•°: {len(all_questions)}å•")
    logger.info(f"   ğŸ—“ï¸ 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿å¯¾è±¡å¹´åº¦: {specialist_years}")
    
    if failed_files:
        logger.warning(f"âš ï¸ èª­ã¿è¾¼ã¿å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ« ({len(failed_files)}ä»¶):")
        for failed in failed_files:
            logger.warning(f"   âŒ {failed}")
    
    # è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ï¼šå•é¡Œãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆ
    if len(all_questions) == 0:
        error_details = [
            "âŒâŒâŒ è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼: å…¨ã¦ã®CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãŒå¤±æ•—ã—ã¾ã—ãŸ",
            f"å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {data_dir}",
            f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹: {files_in_dir}",
            f"å¤±æ•—ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°:",
        ]
        error_details.extend([f"  - {failed}" for failed in failed_files])
        
        full_error_msg = "\n".join(error_details)
        logger.error(full_error_msg)
        raise DataLoadError(f"3,883å•ã®RCCMå•é¡Œãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{full_error_msg}")
    
    logger.info(f"ğŸ‰ RCCMå•é¡Œé›†ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {len(all_questions)}å•")
    return all_questions

def map_category_to_department(category: str) -> str:
    """
    ã‚«ãƒ†ã‚´ãƒªåã‹ã‚‰é©åˆ‡ãªRCCMéƒ¨é–€IDã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
    """
    category_mapping = {
        # é“è·¯
        'é“è·¯': 'road',
        
        # ãƒˆãƒ³ãƒãƒ«
        'ãƒˆãƒ³ãƒãƒ«': 'tunnel',
        
        # æ²³å·ç ‚é˜²æµ·å²¸ï¼ˆå¹´åº¦ã«ã‚ˆã‚‹è¡¨è¨˜ã®é•ã„ã«å¯¾å¿œï¼‰
        'æ²³å·ã€ç ‚é˜²åŠã³æµ·å²¸ãƒ»æµ·æ´‹': 'civil_planning',
        'æ²³å·ç ‚é˜²æµ·å²¸': 'civil_planning',
        'æ²³å·ç ‚é˜²': 'civil_planning',  # 2008å¹´åº¦
        'æ²³å·ç ‚é˜²æµ·å²¸æµ·æ´‹': 'civil_planning',  # 2010å¹´åº¦
        'æ²³å·ç ‚é˜²åŠã³æµ·å²¸ãƒ»æµ·æ´‹': 'civil_planning',  # 2012å¹´åº¦
        'æ²³å·ã€ç ‚é˜²åŠã³æµ·å²¸ï½¥æµ·æ´‹': 'civil_planning',  # 2013å¹´åº¦
        'æ²³å·ãƒ»ç ‚é˜²åŠã³æµ·å²¸ãƒ»æµ·æ´‹': 'civil_planning',  # 2014å¹´åº¦
        
        # éƒ½å¸‚è¨ˆç”»ï¼ˆå¹´åº¦ã«ã‚ˆã‚‹è¡¨è¨˜ã®é•ã„ã«å¯¾å¿œï¼‰
        'éƒ½å¸‚è¨ˆç”»åŠã³åœ°æ–¹è¨ˆç”»': 'urban_planning',
        'éƒ½å¸‚è¨ˆç”»åœ°æ–¹è¨ˆç”»': 'urban_planning',
        
        # é€ åœ’
        'é€ åœ’': 'landscape',
        
        # å»ºè¨­ç’°å¢ƒ
        'å»ºè¨­ç’°å¢ƒ': 'construction_env',
        
        # é‹¼æ§‹é€ ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆï¼ˆå¹´åº¦ã«ã‚ˆã‚‹è¡¨è¨˜ã®é•ã„ã«å¯¾å¿œï¼‰
        'é‹¼æ§‹é€ åŠã³ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ': 'steel_concrete',
        'é‹¼æ§‹é€ ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ': 'steel_concrete',
        
        # åœŸè³ªåŸºç¤
        'åœŸè³ªåŠã³åŸºç¤': 'soil_foundation',
        
        # æ–½å·¥è¨ˆç”»ï¼ˆå¹´åº¦ã«ã‚ˆã‚‹è¡¨è¨˜ã®é•ã„ã«å¯¾å¿œï¼‰
        'æ–½å·¥è¨ˆç”»': 'construction_planning',
        'æ–½å·¥è¨ˆç”»æ–½å·¥è¨­å‚™ç©ç®—': 'construction_planning',
        
        # ä¸Šæ°´é“
        'ä¸Šæ°´é“åŠã³å·¥æ¥­ç”¨æ°´é“': 'water_supply',
        
        # æ£®æ—åœŸæœ¨
        'æ£®æ—åœŸæœ¨': 'forestry',
        
        # è¾²æ¥­åœŸæœ¨
        'è¾²æ¥­åœŸæœ¨': 'agriculture',
        
        # æœªåˆ†é¡ãƒ»ãã®ä»–
        'æœªåˆ†é¡': 'road',  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆéƒ¨é–€
    }
    
    for key, dept in category_mapping.items():
        if key in category:
            return dept
    
    return 'road'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

def resolve_id_conflicts(questions: List[Dict]) -> List[Dict]:
    """
    IDã®é‡è¤‡ã‚’è§£æ±ºã—ã€ä¸€æ„ã®IDã‚’è¨­å®šï¼ˆå•é¡Œç¨®åˆ¥åˆ¥ã«ç¯„å›²åˆ†ã‘ï¼‰
    åŸºç¤ç§‘ç›®: 1-1000, å°‚é–€ç§‘ç›®: 1001-10000
    """
    used_ids = set()
    resolved_questions = []
    
    # åŸºç¤ç§‘ç›®ã¨å°‚é–€ç§‘ç›®ã‚’åˆ†é›¢ã—ã¦å‡¦ç†
    basic_questions = [q for q in questions if q.get('question_type') == 'basic']
    specialist_questions = [q for q in questions if q.get('question_type') == 'specialist']
    other_questions = [q for q in questions if q.get('question_type') not in ['basic', 'specialist']]
    
    # åŸºç¤ç§‘ç›®ã®IDç¯„å›²: 1-1000
    next_basic_id = 1
    for q in basic_questions:
        original_id = q.get('id')
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        while next_basic_id in used_ids and next_basic_id <= 1000:
            next_basic_id += 1
        
        if next_basic_id > 1000:
            logger.warning("åŸºç¤ç§‘ç›®ã®IDç¯„å›²ã‚’è¶…éã—ã¾ã—ãŸ")
            next_basic_id = 1
            while next_basic_id in used_ids:
                next_basic_id += 1
        
        q['id'] = next_basic_id
        q['original_id'] = original_id
        used_ids.add(next_basic_id)
        resolved_questions.append(q)
        next_basic_id += 1
    
    # å°‚é–€ç§‘ç›®ã®IDç¯„å›²: 1001-10000
    next_specialist_id = 1001
    for q in specialist_questions:
        original_id = q.get('id')
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        while next_specialist_id in used_ids and next_specialist_id <= 10000:
            next_specialist_id += 1
        
        if next_specialist_id > 10000:
            logger.warning("å°‚é–€ç§‘ç›®ã®IDç¯„å›²ã‚’è¶…éã—ã¾ã—ãŸ")
            next_specialist_id = 1001
            while next_specialist_id in used_ids:
                next_specialist_id += 1
        
        q['id'] = next_specialist_id
        q['original_id'] = original_id
        used_ids.add(next_specialist_id)
        resolved_questions.append(q)
        next_specialist_id += 1
    
    # ãã®ä»–ã®å•é¡Œ: 10001ä»¥é™
    next_other_id = 10001
    for q in other_questions:
        original_id = q.get('id')
        
        while next_other_id in used_ids:
            next_other_id += 1
        
        q['id'] = next_other_id
        q['original_id'] = original_id
        used_ids.add(next_other_id)
        resolved_questions.append(q)
        next_other_id += 1
    
    logger.info(f"IDè¡çªè§£æ±º: åŸºç¤={len(basic_questions)}å•, å°‚é–€={len(specialist_questions)}å•, ãã®ä»–={len(other_questions)}å•")
    return resolved_questions

def get_sample_data_improved() -> List[Dict]:
    """
    ===== æ³¨æ„ï¼šã“ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã—ã¾ã›ã‚“ =====
    å®Ÿéš›ã®3,883å•ã®RCCMå•é¡Œãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™
    """
    logger.error("âš ï¸ è­¦å‘Š: ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿é–¢æ•°ãŒå‘¼ã³å‡ºã•ã‚Œã¾ã—ãŸ - å®Ÿéš›ã®CSVãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    
    return [
        {
            'id': 1,
            'category': 'ã‚µãƒ³ãƒ—ãƒ«å•é¡Œï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰',
            'department': 'road',
            'question_type': 'basic',
            'question': 'ã€ã‚µãƒ³ãƒ—ãƒ«ã€‘æ™®é€šãƒãƒ«ãƒˆãƒ©ãƒ³ãƒ‰ã‚»ãƒ¡ãƒ³ãƒˆã®å‡çµæ™‚é–“ã«é–¢ã™ã‚‹è¨˜è¿°ã§æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã¯ã©ã‚Œã‹ã€‚',
            'option_a': 'å§‹ç™ºå‡çµæ™‚é–“ã¯45åˆ†ä»¥ä¸Š',
            'option_b': 'çµ‚çµå‡çµæ™‚é–“ã¯8æ™‚é–“ä»¥å†…',
            'option_c': 'å§‹ç™ºå‡çµæ™‚é–“ã¯60åˆ†ä»¥å†…',
            'option_d': 'çµ‚çµå‡çµæ™‚é–“ã¯12æ™‚é–“ä»¥å†…',
            'correct_answer': 'C',
            'explanation': 'JIS R 5210ã§ã¯æ™®é€šãƒãƒ«ãƒˆãƒ©ãƒ³ãƒ‰ã‚»ãƒ¡ãƒ³ãƒˆã®å§‹ç™ºå‡çµæ™‚é–“ã¯60åˆ†ä»¥å†…ã€çµ‚çµå‡çµæ™‚é–“ã¯10æ™‚é–“ä»¥å†…ã¨è¦å®šã•ã‚Œã¦ã„ã¾ã™ã€‚',
            'reference': 'JIS R 5210',
            'difficulty': 'åŸºæœ¬',
            'keywords': 'ã‚»ãƒ¡ãƒ³ãƒˆ,å‡çµæ™‚é–“,å“è³ªç®¡ç†',
            'practical_tip': 'ç¾å ´ã§ã¯æ°—æ¸©ã‚„æ¹¿åº¦ã«ã‚ˆã£ã¦å‡çµæ™‚é–“ãŒå¤‰åŒ–ã™ã‚‹ãŸã‚ã€å­£ç¯€ã«å¿œã˜ãŸæ–½å·¥è¨ˆç”»ã®èª¿æ•´ãŒå¿…è¦ã§ã™ã€‚'
        }
    ]

# === ä¼æ¥­ç’°å¢ƒç”¨CSVãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ– ===

class EnterpriseDataManager:
    """
    ä¼æ¥­ç’°å¢ƒã§ã®CSVãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–
    å¤§é‡åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ãƒ»é«˜é€Ÿèª­ã¿è¾¼ã¿å¯¾å¿œ
    """
    
    def __init__(self, data_dir: str = 'data', cache_manager: CacheManager = None):
        self.data_dir = data_dir
        self.cache_manager = cache_manager or cache_manager_instance
        self.file_watcher = {}
        self.preload_executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix='preload')
        self.compression_enabled = True
        
    def preload_all_data(self):
        """
        ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ã™ã¹ã¦ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰èª­ã¿è¾¼ã¿
        ä¼æ¥­ç’°å¢ƒã§ã®é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºä¿
        """
        logger.info("CSVãƒ‡ãƒ¼ã‚¿ã®äº‹å‰èª­ã¿è¾¼ã¿é–‹å§‹ï¼ˆä¼æ¥­ç’°å¢ƒæœ€é©åŒ–ï¼‰")
        
        try:
            csv_files = [
                '4-1.csv',  # åŸºç¤ç§‘ç›®
                '4-2_2008.csv', '4-2_2009.csv', '4-2_2010.csv',
                '4-2_2011.csv', '4-2_2012.csv', '4-2_2013.csv',
                '4-2_2014.csv', '4-2_2015.csv', '4-2_2016.csv',
                '4-2_2017.csv', '4-2_2018.csv'
            ]
            
            # ä¸¦åˆ—èª­ã¿è¾¼ã¿ã§é«˜é€ŸåŒ–
            futures = []
            for csv_file in csv_files:
                future = self.preload_executor.submit(self._preload_single_file, csv_file)
                futures.append(future)
            
            # çµæœç¢ºèª
            loaded_count = 0
            for future in futures:
                try:
                    if future.result():
                        loaded_count += 1
                except Exception as e:
                    logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«äº‹å‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            logger.info(f"CSVãƒ‡ãƒ¼ã‚¿äº‹å‰èª­ã¿è¾¼ã¿å®Œäº†: {loaded_count}/{len(csv_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
            return loaded_count == len(csv_files)
            
        except Exception as e:
            logger.error(f"CSVãƒ‡ãƒ¼ã‚¿äº‹å‰èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return False
    
    def _preload_single_file(self, filename: str) -> bool:
        """å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰èª­ã¿è¾¼ã¿"""
        try:
            file_path = os.path.join(self.data_dir, filename)
            if not os.path.exists(file_path):
                return False
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            cache_key = f"csv_preload_{filename}"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¤å®š
            stat = os.stat(file_path)
            metadata_key = f"{cache_key}_metadata"
            current_metadata = f"{stat.st_size}_{stat.st_mtime}"
            
            metadata_cache = self.cache_manager.get_cache('file_metadata')
            cached_metadata = metadata_cache.get(metadata_key)
            
            if cached_metadata == current_metadata:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸€è‡´ã™ã‚‹å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿
                return True
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            data = load_questions_improved(file_path)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            questions_cache = self.cache_manager.get_cache('questions')
            questions_cache.put(cache_key, data)
            metadata_cache.put(metadata_key, current_metadata)
            
            logger.debug(f"äº‹å‰èª­ã¿è¾¼ã¿å®Œäº†: {filename} ({len(data)}å•)")
            return True
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«äº‹å‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {filename}: {e}")
            return False
    
    def get_optimized_data(self, filename: str) -> List[Dict]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å–å¾—
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        """
        cache_key = f"csv_preload_{filename}"
        questions_cache = self.cache_manager.get_cache('questions')
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
        cached_data = questions_cache.get(cache_key)
        if cached_data:
            return cached_data
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èª­ã¿è¾¼ã¿
        logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èª­ã¿è¾¼ã¿: {filename}")
        file_path = os.path.join(self.data_dir, filename)
        data = load_questions_improved(file_path)
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        questions_cache.put(cache_key, data)
        return data
    
    def get_file_integrity_check(self) -> Dict[str, Any]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        ä¼æ¥­ç’°å¢ƒã§ã®ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
        """
        try:
            integrity_report = {
                'timestamp': datetime.now().isoformat(),
                'files': {},
                'total_questions': 0,
                'status': 'healthy'
            }
            
            # å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            for filename in os.listdir(self.data_dir):
                if filename.endswith('.csv') and not filename.endswith(('_backup.csv', '_fixed.csv')):
                    file_path = os.path.join(self.data_dir, filename)
                    
                    try:
                        # ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ±
                        stat = os.stat(file_path)
                        
                        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                        file_path_full = os.path.join(self.data_dir, filename)
                        data = load_questions_improved(file_path_full)
                        
                        integrity_report['files'][filename] = {
                            'size_bytes': stat.st_size,
                            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            'question_count': len(data),
                            'status': 'ok'
                        }
                        integrity_report['total_questions'] += len(data)
                        
                    except Exception as e:
                        integrity_report['files'][filename] = {
                            'status': 'error',
                            'error': str(e)
                        }
                        integrity_report['status'] = 'degraded'
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {integrity_report['total_questions']}å•")
            return integrity_report
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'status': 'error',
                'error': str(e)
            }

# ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: é‡è¤‡èª­ã¿è¾¼ã¿é˜²æ­¢ãƒ•ãƒ©ã‚°
_data_already_loaded = False
_data_load_lock = threading.Lock()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹  
cache_manager_instance = CacheManager()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆä¼æ¥­ç’°å¢ƒç”¨ï¼‰
enterprise_data_manager = EnterpriseDataManager(cache_manager=cache_manager_instance)