#!/usr/bin/env python3
"""
ğŸ¯ Comprehensive Test Execution Script
Execute all 39 test cases (13 departments Ã— 3 question counts) with full validation
"""

import sys
import os
import argparse
import logging
from pathlib import Path

# Add test framework to Python path
sys.path.insert(0, str(Path(__file__).parent / "test_framework"))

try:
    from core.test_runner import ComprehensiveTestRunner
    FRAMEWORK_AVAILABLE = True
except ImportError as e:
    print(f"âŒ Test framework import error: {e}")
    FRAMEWORK_AVAILABLE = False

def setup_logging(level: str = "INFO"):
    """Setup logging configuration"""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s | %(levelname)8s | %(name)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

def main():
    """Main execution function"""
    parser = argparse.ArgumentParser(
        description="RCCM Quiz Comprehensive Test Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python run_comprehensive_tests.py --all                    # Run all 39 test cases
  python run_comprehensive_tests.py --department åŸºç¤ç§‘ç›®      # Test one department
  python run_comprehensive_tests.py --questions 10           # Test 10-question config
  python run_comprehensive_tests.py --parallel --workers 4   # Parallel execution
        """
    )
    
    # Test execution options
    parser.add_argument("--all", action="store_true", 
                       help="Run all 39 test cases (13 departments Ã— 3 question counts)")
    parser.add_argument("--department", type=str, 
                       help="Run tests for specific department")
    parser.add_argument("--questions", type=int, choices=[10, 20, 30],
                       help="Run tests for specific question count")
    parser.add_argument("--parallel", action="store_true",
                       help="Run tests in parallel")
    parser.add_argument("--workers", type=int, default=4,
                       help="Number of parallel workers (default: 4)")
    
    # Output options
    parser.add_argument("--log-level", choices=["DEBUG", "INFO", "WARNING", "ERROR"], 
                       default="INFO", help="Logging level")
    parser.add_argument("--no-reports", action="store_true",
                       help="Skip report generation")
    
    # Validation options
    parser.add_argument("--validate-environment", action="store_true",
                       help="Only validate test environment")
    parser.add_argument("--validate-data", action="store_true",
                       help="Only validate department data")
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)
    
    # Check framework availability
    if not FRAMEWORK_AVAILABLE:
        logger.error("âŒ Test framework not available. Please check imports.")
        return 1
    
    # Print framework banner
    print_banner()
    
    try:
        # Initialize test runner
        logger.info("ğŸš€ Initializing Comprehensive Test Runner...")
        runner = ComprehensiveTestRunner()
        
        # Validation-only modes
        if args.validate_environment:
            logger.info("ğŸ” Validating test environment...")
            if runner._validate_test_environment():
                logger.info("âœ… Test environment validation passed")
                return 0
            else:
                logger.error("âŒ Test environment validation failed")
                return 1
        
        if args.validate_data:
            logger.info("ğŸ” Validating department data...")
            if runner._validate_all_department_data():
                logger.info("âœ… Department data validation passed")
                return 0
            else:
                logger.error("âŒ Department data validation failed")
                return 1
        
        # Test execution modes
        success = False
        
        if args.all:
            logger.info("ğŸ¯ Executing comprehensive test suite (39 test cases)...")
            success = runner.execute_comprehensive_suite(
                parallel=args.parallel, 
                max_workers=args.workers
            )
        elif args.department:
            logger.info(f"ğŸ¢ Executing department suite: {args.department}")
            success = runner.execute_department_suite(args.department)
        elif args.questions:
            logger.info(f"ğŸ”¢ Executing question count suite: {args.questions} questions")
            success = runner.execute_configuration_suite(args.questions)
        else:
            parser.print_help()
            return 1
        
        # Print final results
        if success:
            logger.info("ğŸ‰ All tests completed successfully!")
            print_success_summary(runner.get_test_results())
            return 0
        else:
            logger.error("âŒ Some tests failed!")
            print_failure_summary(runner.get_test_results())
            return 1
            
    except KeyboardInterrupt:
        logger.warning("âš ï¸ Test execution interrupted by user")
        return 130
    except Exception as e:
        logger.error(f"ğŸš¨ Unexpected error: {e}")
        return 1

def print_banner():
    """Print framework banner"""
    banner = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ¯ RCCM Quiz Comprehensive Test Framework                  â•‘
â•‘                                                                              â•‘
â•‘  ğŸ“Š Test Matrix: 13 Departments Ã— 3 Question Counts = 39 Total Test Cases   â•‘
â•‘  ğŸ¢ Departments: åŸºç¤ç§‘ç›® + 12 Specialist Departments                         â•‘
â•‘  ğŸ”¢ Question Counts: 10, 20, 30 questions per session                       â•‘
â•‘  âœ… Validation: Complete flow from start to results                          â•‘
â•‘                                                                              â•‘
â•‘  ğŸ–ï¸ Success Criteria: 100% test case success rate                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)

def print_success_summary(test_results: dict):
    """Print success summary"""
    stats = test_results.get('execution_stats', {})
    
    summary = f"""
ğŸ‰ COMPREHENSIVE TEST EXECUTION COMPLETED SUCCESSFULLY!

ğŸ“Š Test Statistics:
â”œâ”€â”€ Total Tests: {stats.get('total_tests', 0)}
â”œâ”€â”€ Completed: {stats.get('completed_tests', 0)}
â”œâ”€â”€ Passed: {stats.get('passed_tests', 0)} âœ…
â”œâ”€â”€ Failed: {stats.get('failed_tests', 0)} âŒ
â”œâ”€â”€ Errors: {stats.get('error_tests', 0)} ğŸš¨
â””â”€â”€ Success Rate: {(stats.get('passed_tests', 0) / max(stats.get('completed_tests', 1), 1) * 100):.1f}%

ğŸ¯ All quality gates passed! System ready for production.
    """
    print(summary)

def print_failure_summary(test_results: dict):
    """Print failure summary"""
    stats = test_results.get('execution_stats', {})
    
    summary = f"""
âŒ TEST EXECUTION COMPLETED WITH FAILURES

ğŸ“Š Test Statistics:
â”œâ”€â”€ Total Tests: {stats.get('total_tests', 0)}
â”œâ”€â”€ Completed: {stats.get('completed_tests', 0)}
â”œâ”€â”€ Passed: {stats.get('passed_tests', 0)} âœ…
â”œâ”€â”€ Failed: {stats.get('failed_tests', 0)} âŒ
â”œâ”€â”€ Errors: {stats.get('error_tests', 0)} ğŸš¨
â””â”€â”€ Success Rate: {(stats.get('passed_tests', 0) / max(stats.get('completed_tests', 1), 1) * 100):.1f}%

ğŸš¨ Quality gates not met. Review failed tests before proceeding.
    """
    print(summary)

if __name__ == "__main__":
    sys.exit(main())