#!/usr/bin/env python3
"""
æ ¹æœ¬çš„æ‰‹æ³•åˆ·æ–°: ä»–è€…çŸ¥è¦‹ã«åŸºã¥ãçœŸã®è¨ºæ–­ã‚·ã‚¹ãƒ†ãƒ 
å¾“æ¥ã®è¡¨é¢çš„æ¤œè¨¼ã‚’å®Œå…¨å¦å®šã—ã€æ·±å±¤åˆ†æã‚’å®Ÿè¡Œ
"""

import requests
import json
import time
from datetime import datetime
import sys

def deep_professional_analysis():
    """å°‚é–€çš„æ·±å±¤åˆ†æ - ä»–è€…çŸ¥è¦‹æ´»ç”¨ç‰ˆ"""
    
    print("ğŸ”¥ æ ¹æœ¬çš„æ‰‹æ³•åˆ·æ–°: å¾“æ¥æ‰‹æ³•å®Œå…¨å¦å®šç‰ˆ")
    print("=" * 80)
    print("âŒ å¾“æ¥ã®èª¤ã£ãŸæ‰‹æ³•ã‚’å®Œå…¨å¦å®š:")
    print("  - HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã®ã¿ã®åˆ¤å®š")
    print("  - è¡¨é¢çš„ãªã€ŒæˆåŠŸã€å ±å‘Š")
    print("  - æ®µéšçš„æ‰‹æ³•ã¸ã®ç›²ä¿¡")
    print("  - ç‹¬å–„çš„ãªå•é¡Œè§£æ±º")
    print()
    print("âœ… æ–°æ‰‹æ³•: å°‚é–€å®¶çŸ¥è¦‹åŸºç›¤ã®çœŸã®è¨ºæ–­")
    print("  - ã‚µãƒ¼ãƒãƒ¼å†…éƒ¨çŠ¶æ…‹è©³ç´°åˆ†æ")
    print("  - ãƒ¡ãƒ¢ãƒªãƒ»ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ç¢ºèª")
    print("  - ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã®è³ªçš„è©•ä¾¡")
    print("  - å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“æ¤œè¨¼")
    print()
    
    base_url = "https://rccm-quiz-2025.onrender.com"
    
    analysis_results = {
        "timestamp": datetime.now().isoformat(),
        "methodology": "å°‚é–€å®¶çŸ¥è¦‹åŸºç›¤è¨ºæ–­",
        "previous_method_rejected": True,
        "diagnostic_categories": {}
    }
    
    # 1. ã‚µãƒ¼ãƒãƒ¼å¥å…¨æ€§ã®æ·±å±¤è¨ºæ–­
    print("ğŸ” 1. ã‚µãƒ¼ãƒãƒ¼å¥å…¨æ€§ã®æ·±å±¤è¨ºæ–­")
    print("-" * 60)
    
    server_health = analyze_server_health(base_url)
    analysis_results["diagnostic_categories"]["server_health"] = server_health
    
    # 2. å°‚é–€ç§‘ç›®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®å®Ÿæ…‹ç¢ºèª
    print("\\nğŸ” 2. å°‚é–€ç§‘ç›®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®å®Ÿæ…‹ç¢ºèª")
    print("-" * 60)
    
    data_loading_reality = analyze_data_loading_reality(base_url)
    analysis_results["diagnostic_categories"]["data_loading"] = data_loading_reality
    
    # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®è³ªçš„è©•ä¾¡
    print("\\nğŸ” 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®è³ªçš„è©•ä¾¡")
    print("-" * 60)
    
    user_experience_quality = analyze_user_experience_quality(base_url)
    analysis_results["diagnostic_categories"]["user_experience"] = user_experience_quality
    
    # 4. ãƒ¡ãƒ¢ãƒªãƒ»ãƒ—ãƒ­ã‚»ã‚¹è² è·æ¨å®š
    print("\\nğŸ” 4. ãƒ¡ãƒ¢ãƒªãƒ»ãƒ—ãƒ­ã‚»ã‚¹è² è·æ¨å®š")
    print("-" * 60)
    
    resource_analysis = analyze_resource_consumption(base_url)
    analysis_results["diagnostic_categories"]["resource_consumption"] = resource_analysis
    
    # 5. çœŸã®å•é¡Œç‰¹å®šã¨å°‚é–€çš„å¯¾ç­–
    print("\\nğŸ” 5. çœŸã®å•é¡Œç‰¹å®šã¨å°‚é–€çš„å¯¾ç­–")
    print("-" * 60)
    
    root_cause_analysis = identify_true_problems(analysis_results)
    analysis_results["root_cause_analysis"] = root_cause_analysis
    
    # çµæœä¿å­˜
    output_file = f"deep_diagnostic_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, ensure_ascii=False, indent=2)
    
    print(f"\\nğŸ’¾ è©³ç´°è¨ºæ–­çµæœ: {output_file}")
    
    return analysis_results

def analyze_server_health(base_url):
    """ã‚µãƒ¼ãƒãƒ¼å¥å…¨æ€§ã®æ·±å±¤åˆ†æ"""
    
    health_metrics = {
        "response_time_analysis": {},
        "error_pattern_analysis": {},
        "load_capacity_estimation": {}
    }
    
    session = requests.Session()
    
    # è¤‡æ•°å›ã®ã‚¢ã‚¯ã‚»ã‚¹ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†æ
    response_times = []
    error_codes = []
    
    for i in range(5):
        try:
            start_time = time.time()
            response = session.get(f"{base_url}/", timeout=15)
            response_time = time.time() - start_time
            
            response_times.append(response_time)
            error_codes.append(response.status_code)
            
            print(f"  è©¦è¡Œ{i+1}: {response.status_code} ({response_time:.3f}s)")
            
        except Exception as e:
            response_times.append(float('inf'))
            error_codes.append('ERROR')
            print(f"  è©¦è¡Œ{i+1}: ERROR - {e}")
        
        time.sleep(1)
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“åˆ†æ
    valid_times = [t for t in response_times if t != float('inf')]
    if valid_times:
        avg_time = sum(valid_times) / len(valid_times)
        max_time = max(valid_times)
        min_time = min(valid_times)
        
        health_metrics["response_time_analysis"] = {
            "average": avg_time,
            "maximum": max_time,
            "minimum": min_time,
            "stability": max_time - min_time < 2.0,
            "performance_acceptable": avg_time < 3.0
        }
        
        print(f"  ğŸ“Š å¹³å‡å¿œç­”: {avg_time:.3f}s")
        print(f"  ğŸ“Š æœ€å¤§å¿œç­”: {max_time:.3f}s")
        print(f"  ğŸ“Š å®‰å®šæ€§: {'âœ… è‰¯å¥½' if max_time - min_time < 2.0 else 'âŒ ä¸å®‰å®š'}")
    
    # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    error_rate = error_codes.count('ERROR') / len(error_codes)
    success_rate = error_codes.count(200) / len(error_codes)
    
    health_metrics["error_pattern_analysis"] = {
        "error_rate": error_rate,
        "success_rate": success_rate,
        "error_codes": error_codes,
        "server_stable": error_rate < 0.2
    }
    
    print(f"  ğŸ“Š æˆåŠŸç‡: {success_rate*100:.1f}%")
    print(f"  ğŸ“Š ã‚¨ãƒ©ãƒ¼ç‡: {error_rate*100:.1f}%")
    print(f"  ğŸ“Š ã‚µãƒ¼ãƒãƒ¼å®‰å®šæ€§: {'âœ… å®‰å®š' if error_rate < 0.2 else 'âŒ ä¸å®‰å®š'}")
    
    return health_metrics

def analyze_data_loading_reality(base_url):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã®å®Ÿæ…‹åˆ†æ"""
    
    data_analysis = {
        "basic_subject_reality": {},
        "specialist_departments_reality": {},
        "data_availability_check": {}
    }
    
    session = requests.Session()
    
    # åŸºç¤ç§‘ç›®ã®å®Ÿæ…‹ç¢ºèª
    print("  ğŸ“‹ åŸºç¤ç§‘ç›®å®Ÿæ…‹ç¢ºèª...")
    try:
        response = session.post(
            f"{base_url}/start_exam/åŸºç¤ç§‘ç›®",
            data={"questions": "10"},
            allow_redirects=True,
            timeout=15
        )
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã®è©³ç´°åˆ†æ
        content_analysis = analyze_response_content(response)
        
        data_analysis["basic_subject_reality"] = {
            "status_code": response.status_code,
            "final_url": response.url,
            "content_length": len(response.text),
            "contains_question": "å•é¡Œ" in response.text,
            "contains_error": "ã‚¨ãƒ©ãƒ¼" in response.text or "error" in response.text.lower(),
            "content_analysis": content_analysis,
            "truly_functional": content_analysis.get("has_question_content", False)
        }
        
        result = "âœ… æ©Ÿèƒ½çš„" if content_analysis.get("has_question_content", False) else "âŒ æ©Ÿèƒ½ä¸å…¨"
        print(f"    åŸºç¤ç§‘ç›®: {result}")
        
    except Exception as e:
        data_analysis["basic_subject_reality"] = {
            "error": str(e),
            "truly_functional": False
        }
        print(f"    åŸºç¤ç§‘ç›®: âŒ ä¾‹å¤–ã‚¨ãƒ©ãƒ¼")
    
    # å°‚é–€ç§‘ç›®ã®å®Ÿæ…‹ç¢ºèªï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
    test_departments = ["é€ åœ’", "é“è·¯", "éƒ½å¸‚è¨ˆç”»"]
    specialist_results = {}
    
    for dept in test_departments:
        print(f"  ğŸ“‹ {dept}éƒ¨é–€å®Ÿæ…‹ç¢ºèª...")
        try:
            response = session.post(
                f"{base_url}/start_exam/{dept}",
                data={"questions": "10", "year": "2016"},
                allow_redirects=True,
                timeout=15
            )
            
            content_analysis = analyze_response_content(response)
            
            specialist_results[dept] = {
                "status_code": response.status_code,
                "final_url": response.url,
                "content_length": len(response.text),
                "content_analysis": content_analysis,
                "truly_functional": content_analysis.get("has_question_content", False)
            }
            
            result = "âœ… æ©Ÿèƒ½çš„" if content_analysis.get("has_question_content", False) else "âŒ æ©Ÿèƒ½ä¸å…¨"
            print(f"    {dept}: {result}")
            
        except Exception as e:
            specialist_results[dept] = {
                "error": str(e),
                "truly_functional": False
            }
            print(f"    {dept}: âŒ ä¾‹å¤–ã‚¨ãƒ©ãƒ¼")
        
        time.sleep(0.5)
    
    data_analysis["specialist_departments_reality"] = specialist_results
    
    # å…¨ä½“çš„ãªæ©Ÿèƒ½æ€§è©•ä¾¡
    functional_count = sum(1 for result in specialist_results.values() 
                          if result.get("truly_functional", False))
    basic_functional = data_analysis["basic_subject_reality"].get("truly_functional", False)
    
    total_functional = functional_count + (1 if basic_functional else 0)
    total_tested = len(specialist_results) + 1
    
    data_analysis["data_availability_check"] = {
        "functional_departments": total_functional,
        "total_tested": total_tested,
        "true_functionality_rate": total_functional / total_tested,
        "system_truly_working": total_functional / total_tested > 0.8
    }
    
    print(f"  ğŸ“Š çœŸã®æ©Ÿèƒ½ç‡: {total_functional}/{total_tested} ({total_functional/total_tested*100:.1f}%)")
    
    return data_analysis

def analyze_response_content(response):
    """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã®è³ªçš„åˆ†æ"""
    
    content = response.text
    content_lower = content.lower()
    
    analysis = {
        "has_question_content": False,
        "has_navigation_elements": False,
        "has_error_indicators": False,
        "has_form_elements": False,
        "content_type": "unknown"
    }
    
    # å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å­˜åœ¨ç¢ºèª
    question_indicators = ["å•é¡Œ", "é¸æŠè‚¢", "option", "ç­”ãˆ", "å›ç­”"]
    analysis["has_question_content"] = any(indicator in content for indicator in question_indicators)
    
    # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã®ç¢ºèª
    nav_indicators = ["æ¬¡ã®å•é¡Œ", "å‰ã®å•é¡Œ", "çµæœ", "é€²æ—"]
    analysis["has_navigation_elements"] = any(indicator in content for indicator in nav_indicators)
    
    # ã‚¨ãƒ©ãƒ¼æŒ‡æ¨™ã®ç¢ºèª
    error_indicators = ["error", "ã‚¨ãƒ©ãƒ¼", "å¤±æ•—", "not found", "åˆ©ç”¨ã§ãã¾ã›ã‚“"]
    analysis["has_error_indicators"] = any(indicator in content_lower for indicator in error_indicators)
    
    # ãƒ•ã‚©ãƒ¼ãƒ è¦ç´ ã®ç¢ºèª
    form_indicators = ["<form", "<input", "<button", "<select"]
    analysis["has_form_elements"] = any(indicator in content_lower for indicator in form_indicators)
    
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã®æ¨å®š
    if analysis["has_question_content"] and analysis["has_navigation_elements"]:
        analysis["content_type"] = "functional_exam"
    elif analysis["has_error_indicators"]:
        analysis["content_type"] = "error_page"
    elif "exam_simulator" in response.url:
        analysis["content_type"] = "simulator_page"
    else:
        analysis["content_type"] = "unknown"
    
    return analysis

def analyze_user_experience_quality(base_url):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®è³ªçš„è©•ä¾¡"""
    
    ux_analysis = {
        "navigation_flow_quality": {},
        "error_handling_quality": {},
        "response_appropriateness": {}
    }
    
    session = requests.Session()
    
    # ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ­ãƒ¼ã®å“è³ªç¢ºèª
    print("  ğŸ“‹ ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ•ãƒ­ãƒ¼å“è³ªç¢ºèª...")
    
    try:
        # åŸºç¤ç§‘ç›®ãƒ•ãƒ­ãƒ¼ç¢ºèª
        response1 = session.post(f"{base_url}/start_exam/åŸºç¤ç§‘ç›®", 
                                data={"questions": "10"}, allow_redirects=False, timeout=10)
        
        if response1.status_code in [301, 302]:
            redirect_url = response1.headers.get('Location', '')
            response2 = session.get(f"{base_url}{redirect_url}", timeout=10)
            
            flow_quality = {
                "proper_redirect": True,
                "redirect_target": redirect_url,
                "final_response_ok": response2.status_code == 200,
                "content_appropriate": analyze_response_content(response2)["content_type"] == "functional_exam"
            }
        else:
            flow_quality = {
                "proper_redirect": False,
                "direct_response_ok": response1.status_code == 200,
                "content_appropriate": False
            }
        
        ux_analysis["navigation_flow_quality"] = flow_quality
        
        flow_ok = flow_quality.get("content_appropriate", False)
        print(f"    ãƒ•ãƒ­ãƒ¼å“è³ª: {'âœ… è‰¯å¥½' if flow_ok else 'âŒ ä¸è‰¯'}")
        
    except Exception as e:
        ux_analysis["navigation_flow_quality"] = {"error": str(e)}
        print("    ãƒ•ãƒ­ãƒ¼å“è³ª: âŒ ã‚¨ãƒ©ãƒ¼")
    
    return ux_analysis

def analyze_resource_consumption(base_url):
    """ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»çŠ¶æ³æ¨å®š"""
    
    resource_analysis = {
        "server_load_indicators": {},
        "memory_usage_estimation": {},
        "performance_bottlenecks": {}
    }
    
    session = requests.Session()
    
    # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã«ã‚ˆã‚‹è² è·ãƒ†ã‚¹ãƒˆ
    print("  ğŸ“‹ ã‚µãƒ¼ãƒãƒ¼è² è·çŠ¶æ³æ¨å®š...")
    
    load_times = []
    for i in range(3):
        try:
            start_time = time.time()
            response = session.get(f"{base_url}/", timeout=15)
            load_time = time.time() - start_time
            load_times.append(load_time)
            print(f"    è² è·ãƒ†ã‚¹ãƒˆ{i+1}: {load_time:.3f}s")
        except Exception as e:
            load_times.append(float('inf'))
            print(f"    è² è·ãƒ†ã‚¹ãƒˆ{i+1}: ã‚¨ãƒ©ãƒ¼")
        time.sleep(0.5)
    
    valid_times = [t for t in load_times if t != float('inf')]
    if valid_times:
        avg_load_time = sum(valid_times) / len(valid_times)
        load_degradation = max(valid_times) - min(valid_times) > 1.0
        
        resource_analysis["server_load_indicators"] = {
            "average_load_time": avg_load_time,
            "load_degradation_detected": load_degradation,
            "server_overloaded": avg_load_time > 5.0
        }
        
        status = "âŒ éè² è·" if avg_load_time > 5.0 else "âœ… æ­£å¸¸"
        print(f"    ã‚µãƒ¼ãƒãƒ¼è² è·: {status} (å¹³å‡{avg_load_time:.3f}s)")
    
    return resource_analysis

def identify_true_problems(analysis_results):
    """çœŸã®å•é¡Œç‰¹å®š"""
    
    print("  ğŸ“‹ çœŸã®å•é¡Œç‰¹å®šä¸­...")
    
    problems = []
    
    # ã‚µãƒ¼ãƒãƒ¼å¥å…¨æ€§å•é¡Œ
    server_health = analysis_results["diagnostic_categories"]["server_health"]
    if not server_health["response_time_analysis"].get("performance_acceptable", True):
        problems.append("ã‚µãƒ¼ãƒãƒ¼å¿œç­”æ€§èƒ½å•é¡Œ")
    if not server_health["error_pattern_analysis"].get("server_stable", True):
        problems.append("ã‚µãƒ¼ãƒãƒ¼å®‰å®šæ€§å•é¡Œ")
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å•é¡Œ
    data_loading = analysis_results["diagnostic_categories"]["data_loading"]
    if not data_loading["data_availability_check"].get("system_truly_working", False):
        problems.append("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ©Ÿèƒ½ä¸å…¨")
    
    # çœŸã®æˆåŠŸç‡
    true_rate = data_loading["data_availability_check"].get("true_functionality_rate", 0)
    
    root_analysis = {
        "identified_problems": problems,
        "true_functionality_rate": true_rate,
        "system_acceptable": true_rate > 0.9,
        "requires_fundamental_fix": len(problems) > 0,
        "previous_claims_accurate": False  # å‰ã®ã€Œ100%æˆåŠŸã€å ±å‘Šã¯è™šå½
    }
    
    print(f"  ğŸ“Š çœŸã®æ©Ÿèƒ½ç‡: {true_rate*100:.1f}%")
    print(f"  ğŸ” ç‰¹å®šå•é¡Œæ•°: {len(problems)}")
    print(f"  âŒ å‰å›å ±å‘Šã®æ­£ç¢ºæ€§: è™šå½")
    
    for i, problem in enumerate(problems, 1):
        print(f"    {i}. {problem}")
    
    return root_analysis

if __name__ == "__main__":
    deep_professional_analysis()