#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ æœ¬ç•ªç’°å¢ƒHTMLæ§‹é€ åˆ†æ
æœ¬ç•ªç’°å¢ƒã§å®Ÿéš›ã®å•é¡Œå†…å®¹æŠ½å‡ºå¤±æ•—ã®åŸå› èª¿æŸ»

ç›®çš„:
1. å®Ÿéš›ã®HTMLãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’è©³ç´°åˆ†æ
2. å•é¡Œæ–‡ãŒå‹•çš„èª­ã¿è¾¼ã¿ã‹é™çš„è¡¨ç¤ºã‹ã‚’ç¢ºèª
3. é©åˆ‡ãªæŠ½å‡ºæ‰‹æ³•ã‚’ç‰¹å®š

å¯¾è±¡: https://rccm-quiz-2025.onrender.com
"""

import requests
import json
import time
from datetime import datetime
import re
import urllib.parse
from typing import Dict, List, Optional, Tuple
import logging
# BeautifulSoupä½¿ç”¨ãªã—ãƒãƒ¼ã‚¸ãƒ§ãƒ³
import base64

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionHtmlStructureAnalysis:
    def __init__(self):
        self.base_url = "https://rccm-quiz-2025.onrender.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # åˆ†æçµæœ
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'analysis_type': 'PRODUCTION_HTML_STRUCTURE_ANALYSIS',
            'target_url': self.base_url,
            'html_samples': {},
            'extraction_methods_tested': {},
            'dynamic_content_indicators': [],
            'static_content_found': [],
            'recommended_extraction_approach': '',
            'critical_findings': []
        }

    def capture_html_samples(self) -> Dict:
        """æœ¬ç•ªç’°å¢ƒã‹ã‚‰ã®HTMLã‚µãƒ³ãƒ—ãƒ«å–å¾—"""
        
        print("ğŸ” æœ¬ç•ªç’°å¢ƒHTMLæ§‹é€ è§£æé–‹å§‹")
        print("=" * 80)
        
        html_samples = {}
        
        try:
            # 1. ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®HTMLæ§‹é€ 
            print("ğŸ“ Step 1: ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸HTMLæ§‹é€ åˆ†æ")
            home_response = self.session.get(self.base_url, timeout=30)
            
            if home_response.status_code == 200:
                html_samples['homepage'] = {
                    'url': self.base_url,
                    'status_code': home_response.status_code,
                    'content_length': len(home_response.text),
                    'raw_html': home_response.text[:5000],  # æœ€åˆã®5000æ–‡å­—
                    'encoding': home_response.encoding,
                    'headers': dict(home_response.headers)
                }
                print(f"  âœ… ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸å–å¾—æˆåŠŸ (é•·ã•: {len(home_response.text)}æ–‡å­—)")
            
            time.sleep(2)
            
            # 2. åŸºç¤ç§‘ç›®è©¦é¨“ã®HTMLæ§‹é€ 
            print("ğŸ“ Step 2: åŸºç¤ç§‘ç›®è©¦é¨“HTMLæ§‹é€ åˆ†æ")
            basic_url = f"{self.base_url}/start_exam/basic"
            basic_data = {'questions': '10'}
            
            basic_response = self.session.post(basic_url, data=basic_data, timeout=30, allow_redirects=True)
            
            if basic_response.status_code in [200, 302]:
                html_samples['basic_exam'] = {
                    'url': basic_url,
                    'data': basic_data,
                    'status_code': basic_response.status_code,
                    'content_length': len(basic_response.text),
                    'raw_html': basic_response.text[:10000],  # æœ€åˆã®10000æ–‡å­—
                    'full_html': basic_response.text,  # å®Œå…¨ãªHTML
                    'encoding': basic_response.encoding,
                    'headers': dict(basic_response.headers),
                    'redirect_history': [r.url for r in basic_response.history]
                }
                print(f"  âœ… åŸºç¤ç§‘ç›®è©¦é¨“å–å¾—æˆåŠŸ (é•·ã•: {len(basic_response.text)}æ–‡å­—)")
            
            time.sleep(2)
            
            # 3. å°‚é–€ç§‘ç›®è©¦é¨“ã®HTMLæ§‹é€  
            print("ğŸ“ Step 3: å°‚é–€ç§‘ç›®è©¦é¨“HTMLæ§‹é€ åˆ†æ")
            specialist_url = f"{self.base_url}/start_exam/specialist"
            specialist_data = {
                'questions': '10',
                'category': 'å»ºè¨­ç’°å¢ƒ',
                'year': '2019'
            }
            
            specialist_response = self.session.post(specialist_url, data=specialist_data, timeout=30, allow_redirects=True)
            
            if specialist_response.status_code in [200, 302]:
                html_samples['specialist_exam'] = {
                    'url': specialist_url,
                    'data': specialist_data,
                    'status_code': specialist_response.status_code,
                    'content_length': len(specialist_response.text),
                    'raw_html': specialist_response.text[:10000],  # æœ€åˆã®10000æ–‡å­—
                    'full_html': specialist_response.text,  # å®Œå…¨ãªHTML
                    'encoding': specialist_response.encoding,
                    'headers': dict(specialist_response.headers),
                    'redirect_history': [r.url for r in specialist_response.history]
                }
                print(f"  âœ… å°‚é–€ç§‘ç›®è©¦é¨“å–å¾—æˆåŠŸ (é•·ã•: {len(specialist_response.text)}æ–‡å­—)")
            
        except Exception as e:
            print(f"ğŸ’¥ HTMLå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
            html_samples['error'] = str(e)
        
        return html_samples

    def analyze_html_structure_deep(self, html_content: str, content_name: str) -> Dict:
        """HTMLæ§‹é€ ã®è©³ç´°åˆ†æ"""
        
        print(f"ğŸ”¬ {content_name} è©³ç´°æ§‹é€ åˆ†æ")
        
        analysis = {
            'content_name': content_name,
            'basic_stats': {},
            'dom_structure': {},
            'javascript_analysis': {},
            'form_analysis': {},
            'question_content_analysis': {},
            'dynamic_indicators': [],
            'extraction_recommendations': []
        }
        
        try:
            # åŸºæœ¬çµ±è¨ˆ
            analysis['basic_stats'] = {
                'total_length': len(html_content),
                'line_count': html_content.count('\n'),
                'tag_count': html_content.count('<'),
                'script_tags': html_content.count('<script'),
                'form_tags': html_content.count('<form'),
                'div_tags': html_content.count('<div'),
                'encoding_detected': 'utf-8' if 'utf-8' in html_content.lower() else 'unknown'
            }
            
            # æ­£è¦è¡¨ç¾ã§ã®DOMåˆ†æ
            try:
                # HTMLã‚¿ã‚°ã®åŸºæœ¬ã‚«ã‚¦ãƒ³ãƒˆ
                title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
                title = title_match.group(1) if title_match else 'No title'
                
                analysis['dom_structure'] = {
                    'title': title.strip(),
                    'meta_tags': len(re.findall(r'<meta[^>]*>', html_content, re.IGNORECASE)),
                    'script_tags': len(re.findall(r'<script[^>]*>', html_content, re.IGNORECASE)),
                    'form_tags': len(re.findall(r'<form[^>]*>', html_content, re.IGNORECASE)),
                    'div_tags': len(re.findall(r'<div[^>]*>', html_content, re.IGNORECASE)),
                    'p_tags': len(re.findall(r'<p[^>]*>', html_content, re.IGNORECASE)),
                    'span_tags': len(re.findall(r'<span[^>]*>', html_content, re.IGNORECASE)),
                    'input_tags': len(re.findall(r'<input[^>]*>', html_content, re.IGNORECASE)),
                    'button_tags': len(re.findall(r'<button[^>]*>', html_content, re.IGNORECASE))
                }
                
                # JavaScriptã®åˆ†æ
                script_blocks = re.findall(r'<script[^>]*>(.*?)</script>', html_content, re.IGNORECASE | re.DOTALL)
                external_scripts = re.findall(r'<script[^>]*src=', html_content, re.IGNORECASE)
                
                js_analysis = {
                    'script_count': len(script_blocks) + len(external_scripts),
                    'external_scripts': len(external_scripts),
                    'inline_scripts': len(script_blocks),
                    'ajax_indicators': 0,
                    'fetch_indicators': 0,
                    'jquery_usage': False,
                    'dynamic_content_loading': False
                }
                
                for script_content in script_blocks:
                    script_lower = script_content.lower()
                    if 'ajax' in script_lower or 'xmlhttprequest' in script_lower:
                        js_analysis['ajax_indicators'] += 1
                    if 'fetch(' in script_lower:
                        js_analysis['fetch_indicators'] += 1
                    if 'jquery' in script_lower or '$(' in script_lower:
                        js_analysis['jquery_usage'] = True
                    if any(indicator in script_lower for indicator in ['load', 'onload', 'domcontentloaded', 'ready']):
                        js_analysis['dynamic_content_loading'] = True
                
                analysis['javascript_analysis'] = js_analysis
                
                # ãƒ•ã‚©ãƒ¼ãƒ åˆ†æ
                form_matches = re.findall(r'<form[^>]*>(.*?)</form>', html_content, re.IGNORECASE | re.DOTALL)
                form_analysis = {
                    'form_count': len(form_matches),
                    'forms_details': []
                }
                
                for form_content in form_matches:
                    action_match = re.search(r'action=["\']([^"\']*)["\']', form_content, re.IGNORECASE)
                    method_match = re.search(r'method=["\']([^"\']*)["\']', form_content, re.IGNORECASE)
                    
                    form_detail = {
                        'action': action_match.group(1) if action_match else '',
                        'method': method_match.group(1) if method_match else '',
                        'inputs': len(re.findall(r'<input[^>]*>', form_content, re.IGNORECASE)),
                        'buttons': len(re.findall(r'<button[^>]*>', form_content, re.IGNORECASE)),
                        'has_submit': bool(re.search(r'type=["\']submit["\']', form_content, re.IGNORECASE))
                    }
                    form_analysis['forms_details'].append(form_detail)
                
                analysis['form_analysis'] = form_analysis
                
            except Exception as e:
                analysis['dom_structure']['error'] = str(e)
            
            # å•é¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç›´æ¥æ¤œç´¢
            question_patterns = [
                r'å•é¡Œ\s*\d+',
                r'å•\s*\d+',
                r'Question\s*\d+',
                r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]',
                r'é¸æŠè‚¢',
                r'ç­”ãˆ',
                r'è§£ç­”'
            ]
            
            question_content = {
                'pattern_matches': {},
                'visible_text_analysis': {},
                'potential_question_blocks': []
            }
            
            for pattern in question_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                question_content['pattern_matches'][pattern] = len(matches)
            
            # è¦‹ãˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º
            try:
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚¿ã‚°ã‚’é™¤å»
                clean_html = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.IGNORECASE | re.DOTALL)
                clean_html = re.sub(r'<style[^>]*>.*?</style>', '', clean_html, flags=re.IGNORECASE | re.DOTALL)
                
                # HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡º
                visible_text = re.sub(r'<[^>]+>', ' ', clean_html)
                visible_text = re.sub(r'\s+', ' ', visible_text).strip()  # ç©ºç™½ã®æ­£è¦åŒ–
                
                question_content['visible_text_analysis'] = {
                    'total_visible_text_length': len(visible_text),
                    'visible_text_sample': visible_text[:1000],
                    'contains_japanese': bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', visible_text)),
                    'question_keywords_found': []
                }
                
                # å•é¡Œé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢
                question_keywords = ['å•é¡Œ', 'å•', 'é¸æŠ', 'å›ç­”', 'è§£ç­”', 'æ¬¡ã®', 'ã«ã¤ã„ã¦', 'ã§ã‚ã‚‹', 'ã‚‚ã®ã¯']
                for keyword in question_keywords:
                    if keyword in visible_text:
                        question_content['visible_text_analysis']['question_keywords_found'].append(keyword)
                
            except Exception as e:
                question_content['visible_text_analysis']['error'] = str(e)
            
            analysis['question_content_analysis'] = question_content
            
            # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŒ‡æ¨™
            dynamic_indicators = []
            
            if js_analysis.get('ajax_indicators', 0) > 0:
                dynamic_indicators.append('Ajaxå‘¼ã³å‡ºã—ã®å­˜åœ¨')
            if js_analysis.get('fetch_indicators', 0) > 0:
                dynamic_indicators.append('Fetch APIä½¿ç”¨ã®å­˜åœ¨')
            if js_analysis.get('dynamic_content_loading'):
                dynamic_indicators.append('å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª­ã¿è¾¼ã¿ã®å­˜åœ¨')
            if 'onload' in html_content.lower():
                dynamic_indicators.append('onloadã‚¤ãƒ™ãƒ³ãƒˆã®å­˜åœ¨')
            
            analysis['dynamic_indicators'] = dynamic_indicators
            
            # æŠ½å‡ºæ¨å¥¨æ‰‹æ³•
            recommendations = []
            
            if len(dynamic_indicators) > 0:
                recommendations.append('ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶(Selenium/Playwright)ã®ä½¿ç”¨ã‚’æ¨å¥¨')
                recommendations.append('JavaScriptå®Ÿè¡Œå¾Œã®DOMå–å¾—ãŒå¿…è¦')
            else:
                recommendations.append('é™çš„HTMLè§£æã§ååˆ†')
                recommendations.append('BeautifulSoup + æ­£è¦è¡¨ç¾ã§æŠ½å‡ºå¯èƒ½')
            
            if question_content['visible_text_analysis'].get('contains_japanese'):
                recommendations.append('æ—¥æœ¬èªæ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œå¿…é ˆ')
            
            analysis['extraction_recommendations'] = recommendations
            
        except Exception as e:
            analysis['error'] = str(e)
        
        return analysis

    def save_html_samples(self, html_samples: Dict):
        """HTMLã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        
        for sample_name, sample_data in html_samples.items():
            if 'full_html' in sample_data:
                filename = f"production_html_sample_{sample_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
                
                try:
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(sample_data['full_html'])
                    print(f"  ğŸ“„ HTMLã‚µãƒ³ãƒ—ãƒ«ä¿å­˜: {filename}")
                except Exception as e:
                    print(f"  âŒ HTMLã‚µãƒ³ãƒ—ãƒ«ä¿å­˜å¤±æ•— {sample_name}: {str(e)}")

    def run_comprehensive_html_analysis(self):
        """åŒ…æ‹¬çš„HTMLæ§‹é€ åˆ†æã®å®Ÿè¡Œ"""
        
        print("ğŸ”¥ æœ¬ç•ªç’°å¢ƒHTMLæ§‹é€ åˆ†æé–‹å§‹")
        print("=" * 80)
        print("ç›®çš„: å•é¡Œå†…å®¹æŠ½å‡ºå¤±æ•—åŸå› ã®ç‰¹å®š")
        print("å¯¾è±¡: https://rccm-quiz-2025.onrender.com")
        print("=" * 80)
        
        start_time = time.time()
        
        # Step 1: HTMLã‚µãƒ³ãƒ—ãƒ«å–å¾—
        html_samples = self.capture_html_samples()
        self.analysis_results['html_samples'] = html_samples
        
        # Step 2: å„ã‚µãƒ³ãƒ—ãƒ«ã®è©³ç´°åˆ†æ
        extraction_methods = {}
        
        for sample_name, sample_data in html_samples.items():
            if 'full_html' in sample_data:
                print(f"\nğŸ”¬ {sample_name} è©³ç´°åˆ†æ")
                print("-" * 60)
                
                analysis = self.analyze_html_structure_deep(sample_data['full_html'], sample_name)
                extraction_methods[sample_name] = analysis
                
                # é‡è¦ãªç™ºè¦‹äº‹é …ã®è¡¨ç¤º
                if analysis.get('dynamic_indicators'):
                    print(f"  ğŸš¨ å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŒ‡æ¨™: {len(analysis['dynamic_indicators'])}ä»¶")
                    for indicator in analysis['dynamic_indicators']:
                        print(f"    â€¢ {indicator}")
                
                if analysis.get('extraction_recommendations'):
                    print(f"  ğŸ’¡ æŠ½å‡ºæ¨å¥¨æ‰‹æ³•:")
                    for rec in analysis['extraction_recommendations']:
                        print(f"    â€¢ {rec}")
                
                question_analysis = analysis.get('question_content_analysis', {})
                visible_analysis = question_analysis.get('visible_text_analysis', {})
                if visible_analysis.get('question_keywords_found'):
                    print(f"  âœ… å•é¡Œé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç™ºè¦‹: {visible_analysis['question_keywords_found']}")
                else:
                    print(f"  âŒ å•é¡Œé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªç™ºè¦‹")
        
        self.analysis_results['extraction_methods_tested'] = extraction_methods
        
        # Step 3: HTMLã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        print(f"\nğŸ’¾ HTMLã‚µãƒ³ãƒ—ãƒ«ä¿å­˜")
        print("-" * 60)
        self.save_html_samples(html_samples)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Step 4: ç·åˆåˆ¤å®šã¨æ¨å¥¨æ‰‹æ³•
        self.generate_analysis_report(duration)
        
        return self.analysis_results

    def generate_analysis_report(self, duration: float):
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        print("\n" + "=" * 80)
        print("ğŸ”¥ æœ¬ç•ªç’°å¢ƒHTMLæ§‹é€ åˆ†æçµæœ")
        print("=" * 80)
        
        extraction_methods = self.analysis_results['extraction_methods_tested']
        
        print(f"ğŸ“Š åˆ†æçµ±è¨ˆ:")
        print(f"  åˆ†æå¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«: {len(extraction_methods)}")
        print(f"  åˆ†æå®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’")
        print()
        
        # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ¤å®š
        dynamic_content_found = False
        static_content_with_questions = False
        
        all_dynamic_indicators = []
        all_recommendations = []
        
        for sample_name, analysis in extraction_methods.items():
            dynamic_indicators = analysis.get('dynamic_indicators', [])
            if dynamic_indicators:
                dynamic_content_found = True
                all_dynamic_indicators.extend(dynamic_indicators)
            
            recommendations = analysis.get('extraction_recommendations', [])
            all_recommendations.extend(recommendations)
            
            question_analysis = analysis.get('question_content_analysis', {})
            visible_analysis = question_analysis.get('visible_text_analysis', {})
            if visible_analysis.get('question_keywords_found'):
                static_content_with_questions = True
        
        # é‡è¦ãªç™ºè¦‹äº‹é …
        critical_findings = []
        
        if dynamic_content_found:
            critical_findings.append("å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å­˜åœ¨ã‚’ç¢ºèª")
            print("ğŸš¨ é‡è¦ç™ºè¦‹: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            print("  å‹•çš„æŒ‡æ¨™:")
            for indicator in set(all_dynamic_indicators):
                print(f"    â€¢ {indicator}")
        
        if not static_content_with_questions:
            critical_findings.append("é™çš„HTMLã«å•é¡Œæ–‡ãŒå«ã¾ã‚Œã¦ã„ãªã„")
            print("ğŸš¨ é‡è¦ç™ºè¦‹: é™çš„HTMLã«å•é¡Œæ–‡ãŒç¢ºèªã§ãã¾ã›ã‚“")
        else:
            critical_findings.append("é™çš„HTMLã«å•é¡Œé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå­˜åœ¨")
            print("âœ… ç™ºè¦‹: é™çš„HTMLã«å•é¡Œé–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå­˜åœ¨ã—ã¾ã™")
        
        self.analysis_results['critical_findings'] = critical_findings
        
        print()
        
        # æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        if dynamic_content_found:
            recommended_approach = "ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶(Selenium/Playwright)ã«ã‚ˆã‚‹å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—"
            print("ğŸ’¡ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã®ä½¿ç”¨")
            print("  ç†ç”±: JavaScriptå®Ÿè¡Œã«ã‚ˆã‚‹å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„èª­ã¿è¾¼ã¿ãŒå¿…è¦")
        else:
            recommended_approach = "é™çš„HTMLè§£æ(BeautifulSoup + æ­£è¦è¡¨ç¾)"
            print("ğŸ’¡ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: é™çš„HTMLè§£æ")
            print("  ç†ç”±: å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ¤œå‡ºã•ã‚Œã¦ã„ãªã„")
        
        self.analysis_results['recommended_extraction_approach'] = recommended_approach
        
        print()
        
        # æœ€çµ‚åˆ¤å®š
        if dynamic_content_found and not static_content_with_questions:
            print("ğŸ¯ çµè«–: å•é¡Œå†…å®¹ã¯å‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„")
            print("  å¯¾ç­–: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã«ã‚ˆã‚‹å®Ÿãƒ–ãƒ©ã‚¦ã‚¶ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦")
            credibility_status = "DYNAMIC_CONTENT_CONFIRMED"
        elif static_content_with_questions:
            print("ğŸ¯ çµè«–: å•é¡Œå†…å®¹ã¯é™çš„HTMLã«å«ã¾ã‚Œã¦ã„ã‚‹")
            print("  å¯¾ç­–: æŠ½å‡ºæ‰‹æ³•ã®æ”¹å–„ã«ã‚ˆã‚Šè§£æ±ºå¯èƒ½")
            credibility_status = "STATIC_CONTENT_CONFIRMED"
        else:
            print("ğŸ¯ çµè«–: è¿½åŠ èª¿æŸ»ãŒå¿…è¦")
            print("  å¯¾ç­–: ã‚ˆã‚Šè©³ç´°ãªåˆ†æã¨ãƒ†ã‚¹ãƒˆæ‰‹æ³•ã®æ¤œè¨")
            credibility_status = "NEEDS_FURTHER_INVESTIGATION"
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_filename = f"production_html_structure_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {report_filename}")
        print("\nğŸ”’ æœ¬ç•ªç’°å¢ƒHTMLæ§‹é€ åˆ†æå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¥ æœ¬ç•ªç’°å¢ƒHTMLæ§‹é€ åˆ†æ")
    print("å•é¡Œå†…å®¹æŠ½å‡ºå¤±æ•—åŸå› ã®ç‰¹å®š")
    print()
    
    analyzer = ProductionHtmlStructureAnalysis()
    results = analyzer.run_comprehensive_html_analysis()
    
    return results

if __name__ == "__main__":
    main()