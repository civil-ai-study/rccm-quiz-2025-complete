"""
RCCMå­¦ç¿’ã‚¢ãƒ—ãƒª - ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆ & é«˜æ€§èƒ½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  + Redisçµ±åˆ
"""

import csv
import os
import logging
import threading
import time
import hashlib
import functools
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Callable, Tuple
from collections import OrderedDict
# ğŸ”¥ ULTRA SYNC FILE SAFETY: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®‰å…¨æ€§å¼·åŒ–ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor

# âš¡ Redis Cache Integration
try:
    from redis_cache import cache_manager, cached_questions, get_cached_questions, cache_questions
    REDIS_CACHE_AVAILABLE = True
except ImportError:
    REDIS_CACHE_AVAILABLE = False
    cache_manager = None

# ğŸ”¥ ULTRA SYNC LOG FIX: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è‚¥å¤§åŒ–é˜²æ­¢ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½è¿½åŠ ï¼‰
import logging.handlers

# ãƒ­ã‚°è¨­å®šï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ä»˜ãï¼‰
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# ãƒ­ãƒ¼ãƒ†ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©: æœ€å¤§10MBã€5ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ä¿æŒ
rotating_handler = logging.handlers.RotatingFileHandler(
    'rccm_app.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,  # æœ€å¤§5å€‹ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«
    encoding='utf-8'
)
rotating_handler.setFormatter(log_formatter)

# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

# ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    handlers=[rotating_handler, console_handler]
)

logger = logging.getLogger(__name__)

# === ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£é–¢æ•° ===

def validate_file_path(path: str, allowed_dir: str = None) -> str:
    """
    ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒã‚’é˜²ããƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ¤œè¨¼
    ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ– - ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹å¯¾å¿œ
    """
    import os.path
    
    if not path:
        raise ValueError("ãƒ‘ã‚¹ãŒç©ºã§ã™")
    
    # ãƒ‘ã‚¹ã®æ­£è¦åŒ–
    normalized_path = os.path.normpath(path)
    
    # ğŸ”§ ULTRA SYNCä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’è¨±å¯
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å®‰å…¨ã«è¨±å¯
    # å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ãªãã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€åŸºæº–ã§ãƒ‘ã‚¹ã‚’è§£æ±º
    utils_dir = os.path.dirname(os.path.abspath(__file__))
    project_data_dir = os.path.join(utils_dir, 'data')
    
    # çµ¶å¯¾ãƒ‘ã‚¹ã®å ´åˆã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ãƒã‚§ãƒƒã‚¯
    if os.path.isabs(normalized_path):
        # ğŸ”¥ ULTRA SYNCæœ¬ç•ªç’°å¢ƒå¯¾å¿œ: è¤‡æ•°ã®å®‰å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        is_safe_path = False
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: é–‹ç™ºç’°å¢ƒï¼ˆå¾“æ¥é€šã‚Šï¼‰
        if normalized_path.startswith(project_data_dir):
            is_safe_path = True
            
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æœ¬ç•ªç’°å¢ƒï¼ˆdataãƒ•ã‚¡ã‚¤ãƒ«åãƒã‚§ãƒƒã‚¯ï¼‰
        # 4-2_YYYY.csvã‚„questions.csvãªã©å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿è¨±å¯
        safe_file_patterns = ['4-1.csv', '4-2_', 'questions.csv']
        filename = os.path.basename(normalized_path)
        if any(pattern in filename for pattern in safe_file_patterns):
            # è¿½åŠ : dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if 'data' in normalized_path and not '..' in normalized_path:
                is_safe_path = True
        
        if is_safe_path:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã¯è¨±å¯
            return normalized_path
        else:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤–ã®çµ¶å¯¾ãƒ‘ã‚¹ã¯æ‹’å¦
            raise ValueError(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤–ã¸ã®ä¸æ­£ãƒ‘ã‚¹: {path}")
    
    # ç›¸å¯¾ãƒ‘ã‚¹ã§ã®ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒãƒã‚§ãƒƒã‚¯
    if '..' in normalized_path:
        raise ValueError(f"ä¸æ­£ãªãƒ‘ã‚¹: {path}")
    
    # è¨±å¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æŒ‡å®šãŒã‚ã‚‹å ´åˆã®è¿½åŠ ãƒã‚§ãƒƒã‚¯
    if allowed_dir:
        allowed_dir = os.path.normpath(allowed_dir)
        full_path = os.path.normpath(os.path.join(allowed_dir, normalized_path))
        if not full_path.startswith(allowed_dir):
            raise ValueError(f"è¨±å¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤–ã®ãƒ‘ã‚¹: {path}")
        return full_path
    
    return normalized_path

# === ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  ===

class LRUCache:
    """
    ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…
    ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã‚¢ã‚¯ã‚»ã‚¹é€Ÿåº¦ã‚’ä¸¡ç«‹
    """
    
    def __init__(self, maxsize: int = 100, ttl: int = 3600):
        self.maxsize = maxsize
        self.ttl = ttl  # Time-To-Live (ç§’)
        self.cache = OrderedDict()
        self.timestamps = {}
        self.access_count = {}
        self.hit_count = 0
        self.miss_count = 0
        self.lock = threading.RLock()
        
    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key not in self.cache:
                self.miss_count += 1
                return None
            
            # TTLãƒã‚§ãƒƒã‚¯
            if self._is_expired(key):
                del self.cache[key]
                del self.timestamps[key]
                if key in self.access_count:
                    del self.access_count[key]
                self.miss_count += 1
                return None
            
            # LRUæ›´æ–°
            value = self.cache.pop(key)
            self.cache[key] = value
            self.access_count[key] = self.access_count.get(key, 0) + 1
            self.hit_count += 1
            
            return value
    
    def put(self, key: str, value: Any) -> None:
        with self.lock:
            if key in self.cache:
                self.cache.pop(key)
            elif len(self.cache) >= self.maxsize and self.maxsize > 0:
                # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å®‰å…¨ã«å‰Šé™¤ï¼ˆæ”¹ä¿®ç‰ˆï¼‰
                try:
                    oldest_key = next(iter(self.cache))
                    del self.cache[oldest_key]
                    self.timestamps.pop(oldest_key, None)  # safe removal
                    self.access_count.pop(oldest_key, None)  # safe removal
                except (StopIteration, KeyError) as e:
                    logger.warning(f"Cache cleanup error: {e}")
            
            self.cache[key] = value
            self.timestamps[key] = time.time()
            self.access_count[key] = 0
    
    def _is_expired(self, key: str) -> bool:
        if key not in self.timestamps:
            return True
        return time.time() - self.timestamps[key] > self.ttl
    
    def clear(self) -> None:
        with self.lock:
            self.cache.clear()
            self.timestamps.clear()
            self.access_count.clear()
            self.hit_count = 0
            self.miss_count = 0
    
    def stats(self) -> Dict[str, Any]:
        with self.lock:
            total_requests = self.hit_count + self.miss_count
            hit_rate = self.hit_count / total_requests if total_requests > 0 else 0
            
            return {
                'size': len(self.cache),
                'maxsize': self.maxsize,
                'hit_count': self.hit_count,
                'miss_count': self.miss_count,
                'hit_rate': hit_rate,
                'total_requests': total_requests,
                'most_accessed': sorted(
                    self.access_count.items(), 
                    key=lambda x: x[1], 
                    reverse=True
                )[:5]
            }

class CacheManager:
    """
    çµ±åˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    è¤‡æ•°ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç®¡ç†
    """
    
    def __init__(self):
        # ä¼æ¥­ç’°å¢ƒç”¨ã«æ‹¡å¼µã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        self.caches = {
            'questions': LRUCache(maxsize=50, ttl=7200),  # å•é¡Œãƒ‡ãƒ¼ã‚¿ï¼ˆä¼æ¥­ç”¨æ‹¡å¼µï¼‰
            'validation': LRUCache(maxsize=100, ttl=3600),  # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼çµæœ
            'csv_parsing': LRUCache(maxsize=50, ttl=7200),  # CSVè§£æçµæœ
            'file_metadata': LRUCache(maxsize=200, ttl=600),  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            'department_mapping': LRUCache(maxsize=500, ttl=14400),  # éƒ¨é–€ãƒãƒƒãƒ”ãƒ³ã‚°
            'user_sessions': LRUCache(maxsize=1000, ttl=1800),  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³
            'question_filters': LRUCache(maxsize=200, ttl=3600),  # å•é¡Œãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
            'aggregated_stats': LRUCache(maxsize=100, ttl=900),  # é›†è¨ˆçµ±è¨ˆ
        }
        self.background_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix='cache_bg')
        
    def get_cache(self, cache_name: str) -> LRUCache:
        return self.caches.get(cache_name)
    
    def clear_all(self) -> None:
        for cache in self.caches.values():
            cache.clear()
        logger.info("å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
    
    def get_stats(self) -> Dict[str, Any]:
        stats = {}
        for name, cache in self.caches.items():
            stats[name] = cache.stats()
        return stats
    
    def log_stats(self) -> None:
        stats = self.get_stats()
        logger.info("=== ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ ===")
        for cache_name, cache_stats in stats.items():
            logger.info(f"{cache_name}: ã‚µã‚¤ã‚º={cache_stats['size']}/{cache_stats['maxsize']}, "
                       f"ãƒ’ãƒƒãƒˆç‡={cache_stats['hit_rate']:.2%}, "
                       f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ={cache_stats['total_requests']}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
cache_manager = CacheManager()

def cache_result(cache_name: str, ttl: Optional[int] = None):
    """
    é–¢æ•°çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            key_data = str(args) + str(sorted(kwargs.items()))
            cache_key = hashlib.md5(key_data.encode()).hexdigest()
            
            cache = cache_manager.get_cache(cache_name)
            if cache is None:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç›´æ¥å®Ÿè¡Œ
                return func(*args, **kwargs)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã‚’è©¦è¡Œ
            result = cache.get(cache_key)
            if result is not None:
                logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {func.__name__}")
                return result
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: é–¢æ•°å®Ÿè¡Œ
            logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹: {func.__name__}")
            result = func(*args, **kwargs)
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            cache.put(cache_key, result)
            
            return result
        return wrapper
    return decorator

def get_file_hash(filepath: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
    try:
        validated_filepath = validate_file_path(filepath)
    except ValueError as e:
        logger.error(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ãƒãƒƒã‚·ãƒ¥è¨ˆç®—): {e}")
        return ""
    
    if not os.path.exists(validated_filepath):
        return ""
    
    hash_md5 = hashlib.md5()
    with open(validated_filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def is_file_modified(filepath: str, cached_hash: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
    current_hash = get_file_hash(filepath)
    return current_hash != cached_hash

# ğŸ”¥ ULTRA SYNC FILE SAFETY: ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®‰å…¨æ€§ç›£è¦–ã‚¯ãƒ©ã‚¹
class FileHandleMonitor:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«ä½¿ç”¨çŠ¶æ³ç›£è¦–ãƒ»åˆ¶é™ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_concurrent_files=50):
        self._active_files = set()
        self._lock = threading.Lock()
        self._max_concurrent = max_concurrent_files
        
    def acquire_handle(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«å–å¾—ï¼ˆåˆ¶é™ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        with self._lock:
            if len(self._active_files) >= self._max_concurrent:
                raise RuntimeError(f"åŒæ™‚ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶é™è¶…é: {len(self._active_files)}/{self._max_concurrent}")
            self._active_files.add(filepath)
            
    def release_handle(self, filepath):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«è§£æ”¾"""
        with self._lock:
            self._active_files.discard(filepath)
            
    def get_active_count(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°å–å¾—"""
        with self._lock:
            return len(self._active_files)
            
    def get_active_files(self):
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—"""
        with self._lock:
            return list(self._active_files)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«ç›£è¦–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_file_monitor = FileHandleMonitor()

@contextmanager
def monitored_file_open(filepath, mode='r', encoding='utf-8', **kwargs):
    """ç›£è¦–ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³ï¼ˆã‚¦ãƒ«ãƒˆãƒ©ã‚·ãƒ³ã‚¯å®‰å…¨æ€§ä¿è¨¼ï¼‰"""
    # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
    try:
        validated_filepath = validate_file_path(filepath)
    except ValueError as e:
        logger.error(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (ç›£è¦–ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³): {e}")
        raise ValueError(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {e}")
    
    file_handle = None
    try:
        # ãƒãƒ³ãƒ‰ãƒ«å–å¾—åˆ¶é™ãƒã‚§ãƒƒã‚¯
        _file_monitor.acquire_handle(validated_filepath)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚ªãƒ¼ãƒ—ãƒ³
        file_handle = open(validated_filepath, mode, encoding=encoding, **kwargs)
        yield file_handle
        
    except Exception as e:
        logger.error(f"ç›£è¦–ä»˜ããƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã‚¨ãƒ©ãƒ¼: {filepath} - {e}")
        raise
    finally:
        # ç¢ºå®Ÿã«ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾
        if file_handle and not file_handle.closed:
            try:
                file_handle.close()
            except Exception as close_error:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒ­ãƒ¼ã‚ºã‚¨ãƒ©ãƒ¼: {filepath} - {close_error}")
        
        # ç›£è¦–ã‹ã‚‰é™¤å»
        _file_monitor.release_handle(filepath)

def get_file_monitor_stats():
    """ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–çµ±è¨ˆå–å¾—"""
    return {
        'active_count': _file_monitor.get_active_count(),
        'active_files': _file_monitor.get_active_files(),
        'max_concurrent': _file_monitor._max_concurrent
    }

class DataLoadError(Exception):
    """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å°‚ç”¨ã‚¨ãƒ©ãƒ¼"""
    pass

class DataValidationError(Exception):
    """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼å°‚ç”¨ã‚¨ãƒ©ãƒ¼"""
    pass

@cache_result('questions', ttl=3600)
@cached_questions(timeout=300, key_suffix="improved")
def load_questions_improved(csv_path: str) -> List[Dict]:
    """
    âš¡ Redisçµ±åˆ æ”¹å–„ç‰ˆå•é¡Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨é«˜é€ŸRedisã‚­ãƒ£ãƒƒã‚·ãƒ¥
    """
    # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
    try:
        csv_path = validate_file_path(csv_path)  # allowed_dirã¯æŒ‡å®šã—ãªã„
    except ValueError as e:
        logger.error(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {e}")
        raise DataLoadError(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {e}")
    
    logger.info(f"å•é¡Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {csv_path}")
    
    # âš¡ Redis Cache Integration - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    department = os.path.basename(csv_path).replace('.csv', '')
    if REDIS_CACHE_AVAILABLE:
        cached_questions_data = get_cached_questions(department)
        if cached_questions_data:
            logger.info(f"ğŸ¯ Redis Cache HIT: {department} ({len(cached_questions_data)} questions)")
            return cached_questions_data
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    if not os.path.exists(csv_path):
        logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        raise FileNotFoundError(f"å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
    file_size = os.path.getsize(csv_path)
    if file_size == 0:
        logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™: {csv_path}")
        raise DataLoadError("å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã§ã™")
    
    logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size} bytes")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ãƒã‚§ãƒƒã‚¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–ç”¨ï¼‰
    file_hash = get_file_hash(csv_path)
    cache_key = f"{csv_path}_{file_hash}"
    
    # å¾“æ¥ã®CSVãƒ‘ãƒ¼ã‚¹çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¢ºèªï¼ˆRedisãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    csv_cache = cache_manager.get_cache('csv_parsing') if hasattr(cache_manager, 'get_cache') and cache_manager else None
    cached_df = csv_cache.get(cache_key) if csv_cache else None
    
    if cached_df is not None:
        logger.debug(f"CSVè§£æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—: {csv_path}")
        df, used_encoding = cached_df
    else:
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ¥èª­ã¿è¾¼ã¿è©¦è¡Œï¼ˆCLAUDE.mdæº–æ‹ : Shift_JISå„ªå…ˆï¼‰
        encodings = ['utf-8-sig', 'utf-8', 'shift_jis', 'cp932', 'iso-2022-jp']
        df = None
        used_encoding = None
        
        for encoding in encodings:
            try:
                logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è©¦è¡Œ: {encoding}")
                with open(csv_path, 'r', encoding=encoding, newline='') as f:
                    reader = csv.DictReader(f)
                    rows = list(reader)
                    if not rows:
                        logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                        raise DataLoadError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    df = rows
                    used_encoding = encoding
                    logger.info(f"èª­ã¿è¾¼ã¿æˆåŠŸ: {encoding} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°")
                    break
            except UnicodeDecodeError as e:
                logger.debug(f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ {encoding}: {e}")
                continue
            except Exception as e:
                logger.error(f"CSVè§£æã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # è§£æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        if df is not None and csv_cache:
            csv_cache.put(cache_key, (df, used_encoding))
    
    if df is None:
        logger.error("ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        raise DataLoadError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç‰¹å®šã§ãã¾ã›ã‚“")
    
    if not df:
        logger.error("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        raise DataLoadError("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    
    logger.info(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}è¡Œ, ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°: {used_encoding}")
    
    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ æ¤œè¨¼
    required_columns = [
        'id', 'category', 'question', 'option_a', 'option_b', 
        'option_c', 'option_d', 'correct_answer'
    ]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€åˆã®è¡Œã‹ã‚‰ã‚«ãƒ©ãƒ ã‚’å–å¾—
    if df:
        columns = list(df[0].keys())
        # ğŸ›¡ï¸ ULTRATHINåŒº: BOMé™¤å»å‡¦ç†
        columns = [col.lstrip('\ufeff') for col in columns]
        # æœ€åˆã®è¡Œã®ã‚­ãƒ¼ã‚‚ä¿®æ­£
        if df[0] and '\ufeffid' in df[0]:
            for row in df:
                if '\ufeffid' in row:
                    row['id'] = row.pop('\ufeffid')
    else:
        columns = []
    
    missing_columns = [col for col in required_columns if col not in columns]
    if missing_columns:
        error_msg = f"å¿…é ˆåˆ—ãŒä¸è¶³: {missing_columns}"
        logger.error(error_msg)
        raise DataValidationError(error_msg)
    
    # ãƒ‡ãƒ¼ã‚¿å†…å®¹æ¤œè¨¼
    valid_questions = []
    validation_errors = []
    
    for index, row in enumerate(df):
        try:
            validated_question = validate_question_data(row, index)
            if validated_question:
                valid_questions.append(validated_question)
        except DataValidationError as e:
            validation_errors.append(f"è¡Œ{index + 2}: {e}")
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ è¡Œ{index + 2}: {e}")
    
    if not valid_questions:
        error_msg = "æœ‰åŠ¹ãªå•é¡Œãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
        logger.error(error_msg)
        if validation_errors:
            error_msg += f"\næ¤œè¨¼ã‚¨ãƒ©ãƒ¼:\n" + "\n".join(validation_errors[:5])
        raise DataValidationError(error_msg)
    
    if validation_errors:
        logger.warning(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã®ã‚ã‚‹è¡Œ: {len(validation_errors)}ä»¶")
        # æœ€åˆã®5ä»¶ã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
        for error in validation_errors[:5]:
            logger.warning(error)
    
    logger.info(f"æœ‰åŠ¹ãªå•é¡Œãƒ‡ãƒ¼ã‚¿: {len(valid_questions)}å•")
    
    # âš¡ Redis Cache Integration - çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    if REDIS_CACHE_AVAILABLE and valid_questions:
        success = cache_questions(department, valid_questions, timeout=300)
        if success:
            logger.info(f"ğŸ’¾ Redis Cache SET: {department} ({len(valid_questions)} questions)")
        else:
            logger.warning(f"âš ï¸ Redis Cache SET failed: {department}")
    
    return valid_questions

def validate_question_data(row: Dict[str, Any], index: int) -> Optional[Dict]:
    """
    å€‹åˆ¥å•é¡Œãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    """
    # IDæ¤œè¨¼
    if not row.get('id') or row.get('id') == '':
        raise DataValidationError("IDãŒç©ºã§ã™")
    
    try:
        question_id = int(float(row['id']))
    except (ValueError, TypeError):
        raise DataValidationError(f"IDãŒæ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {row.get('id')}")
    
    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ¤œè¨¼
    if not row.get('question') or row.get('question') == '':
        raise DataValidationError("å•é¡Œæ–‡ãŒç©ºã§ã™")
    
    if not row.get('correct_answer') or row.get('correct_answer') == '':
        raise DataValidationError("æ­£è§£ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # æ­£è§£é¸æŠè‚¢æ¤œè¨¼
    correct_answer = str(row['correct_answer']).strip().upper()
    if correct_answer not in ['A', 'B', 'C', 'D']:
        raise DataValidationError(f"æ­£è§£é¸æŠè‚¢ãŒç„¡åŠ¹: {correct_answer}")
    
    # é¸æŠè‚¢å­˜åœ¨ç¢ºèª
    options = {
        'A': row.get('option_a'),
        'B': row.get('option_b'),
        'C': row.get('option_c'),
        'D': row.get('option_d')
    }
    
    for option_key, option_text in options.items():
        if not option_text or option_text == '':
            raise DataValidationError(f"é¸æŠè‚¢{option_key}ãŒç©ºã§ã™")
    
    # æ­£è§£é¸æŠè‚¢ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    correct_option = options.get(correct_answer)
    if not correct_option or correct_option == '':
        raise DataValidationError(f"æ­£è§£é¸æŠè‚¢{correct_answer}ã«å¯¾å¿œã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“")
    
    # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
    question_data = {
        'id': question_id,
        'category': str(row.get('category', '')).strip(),
        'question': str(row['question']).strip(),
        'option_a': str(row['option_a']).strip(),
        'option_b': str(row['option_b']).strip(),
        'option_c': str(row['option_c']).strip(),
        'option_d': str(row['option_d']).strip(),
        'correct_answer': correct_answer,
        'explanation': str(row.get('explanation', '')).strip(),
        'reference': str(row.get('reference', '')).strip(),
        'difficulty': str(row.get('difficulty', 'æ¨™æº–')).strip(),
        'keywords': str(row.get('keywords', '')).strip(),
        'practical_tip': str(row.get('practical_tip', '')).strip()
    }
    
    return question_data

@cached_questions(timeout=600, key_suffix="rccm_data_files")  
def load_rccm_data_files(data_dir: str) -> List[Dict]:
    """
    âš¡ Redisçµ±åˆ RCCMå°‚ç”¨ï¼š4-1åŸºç¤ãƒ»4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆèª­ã¿è¾¼ã¿
    ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: é‡è¤‡èª­ã¿è¾¼ã¿é˜²æ­¢æ©Ÿèƒ½ä»˜ã + é«˜é€ŸRedisã‚­ãƒ£ãƒƒã‚·ãƒ¥
    """
    global _data_already_loaded, _data_load_lock
    
    # âš¡ Redis Cache Integration - çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    cache_key = f"rccm_all_data_{data_dir.replace('/', '_')}"
    if REDIS_CACHE_AVAILABLE:
        cached_all_data = get_cached_questions(cache_key)
        if cached_all_data:
            logger.info(f"ğŸ¯ Redis Cache HIT: çµ±åˆãƒ‡ãƒ¼ã‚¿ ({len(cached_all_data)} questions)")
            return cached_all_data
    
    # é‡è¤‡èª­ã¿è¾¼ã¿é˜²æ­¢ãƒã‚§ãƒƒã‚¯
    with _data_load_lock:
        if _data_already_loaded:
            logger.info("ğŸš€ ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: ãƒ‡ãƒ¼ã‚¿æ—¢èª­ã¿è¾¼ã¿æ¸ˆã¿ - ã‚¹ã‚­ãƒƒãƒ—")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
            if hasattr(cache_manager_instance, '_global_questions_cache'):
                return cache_manager_instance._global_questions_cache
            else:
                logger.warning("âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - èª­ã¿è¾¼ã¿ç¶šè¡Œ")
    
    logger.info(f"RCCMçµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹: {data_dir}")
    
    all_questions = []
    file_count = 0
    
    # 4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    basic_file = os.path.join(data_dir, '4-1.csv')
    # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
    try:
        validated_basic_file = validate_file_path(basic_file)  # allowed_dirã¯æŒ‡å®šã—ãªã„
    except ValueError as e:
        logger.error(f"ä¸æ­£ãªåŸºç¤ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {e}")
        validated_basic_file = None
    
    if validated_basic_file and os.path.exists(validated_basic_file):
        try:
            basic_questions = load_questions_improved(validated_basic_file)
            for q in basic_questions:
                q['question_type'] = 'basic'
                q['department'] = 'common'  # åŸºç¤ç§‘ç›®ã¯å…±é€š
                q['category'] = 'å…±é€š'  # ã‚«ãƒ†ã‚´ãƒªã‚‚çµ±ä¸€
                # åŸºç¤ç§‘ç›®ã«ã¯å¹´åº¦æƒ…å ±ã‚’è¨­å®šã—ãªã„ï¼ˆå¹´åº¦ä¸å•ï¼‰
                q['year'] = None
            all_questions.extend(basic_questions)
            file_count += 1
            logger.info(f"4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(basic_questions)}å•")
        except Exception as e:
            logger.warning(f"4-1åŸºç¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå¹´åº¦åˆ¥ï¼‰
    specialist_years = []
    for year in range(2008, 2020):  # 2008-2019å¹´ã®ç¯„å›²ã§ç¢ºèª
        specialist_file = os.path.join(data_dir, f'4-2_{year}.csv')
        # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
        try:
            validated_specialist_file = validate_file_path(specialist_file)  # allowed_dirã¯æŒ‡å®šã—ãªã„
        except ValueError as e:
            logger.error(f"ä¸æ­£ãªå°‚é–€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ ({year}å¹´): {e}")
            continue
        
        if os.path.exists(validated_specialist_file):
            try:
                year_questions = load_questions_improved(validated_specialist_file)
                for q in year_questions:
                    q['question_type'] = 'specialist'
                    q['year'] = year
                    # ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰éƒ¨é–€ã‚’æ¨å®š
                    q['department'] = map_category_to_department(q.get('category', ''))
                    # å°‚é–€ç§‘ç›®ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¢ºã«æ¨™è¨˜
                    if not q.get('category'):
                        q['category'] = 'å°‚é–€ç§‘ç›®'
                
                all_questions.extend(year_questions)
                specialist_years.append(year)
                file_count += 1
                logger.info(f"4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿{year}å¹´èª­ã¿è¾¼ã¿å®Œäº†: {len(year_questions)}å•")
            except Exception as e:
                logger.warning(f"4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿{year}å¹´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # æ³¨: æ—§questions.csvãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰ã¯ä½¿ç”¨ã—ã¾ã›ã‚“
    # RCCMè©¦é¨“ãƒ‡ãƒ¼ã‚¿ã¯4-1.csvã¨4-2_*.csvã‹ã‚‰èª­ã¿è¾¼ã¾ã‚Œã¾ã™
    
    # IDã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ãƒ»èª¿æ•´
    all_questions = resolve_id_conflicts(all_questions)
    
    # ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ•ãƒ©ã‚°ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    with _data_load_lock:
        _data_already_loaded = True
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        cache_manager_instance._global_questions_cache = all_questions
        logger.info("ğŸš€ ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Œäº† - æ¬¡å›èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–")
    
    logger.info(f"RCCMçµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {file_count}ãƒ•ã‚¡ã‚¤ãƒ«, ç·è¨ˆ{len(all_questions)}å•")
    logger.info(f"4-2å°‚é–€ãƒ‡ãƒ¼ã‚¿å¯¾è±¡å¹´åº¦: {specialist_years}")
    
    # âš¡ Redis Cache Integration - çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
    if REDIS_CACHE_AVAILABLE and all_questions:
        success = cache_questions(cache_key, all_questions, timeout=600)
        if success:
            logger.info(f"ğŸ’¾ Redis Cache SET: çµ±åˆãƒ‡ãƒ¼ã‚¿ ({len(all_questions)} questions, TTL: 10min)")
        else:
            logger.warning(f"âš ï¸ Redis Cache SET failed: çµ±åˆãƒ‡ãƒ¼ã‚¿")
    
    return all_questions

def map_category_to_department(category: str) -> str:
    """
    ã‚«ãƒ†ã‚´ãƒªåã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼ˆã‚·ãƒ³ãƒ—ãƒ«è¨­è¨ˆï¼‰
    4-1.csv: ã€Œå…±é€šã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ãã®ã¾ã¾ä½¿ç”¨
    4-2.csv: æ—¥æœ¬èªã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚’ãã®ã¾ã¾ä½¿ç”¨
    """
    # åŸºç¤ç§‘ç›®ï¼ˆ4-1.csvï¼‰ã®ã€Œå…±é€šã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¯ãã®ã¾ã¾ä½¿ç”¨
    if category == 'å…±é€š':
        return 'å…±é€š'
    
    # å°‚é–€ç§‘ç›®ï¼ˆ4-2.csvï¼‰ã®æ—¥æœ¬èªã‚«ãƒ†ã‚´ãƒªãƒ¼åã‚’ãã®ã¾ã¾ä½¿ç”¨
    # å¹´åº¦ã«ã‚ˆã‚‹è¡¨è¨˜ã®é•ã„ã®ã¿æ­£è¦åŒ–
    if 'æ²³å·' in category and 'ç ‚é˜²' in category:
        return 'æ²³å·ã€ç ‚é˜²åŠã³æµ·å²¸ãƒ»æµ·æ´‹'
    elif 'éƒ½å¸‚è¨ˆç”»' in category and 'åœ°æ–¹è¨ˆç”»' in category:
        return 'éƒ½å¸‚è¨ˆç”»åŠã³åœ°æ–¹è¨ˆç”»'
    elif 'é‹¼æ§‹é€ ' in category and 'ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ' in category:
        return 'é‹¼æ§‹é€ åŠã³ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ'
    elif 'åœŸè³ª' in category and 'åŸºç¤' in category:
        return 'åœŸè³ªåŠã³åŸºç¤'
    elif 'æ–½å·¥è¨ˆç”»' in category:
        return 'æ–½å·¥è¨ˆç”»ã€æ–½å·¥è¨­å‚™åŠã³ç©ç®—'
    elif 'ä¸Šæ°´é“' in category:
        return 'ä¸Šæ°´é“åŠã³å·¥æ¥­ç”¨æ°´é“'
    
    # ãã®ä»–ã®å°‚é–€ç§‘ç›®ã¯ãã®ã¾ã¾ä½¿ç”¨
    return category

def resolve_id_conflicts(questions: List[Dict]) -> List[Dict]:
    """
    IDã®é‡è¤‡ã‚’è§£æ±ºã—ã€ä¸€æ„ã®IDã‚’è¨­å®šï¼ˆå•é¡Œç¨®åˆ¥åˆ¥ã«ç¯„å›²åˆ†ã‘ï¼‰
    åŸºç¤ç§‘ç›®: 1000000-1999999, å°‚é–€ç§‘ç›®: 2000000-2999999
    âš ï¸ é‡è¤‡å•é¡Œã®æ ¹æœ¬åŸå› è§£æ±ºã¨å®Œå…¨ãªIDæ•´åˆæ€§ä¿è¨¼
    """
    logger.info(f"IDé‡è¤‡è§£æ±ºé–‹å§‹: å…¥åŠ›å•é¡Œæ•°={len(questions)}å•")
    
    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼
    if not questions:
        logger.warning("ç©ºã®å•é¡Œãƒªã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¾ã—ãŸ")
        return []
    
    # å…ƒã®IDã®é‡è¤‡çŠ¶æ³ã‚’åˆ†æ
    original_id_counts = {}
    for q in questions:
        original_id = str(q.get('id', 'unknown'))
        original_id_counts[original_id] = original_id_counts.get(original_id, 0) + 1
    
    duplicated_ids = [id_val for id_val, count in original_id_counts.items() if count > 1]
    if duplicated_ids:
        # ğŸ”¥ ULTRA SYNC FIX: é‡è¤‡IDæ¤œå‡ºã¯æ­£å¸¸ãªå‡¦ç†å·¥ç¨‹ï¼ˆè­¦å‘Šãƒ¬ãƒ™ãƒ«ä¸‹ã’ï¼‰
        logger.info(f"IDé‡è¤‡è§£æ±ºå‡¦ç†é–‹å§‹: {len(duplicated_ids)}å€‹ã®IDã‚’é‡è¤‡è§£æ±ºä¸­ (ä¾‹: {duplicated_ids[:10]})")
    
    used_ids = set()
    resolved_questions = []
    id_mapping = {}  # å…ƒID â†’ æ–°IDã®ãƒãƒƒãƒ”ãƒ³ã‚°è¨˜éŒ²
    
    # å•é¡Œç¨®åˆ¥åˆ¥ã«åˆ†é›¢
    basic_questions = [q for q in questions if q.get('question_type') == 'basic']
    specialist_questions = [q for q in questions if q.get('question_type') == 'specialist']
    other_questions = [q for q in questions if q.get('question_type') not in ['basic', 'specialist']]
    
    logger.info(f"å•é¡Œåˆ†é¡: åŸºç¤={len(basic_questions)}å•, å°‚é–€={len(specialist_questions)}å•, ãã®ä»–={len(other_questions)}å•")
    
    # åŸºç¤ç§‘ç›®ã®IDç¯„å›²: 1000000-1999999
    next_basic_id = 1000000
    for index, q in enumerate(basic_questions):
        original_id = q.get('id')
        
        # å®‰å…¨ãªæ¬¡ã®IDã‚’è¦‹ã¤ã‘ã‚‹
        while next_basic_id in used_ids or next_basic_id > 1999999:
            next_basic_id += 1
            if next_basic_id > 1999999:
                logger.error(f"åŸºç¤ç§‘ç›®ã®IDç¯„å›²(1000000-1999999)ã‚’è¶…é: {len(basic_questions)}å•ã¯ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™")
                raise DataValidationError(f"åŸºç¤ç§‘ç›®ã®å•é¡Œæ•°({len(basic_questions)})ãŒIDç¯„å›²(1000000-1999999)ã‚’è¶…é")
        
        # IDã‚’æ›´æ–°
        q['id'] = next_basic_id
        q['original_id'] = original_id
        q['file_source'] = '4-1.csv'  # ãƒ‡ãƒ¼ã‚¿æ¥æºã‚’è¨˜éŒ²
        used_ids.add(next_basic_id)
        resolved_questions.append(q)
        id_mapping[f"basic_{original_id}"] = next_basic_id
        next_basic_id += 1
    
    # å°‚é–€ç§‘ç›®ã®IDç¯„å›²: 2000000-2999999
    next_specialist_id = 2000000
    for index, q in enumerate(specialist_questions):
        original_id = q.get('id')
        year = q.get('year', 'unknown')
        
        # å®‰å…¨ãªæ¬¡ã®IDã‚’è¦‹ã¤ã‘ã‚‹
        while next_specialist_id in used_ids or next_specialist_id > 2999999:
            next_specialist_id += 1
            if next_specialist_id > 2999999:
                logger.error(f"å°‚é–€ç§‘ç›®ã®IDç¯„å›²(2000000-2999999)ã‚’è¶…é: {len(specialist_questions)}å•ã¯ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™")
                raise DataValidationError(f"å°‚é–€ç§‘ç›®ã®å•é¡Œæ•°({len(specialist_questions)})ãŒIDç¯„å›²(2000000-2999999)ã‚’è¶…é")
        
        # IDã‚’æ›´æ–°
        q['id'] = next_specialist_id
        q['original_id'] = original_id
        q['file_source'] = f'4-2_{year}.csv'  # ãƒ‡ãƒ¼ã‚¿æ¥æºã‚’è¨˜éŒ²
        used_ids.add(next_specialist_id)
        resolved_questions.append(q)
        id_mapping[f"specialist_{year}_{original_id}"] = next_specialist_id
        next_specialist_id += 1
    
    # ãã®ä»–ã®å•é¡Œ: 3000000ä»¥é™
    next_other_id = 3000000
    for index, q in enumerate(other_questions):
        original_id = q.get('id')
        
        # å®‰å…¨ãªæ¬¡ã®IDã‚’è¦‹ã¤ã‘ã‚‹
        while next_other_id in used_ids:
            next_other_id += 1
            # ç†è«–ä¸Šã®ä¸Šé™ãƒã‚§ãƒƒã‚¯ï¼ˆ100ä¸‡å•ã¾ã§ï¼‰
            if next_other_id > 1000000:
                logger.error("ãã®ä»–å•é¡Œã®IDç¯„å›²ã‚’è¶…éã—ã¾ã—ãŸ")
                raise DataValidationError("ãã®ä»–å•é¡Œã®æ•°ãŒä¸Šé™ã‚’è¶…é")
        
        q['id'] = next_other_id
        q['original_id'] = original_id
        q['file_source'] = 'legacy.csv'
        used_ids.add(next_other_id)
        resolved_questions.append(q)
        id_mapping[f"other_{original_id}"] = next_other_id
        next_other_id += 1
    
    # æœ€çµ‚æ¤œè¨¼
    final_ids = [q['id'] for q in resolved_questions]
    final_unique_ids = set(final_ids)
    
    if len(final_ids) != len(final_unique_ids):
        logger.error(f"IDé‡è¤‡è§£æ±ºå¤±æ•—: {len(final_ids)}å•ä¸­{len(final_unique_ids)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ID")
        raise DataValidationError("IDé‡è¤‡è§£æ±ºã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    # IDç¯„å›²ãƒã‚§ãƒƒã‚¯
    basic_ids = [q['id'] for q in resolved_questions if q.get('question_type') == 'basic']
    specialist_ids = [q['id'] for q in resolved_questions if q.get('question_type') == 'specialist']
    
    if basic_ids and (min(basic_ids) < 1000000 or max(basic_ids) > 1999999):
        logger.error(f"åŸºç¤ç§‘ç›®IDç¯„å›²ã‚¨ãƒ©ãƒ¼: {min(basic_ids)}-{max(basic_ids)}")
        raise DataValidationError("åŸºç¤ç§‘ç›®ã®IDç¯„å›²ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
    
    if specialist_ids and (min(specialist_ids) < 2000000 or max(specialist_ids) > 2999999):
        logger.error(f"å°‚é–€ç§‘ç›®IDç¯„å›²ã‚¨ãƒ©ãƒ¼: {min(specialist_ids)}-{max(specialist_ids)}")
        raise DataValidationError("å°‚é–€ç§‘ç›®ã®IDç¯„å›²ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
    
    logger.info(f"âœ… IDé‡è¤‡è§£æ±ºå®Œäº†: åŸºç¤={len(basic_questions)}å•(1000000-{max(basic_ids) if basic_ids else 1000000}), "
               f"å°‚é–€={len(specialist_questions)}å•(2000000-{max(specialist_ids) if specialist_ids else 2000000}), "
               f"ãã®ä»–={len(other_questions)}å•, ç·è¨ˆ={len(resolved_questions)}å•")
    
    # IDå¤‰æ›´ã®è¨˜éŒ²ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    if duplicated_ids:
        logger.info(f"é‡è¤‡è§£æ±ºä¾‹: {list(id_mapping.items())[:5]}")
    
    return resolved_questions

def get_sample_data_improved() -> List[Dict]:
    """
    æ”¹å–„ç‰ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
    """
    logger.info("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
    
    return [
        {
            'id': 1,
            'category': 'ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ',
            'department': 'road',
            'question_type': 'basic',
            'question': 'æ™®é€šãƒãƒ«ãƒˆãƒ©ãƒ³ãƒ‰ã‚»ãƒ¡ãƒ³ãƒˆã®å‡çµæ™‚é–“ã«é–¢ã™ã‚‹è¨˜è¿°ã§æœ€ã‚‚é©åˆ‡ãªã‚‚ã®ã¯ã©ã‚Œã‹ã€‚',
            'option_a': 'å§‹ç™ºå‡çµæ™‚é–“ã¯45åˆ†ä»¥ä¸Š',
            'option_b': 'çµ‚çµå‡çµæ™‚é–“ã¯8æ™‚é–“ä»¥å†…',
            'option_c': 'å§‹ç™ºå‡çµæ™‚é–“ã¯60åˆ†ä»¥å†…',
            'option_d': 'çµ‚çµå‡çµæ™‚é–“ã¯12æ™‚é–“ä»¥å†…',
            'correct_answer': 'C',
            'explanation': 'JIS R 5210ã§ã¯æ™®é€šãƒãƒ«ãƒˆãƒ©ãƒ³ãƒ‰ã‚»ãƒ¡ãƒ³ãƒˆã®å§‹ç™ºå‡çµæ™‚é–“ã¯60åˆ†ä»¥å†…ã€çµ‚çµå‡çµæ™‚é–“ã¯10æ™‚é–“ä»¥å†…ã¨è¦å®šã•ã‚Œã¦ã„ã¾ã™ã€‚',
            'reference': 'JIS R 5210',
            'difficulty': 'åŸºæœ¬',
            'keywords': 'ã‚»ãƒ¡ãƒ³ãƒˆ,å‡çµæ™‚é–“,å“è³ªç®¡ç†',
            'practical_tip': 'ç¾å ´ã§ã¯æ°—æ¸©ã‚„æ¹¿åº¦ã«ã‚ˆã£ã¦å‡çµæ™‚é–“ãŒå¤‰åŒ–ã™ã‚‹ãŸã‚ã€å­£ç¯€ã«å¿œã˜ãŸæ–½å·¥è¨ˆç”»ã®èª¿æ•´ãŒå¿…è¦ã§ã™ã€‚'
        }
    ]

# === ä¼æ¥­ç’°å¢ƒç”¨CSVãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ– ===

class EnterpriseDataManager:
    """
    ä¼æ¥­ç’°å¢ƒã§ã®CSVãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–
    å¤§é‡åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹ãƒ»é«˜é€Ÿèª­ã¿è¾¼ã¿å¯¾å¿œ
    """
    
    def __init__(self, data_dir: str = 'data', cache_manager: CacheManager = None):
        self.data_dir = data_dir
        self.cache_manager = cache_manager or cache_manager_instance
        self.file_watcher = {}
        self.preload_executor = ThreadPoolExecutor(max_workers=4, thread_name_prefix='preload')
        self.compression_enabled = True
        
    def preload_all_data(self):
        """
        ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ã™ã¹ã¦ã®CSVãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰èª­ã¿è¾¼ã¿
        ä¼æ¥­ç’°å¢ƒã§ã®é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ç¢ºä¿
        """
        logger.info("CSVãƒ‡ãƒ¼ã‚¿ã®äº‹å‰èª­ã¿è¾¼ã¿é–‹å§‹ï¼ˆä¼æ¥­ç’°å¢ƒæœ€é©åŒ–ï¼‰")
        
        try:
            csv_files = [
                '4-1.csv',  # åŸºç¤ç§‘ç›®
                '4-2_2008.csv', '4-2_2009.csv', '4-2_2010.csv',
                '4-2_2011.csv', '4-2_2012.csv', '4-2_2013.csv',
                '4-2_2014.csv', '4-2_2015.csv', '4-2_2016.csv',
                '4-2_2017.csv', '4-2_2018.csv', '4-2_2019.csv'
            ]
            
            # ä¸¦åˆ—èª­ã¿è¾¼ã¿ã§é«˜é€ŸåŒ–
            futures = []
            for csv_file in csv_files:
                future = self.preload_executor.submit(self._preload_single_file, csv_file)
                futures.append(future)
            
            # çµæœç¢ºèª
            loaded_count = 0
            for future in futures:
                try:
                    if future.result():
                        loaded_count += 1
                except Exception as e:
                    logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«äº‹å‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            
            logger.info(f"CSVãƒ‡ãƒ¼ã‚¿äº‹å‰èª­ã¿è¾¼ã¿å®Œäº†: {loaded_count}/{len(csv_files)} ãƒ•ã‚¡ã‚¤ãƒ«")
            return loaded_count == len(csv_files)
            
        except Exception as e:
            logger.error(f"CSVãƒ‡ãƒ¼ã‚¿äº‹å‰èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return False
    
    def _preload_single_file(self, filename: str) -> bool:
        """å˜ä¸€CSVãƒ•ã‚¡ã‚¤ãƒ«ã®äº‹å‰èª­ã¿è¾¼ã¿"""
        try:
            file_path = os.path.join(self.data_dir, filename)
            # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
            try:
                validated_file_path = validate_file_path(file_path)  # allowed_dirã¯æŒ‡å®šã—ãªã„
            except ValueError as e:
                logger.error(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (äº‹å‰èª­ã¿è¾¼ã¿): {e}")
                return False
            
            if not os.path.exists(validated_file_path):
                return False
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
            cache_key = f"csv_preload_{filename}"
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¤å®š
            stat = os.stat(validated_file_path)
            metadata_key = f"{cache_key}_metadata"
            current_metadata = f"{stat.st_size}_{stat.st_mtime}"
            
            metadata_cache = self.cache_manager.get_cache('file_metadata')
            cached_metadata = metadata_cache.get(metadata_key)
            
            if cached_metadata == current_metadata:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸€è‡´ã™ã‚‹å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿
                return True
            
            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            data = load_questions_improved(validated_file_path)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            questions_cache = self.cache_manager.get_cache('questions')
            questions_cache.put(cache_key, data)
            metadata_cache.put(metadata_key, current_metadata)
            
            logger.debug(f"äº‹å‰èª­ã¿è¾¼ã¿å®Œäº†: {filename} ({len(data)}å•)")
            return True
            
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«äº‹å‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {filename}: {e}")
            return False
    
    def get_optimized_data(self, filename: str) -> List[Dict]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å–å¾—
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ãƒ¼ã‚¹ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        """
        cache_key = f"csv_preload_{filename}"
        questions_cache = self.cache_manager.get_cache('questions')
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
        cached_data = questions_cache.get(cache_key)
        if cached_data:
            return cached_data
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹æ™‚ã¯ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èª­ã¿è¾¼ã¿
        logger.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èª­ã¿è¾¼ã¿: {filename}")
        file_path = os.path.join(self.data_dir, filename)
        # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
        try:
            validated_file_path = validate_file_path(file_path)  # allowed_dirã¯æŒ‡å®šã—ãªã„
        except ValueError as e:
            logger.error(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿å–å¾—): {e}")
            return []
        
        data = load_questions_improved(validated_file_path)
        
        # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        questions_cache.put(cache_key, data)
        return data
    
    def get_file_integrity_check(self) -> Dict[str, Any]:
        """
        CSVãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        ä¼æ¥­ç’°å¢ƒã§ã®ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
        """
        try:
            integrity_report = {
                'timestamp': datetime.now().isoformat(),
                'files': {},
                'total_questions': 0,
                'status': 'healthy'
            }
            
            # å„CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            for filename in os.listdir(self.data_dir):
                if filename.endswith('.csv') and not filename.endswith(('_backup.csv', '_fixed.csv')):
                    file_path = os.path.join(self.data_dir, filename)
                    
                    try:
                        # ğŸ›¡ï¸ ULTRA SYNC ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒé˜²æ­¢
                        try:
                            validated_file_path = validate_file_path(file_path)  # allowed_dirã¯æŒ‡å®šã—ãªã„
                        except ValueError as e:
                            logger.error(f"ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯): {e}")
                            integrity_report['files'][filename] = {
                                'status': 'error',
                                'error': f'ä¸æ­£ãªãƒ‘ã‚¹: {e}'
                            }
                            integrity_report['status'] = 'degraded'
                            continue
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«åŸºæœ¬æƒ…å ±
                        stat = os.stat(validated_file_path)
                        
                        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
                        data = load_questions_improved(validated_file_path)
                        
                        integrity_report['files'][filename] = {
                            'size_bytes': stat.st_size,
                            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                            'question_count': len(data),
                            'status': 'ok'
                        }
                        integrity_report['total_questions'] += len(data)
                        
                    except Exception as e:
                        integrity_report['files'][filename] = {
                            'status': 'error',
                            'error': str(e)
                        }
                        integrity_report['status'] = 'degraded'
            
            logger.info(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {integrity_report['total_questions']}å•")
            return integrity_report
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'timestamp': datetime.now().isoformat(),
                'status': 'error',
                'error': str(e)
            }

# ä¼æ¥­ç’°å¢ƒæœ€é©åŒ–: é‡è¤‡èª­ã¿è¾¼ã¿é˜²æ­¢ãƒ•ãƒ©ã‚°
_data_already_loaded = False
_data_load_lock = threading.Lock()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹  
cache_manager_instance = CacheManager()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆä¼æ¥­ç’°å¢ƒç”¨ï¼‰
enterprise_data_manager = EnterpriseDataManager(cache_manager=cache_manager_instance)


# ========================================
# ğŸš€ ULTRATHINåŒº: å®Œå…¨åˆ†é›¢ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•°
# ========================================

def load_basic_questions_only(data_dir: str = 'data') -> List[Dict]:
    """
    ğŸ›¡ï¸ ULTRATHINåŒº: åŸºç¤ç§‘ç›®(4-1)å°‚ç”¨èª­ã¿è¾¼ã¿é–¢æ•°
    çµ¶å¯¾ã«å°‚é–€ç§‘ç›®ã¨æ··ãœãªã„å®‰å…¨è¨­è¨ˆ
    """
    logger.info("ğŸ›¡ï¸ ULTRATHINåŒº: åŸºç¤ç§‘ç›®å°‚ç”¨èª­ã¿è¾¼ã¿é–‹å§‹")
    
    basic_questions = []
    basic_file = os.path.join(data_dir, '4-1.csv')
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
    try:
        validated_basic_file = validate_file_path(basic_file)
    except ValueError as e:
        logger.error(f"ğŸš¨ åŸºç¤ç§‘ç›®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    
    if not os.path.exists(validated_basic_file):
        logger.error(f"ğŸš¨ åŸºç¤ç§‘ç›®ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {validated_basic_file}")
        return []
    
    try:
        questions = load_questions_improved(validated_basic_file)
        
        # åŸºç¤ç§‘ç›®å°‚ç”¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¨­å®š
        for q in questions:
            q['question_type'] = 'basic'  # çµ¶å¯¾ã«'basic'
            q['department'] = 'common'     # åŸºç¤ç§‘ç›®ã¯å…±é€š
            q['category'] = 'å…±é€š'          # ã‚«ãƒ†ã‚´ãƒªã‚‚çµ±ä¸€
            q['year'] = None              # åŸºç¤ç§‘ç›®ã¯å¹´åº¦ä¸å•
            q['source_file'] = '4-1.csv'  # ã‚½ãƒ¼ã‚¹è­˜åˆ¥
        
        basic_questions = questions
        logger.info(f"âœ… ULTRATHINåŒº: åŸºç¤ç§‘ç›®èª­ã¿è¾¼ã¿å®Œäº† - {len(basic_questions)}å•")
        
    except Exception as e:
        logger.error(f"ğŸš¨ åŸºç¤ç§‘ç›®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    
    return basic_questions


def load_specialist_questions_only(department: str, year: int, data_dir: str = 'data') -> List[Dict]:
    """
    ğŸ›¡ï¸ ULTRATHINåŒº: å°‚é–€ç§‘ç›®(4-2)å°‚ç”¨èª­ã¿è¾¼ã¿é–¢æ•°
    æŒ‡å®šéƒ¨é–€ãƒ»å¹´åº¦ã®ã¿ã€çµ¶å¯¾ã«åŸºç¤ç§‘ç›®ã¨æ··ãœãªã„å®‰å…¨è¨­è¨ˆ
    """
    logger.info(f"ğŸ›¡ï¸ ULTRATHINåŒº: å°‚é–€ç§‘ç›®å°‚ç”¨èª­ã¿è¾¼ã¿é–‹å§‹ - {department}/{year}å¹´")
    
    specialist_questions = []
    specialist_file = os.path.join(data_dir, f'4-2_{year}.csv')
    
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
    try:
        validated_specialist_file = validate_file_path(specialist_file)
    except ValueError as e:
        logger.error(f"ğŸš¨ å°‚é–€ç§‘ç›®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return []
    
    if not os.path.exists(validated_specialist_file):
        logger.error(f"ğŸš¨ å°‚é–€ç§‘ç›®ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹: {validated_specialist_file}")
        return []
    
    try:
        questions = load_questions_improved(validated_specialist_file)
        
        # æŒ‡å®šéƒ¨é–€ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        department_questions = []
        for q in questions:
            if q.get('category') == department:
                q['question_type'] = 'specialist'  # çµ¶å¯¾ã«'specialist'
                q['department'] = map_category_to_department(department)
                q['year'] = year                   # å¹´åº¦æƒ…å ±å¿…é ˆ
                q['source_file'] = f'4-2_{year}.csv'  # ã‚½ãƒ¼ã‚¹è­˜åˆ¥
                department_questions.append(q)
        
        specialist_questions = department_questions
        logger.info(f"âœ… ULTRATHINåŒº: å°‚é–€ç§‘ç›®èª­ã¿è¾¼ã¿å®Œäº† - {department}/{year}å¹´ {len(specialist_questions)}å•")
        
    except Exception as e:
        logger.error(f"ğŸš¨ å°‚é–€ç§‘ç›®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°: type={type(e).__name__}, file={validated_specialist_file}, department={department}, year={year}")
        logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª: {os.path.exists(validated_specialist_file)}")
        if os.path.exists(validated_specialist_file):
            logger.error(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {os.path.getsize(validated_specialist_file)} bytes")
        import traceback
        logger.error(f"ğŸ” ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")
        return []
    
    return specialist_questions 