#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ æœ¬ç•ªç’°å¢ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æ
ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã§è¿”ã•ã‚Œã‚‹å•é¡Œã®åŸå› èª¿æŸ»

ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ:
- HTMLãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
- æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å•é¡Œã§ãƒ†ã‚­ã‚¹ãƒˆãŒèª­ã‚ãªã„
- gzipåœ§ç¸®ã¾ãŸã¯ãã®ä»–ã®åœ§ç¸®ãŒå½±éŸ¿ã—ã¦ã„ã‚‹å¯èƒ½æ€§

å¯¾ç­–ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ãƒ¼ã®è©³ç´°åˆ†æ
2. Content-Encodingã®ç¢ºèª
3. é©åˆ‡ãªãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•ã®ç‰¹å®š
"""

import requests
import json
import time
from datetime import datetime
import re
import gzip
import zlib
import base64
from typing import Dict, List, Optional, Tuple
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProductionContentEncodingAnalysis:
    def __init__(self):
        self.base_url = "https://rccm-quiz-2025.onrender.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',  # æ˜ç¤ºçš„ã«åœ§ç¸®ã‚’å—ã‘å…¥ã‚Œ
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # åˆ†æçµæœ
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'analysis_type': 'PRODUCTION_CONTENT_ENCODING_ANALYSIS',
            'target_url': self.base_url,
            'encoding_tests': {},
            'decoding_attempts': {},
            'successful_extractions': [],
            'critical_findings': []
        }

    def analyze_response_encoding(self, response: requests.Response, test_name: str) -> Dict:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®è©³ç´°åˆ†æ"""
        
        print(f"ğŸ”¬ {test_name} ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æ")
        
        encoding_analysis = {
            'test_name': test_name,
            'status_code': response.status_code,
            'headers': dict(response.headers),
            'encoding_detected': response.encoding,
            'content_length': len(response.content),
            'text_length': len(response.text),
            'is_binary': False,
            'compression_detected': None,
            'charset_from_headers': None,
            'decoding_attempts': []
        }
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ã®æƒ…å ±æŠ½å‡º
        content_type = response.headers.get('content-type', '')
        content_encoding = response.headers.get('content-encoding', '')
        
        encoding_analysis['charset_from_headers'] = content_type
        encoding_analysis['compression_detected'] = content_encoding
        
        print(f"  ğŸ“Š åŸºæœ¬æƒ…å ±:")
        print(f"    Status: {response.status_code}")
        print(f"    Content-Type: {content_type}")
        print(f"    Content-Encoding: {content_encoding}")
        print(f"    Content-Length: {len(response.content)} bytes")
        print(f"    Response Encoding: {response.encoding}")
        
        # ãƒã‚¤ãƒŠãƒªåˆ¤å®š
        try:
            response.content.decode('utf-8')
            encoding_analysis['is_binary'] = False
        except UnicodeDecodeError:
            encoding_analysis['is_binary'] = True
            print(f"  ğŸš¨ ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿æ¤œå‡º")
        
        # æ§˜ã€…ãªãƒ‡ã‚³ãƒ¼ãƒ‰è©¦è¡Œ
        decoding_methods = [
            ('utf-8', lambda x: x.decode('utf-8')),
            ('shift_jis', lambda x: x.decode('shift_jis')),
            ('euc-jp', lambda x: x.decode('euc-jp')),
            ('iso-2022-jp', lambda x: x.decode('iso-2022-jp')),
            ('gzip+utf-8', lambda x: gzip.decompress(x).decode('utf-8')),
            ('deflate+utf-8', lambda x: zlib.decompress(x).decode('utf-8')),
            ('latin1+utf-8', lambda x: x.decode('latin1').encode('latin1').decode('utf-8'))
        ]
        
        for method_name, decode_func in decoding_methods:
            try:
                decoded_content = decode_func(response.content)
                
                # æ—¥æœ¬èªæ–‡å­—ã®å­˜åœ¨ç¢ºèª
                japanese_found = bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', decoded_content))
                
                # å•é¡Œé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¢ºèª
                question_keywords = ['å•é¡Œ', 'å•', 'é¸æŠ', 'å›ç­”', 'è§£ç­”', 'Question']
                keywords_found = [kw for kw in question_keywords if kw in decoded_content]
                
                decoding_result = {
                    'method': method_name,
                    'success': True,
                    'decoded_length': len(decoded_content),
                    'japanese_detected': japanese_found,
                    'question_keywords_found': keywords_found,
                    'sample_text': decoded_content[:500] if decoded_content else '',
                    'html_tags_found': bool(re.search(r'<[^>]+>', decoded_content))
                }
                
                encoding_analysis['decoding_attempts'].append(decoding_result)
                
                if keywords_found or japanese_found:
                    print(f"  âœ… {method_name} æˆåŠŸ: æ—¥æœ¬èª={japanese_found}, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={len(keywords_found)}å€‹")
                    if keywords_found:
                        print(f"    ç™ºè¦‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords_found}")
                else:
                    print(f"  âš ï¸ {method_name} éƒ¨åˆ†æˆåŠŸ: å†…å®¹ç¢ºèªå¿…è¦")
                
            except Exception as e:
                decoding_result = {
                    'method': method_name,
                    'success': False,
                    'error': str(e)
                }
                encoding_analysis['decoding_attempts'].append(decoding_result)
                print(f"  âŒ {method_name} å¤±æ•—: {str(e)}")
        
        return encoding_analysis

    def test_different_request_methods(self) -> Dict:
        """ç•°ãªã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–¹æ³•ã§ã®ãƒ†ã‚¹ãƒˆ"""
        
        print("ğŸ§ª è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–¹æ³•ãƒ†ã‚¹ãƒˆ")
        print("=" * 60)
        
        tests = {}
        
        # ãƒ†ã‚¹ãƒˆ1: Accept-Encodingãªã—
        print("ğŸ“ Test 1: Accept-Encodingãƒ˜ãƒƒãƒ€ãƒ¼ãªã—")
        session_no_encoding = requests.Session()
        session_no_encoding.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8'
            # Accept-Encodingã‚’æ„å›³çš„ã«é™¤å¤–
        })
        
        try:
            specialist_data = {
                'questions': '10',
                'category': 'å»ºè¨­ç’°å¢ƒ',
                'year': '2019'
            }
            
            response1 = session_no_encoding.post(
                f"{self.base_url}/start_exam/specialist", 
                data=specialist_data, 
                timeout=30, 
                allow_redirects=True
            )
            
            tests['no_compression'] = self.analyze_response_encoding(response1, "åœ§ç¸®ãªã—ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")
            
        except Exception as e:
            tests['no_compression'] = {'error': str(e)}
            print(f"  ğŸ’¥ åœ§ç¸®ãªã—ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {str(e)}")
        
        time.sleep(2)
        
        # ãƒ†ã‚¹ãƒˆ2: æ˜ç¤ºçš„ã«identityè¦æ±‚
        print("\nğŸ“ Test 2: identity encodingæ˜ç¤º")
        session_identity = requests.Session()
        session_identity.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'identity'  # åœ§ç¸®ãªã—ã‚’æ˜ç¤º
        })
        
        try:
            response2 = session_identity.post(
                f"{self.base_url}/start_exam/specialist", 
                data=specialist_data, 
                timeout=30, 
                allow_redirects=True
            )
            
            tests['identity_encoding'] = self.analyze_response_encoding(response2, "identity encoding")
            
        except Exception as e:
            tests['identity_encoding'] = {'error': str(e)}
            print(f"  ğŸ’¥ identity encodingãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {str(e)}")
        
        time.sleep(2)
        
        # ãƒ†ã‚¹ãƒˆ3: gzipã®ã¿
        print("\nğŸ“ Test 3: gzipåœ§ç¸®ã®ã¿")
        session_gzip = requests.Session()
        session_gzip.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip'  # gzipã®ã¿
        })
        
        try:
            response3 = session_gzip.post(
                f"{self.base_url}/start_exam/specialist", 
                data=specialist_data, 
                timeout=30, 
                allow_redirects=True
            )
            
            tests['gzip_only'] = self.analyze_response_encoding(response3, "gzipåœ§ç¸®ã®ã¿")
            
        except Exception as e:
            tests['gzip_only'] = {'error': str(e)}
            print(f"  ğŸ’¥ gzipåœ§ç¸®ã®ã¿ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: {str(e)}")
        
        time.sleep(2)
        
        # ãƒ†ã‚¹ãƒˆ4: åŸºç¤ç§‘ç›®ã§ã®æ¯”è¼ƒ
        print("\nğŸ“ Test 4: åŸºç¤ç§‘ç›®ã§ã®æ¯”è¼ƒ")
        try:
            basic_data = {'questions': '10'}
            
            response4 = self.session.post(
                f"{self.base_url}/start_exam/basic", 
                data=basic_data, 
                timeout=30, 
                allow_redirects=True
            )
            
            tests['basic_exam'] = self.analyze_response_encoding(response4, "åŸºç¤ç§‘ç›®è©¦é¨“")
            
        except Exception as e:
            tests['basic_exam'] = {'error': str(e)}
            print(f"  ğŸ’¥ åŸºç¤ç§‘ç›®è©¦é¨“å¤±æ•—: {str(e)}")
        
        return tests

    def extract_successful_content(self, tests: Dict) -> List[Dict]:
        """æˆåŠŸã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º"""
        
        print("\nğŸ¯ æˆåŠŸã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º")
        print("=" * 60)
        
        successful_extractions = []
        
        for test_name, test_result in tests.items():
            if 'decoding_attempts' not in test_result:
                continue
            
            for attempt in test_result['decoding_attempts']:
                if (attempt.get('success') and 
                    (attempt.get('question_keywords_found') or attempt.get('japanese_detected'))):
                    
                    print(f"âœ… {test_name} - {attempt['method']} ã§æˆåŠŸ")
                    
                    decoded_text = attempt.get('sample_text', '')
                    
                    # å•é¡Œæ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°æ¤œç´¢
                    question_patterns = [
                        r'å•é¡Œ\s*(\d+)\s*[:ï¼š.ï¼]\s*([^å•]{50,500})',
                        r'å•\s*(\d+)\s*[:ï¼š.ï¼]\s*([^å•]{50,500})',
                        r'(\d+)\s*[:ï¼š.ï¼]\s*([^0-9]{50,500})',
                        r'Question\s*(\d+)\s*[:ï¼š.ï¼]\s*([^Q]{50,500})'
                    ]
                    
                    questions_found = []
                    for pattern in question_patterns:
                        matches = re.findall(pattern, decoded_text, re.DOTALL)
                        for match in matches:
                            questions_found.append({
                                'number': match[0],
                                'content': match[1].strip()[:200],
                                'pattern': pattern
                            })
                    
                    # é¸æŠè‚¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢
                    choice_patterns = [
                        r'[â‘ â‘¡â‘¢â‘£â‘¤]([^â‘ â‘¡â‘¢â‘£â‘¤]{10,100})',
                        r'[ï¼‘ï¼’ï¼“ï¼”ï¼•]\.([^ï¼‘ï¼’ï¼“ï¼”ï¼•]{10,100})',
                        r'[ã‚¢-ã‚ª]\.([^ã‚¢-ã‚ª]{10,100})'
                    ]
                    
                    choices_found = []
                    for pattern in choice_patterns:
                        matches = re.findall(pattern, decoded_text)
                        choices_found.extend([choice.strip() for choice in matches if len(choice.strip()) > 5])
                    
                    extraction_result = {
                        'test_name': test_name,
                        'decoding_method': attempt['method'],
                        'questions_extracted': questions_found,
                        'choices_extracted': choices_found[:10],  # æœ€åˆã®10å€‹
                        'japanese_content_confirmed': attempt.get('japanese_detected', False),
                        'html_structure_confirmed': attempt.get('html_tags_found', False),
                        'extraction_confidence': 'HIGH' if questions_found else 'MEDIUM' if choices_found else 'LOW'
                    }
                    
                    successful_extractions.append(extraction_result)
                    
                    print(f"  ğŸ“ å•é¡ŒæŠ½å‡º: {len(questions_found)}å€‹")
                    print(f"  ğŸ“ é¸æŠè‚¢æŠ½å‡º: {len(choices_found)}å€‹")
                    print(f"  ğŸ“Š ä¿¡é ¼åº¦: {extraction_result['extraction_confidence']}")
                    
                    if questions_found:
                        print(f"  ğŸ“„ å•é¡Œä¾‹: {questions_found[0]['content'][:100]}...")
        
        return successful_extractions

    def run_comprehensive_encoding_analysis(self):
        """åŒ…æ‹¬çš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æã®å®Ÿè¡Œ"""
        
        print("ğŸ”¥ æœ¬ç•ªç’°å¢ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æé–‹å§‹")
        print("=" * 80)
        print("ç›®çš„: ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã§è¿”ã•ã‚Œã‚‹å•é¡Œã®åŸå› èª¿æŸ»")
        print("å¯¾è±¡: https://rccm-quiz-2025.onrender.com")
        print("=" * 80)
        
        start_time = time.time()
        
        # Step 1: è¤‡æ•°ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ–¹æ³•ã§ã®ãƒ†ã‚¹ãƒˆ
        encoding_tests = self.test_different_request_methods()
        self.analysis_results['encoding_tests'] = encoding_tests
        
        # Step 2: æˆåŠŸã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
        successful_extractions = self.extract_successful_content(encoding_tests)
        self.analysis_results['successful_extractions'] = successful_extractions
        
        end_time = time.time()
        duration = end_time - start_time
        
        # Step 3: çµæœåˆ†æã¨æ¨å¥¨æ‰‹æ³•ã®ç‰¹å®š
        self.generate_encoding_analysis_report(duration)
        
        return self.analysis_results

    def generate_encoding_analysis_report(self, duration: float):
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        print("\n" + "=" * 80)
        print("ğŸ”¥ æœ¬ç•ªç’°å¢ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æçµæœ")
        print("=" * 80)
        
        encoding_tests = self.analysis_results['encoding_tests']
        successful_extractions = self.analysis_results['successful_extractions']
        
        print(f"ğŸ“Š åˆ†æçµ±è¨ˆ:")
        print(f"  å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°: {len(encoding_tests)}")
        print(f"  æˆåŠŸãƒ‡ã‚³ãƒ¼ãƒ‰æ•°: {len(successful_extractions)}")
        print(f"  åˆ†æå®Ÿè¡Œæ™‚é–“: {duration:.1f}ç§’")
        print()
        
        # é‡è¦ãªç™ºè¦‹äº‹é …
        critical_findings = []
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸç‡
        total_attempts = 0
        successful_attempts = 0
        
        for test_name, test_result in encoding_tests.items():
            if 'decoding_attempts' in test_result:
                total_attempts += len(test_result['decoding_attempts'])
                successful_attempts += sum(1 for attempt in test_result['decoding_attempts'] if attempt.get('success'))
        
        success_rate = (successful_attempts / total_attempts) * 100 if total_attempts > 0 else 0
        
        print(f"ğŸ¯ ãƒ‡ã‚³ãƒ¼ãƒ‰æˆåŠŸç‡: {success_rate:.1f}% ({successful_attempts}/{total_attempts})")
        print()
        
        # æœ€é©ãªãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•ã®ç‰¹å®š
        decoding_methods_success = {}
        
        for extraction in successful_extractions:
            method = extraction['decoding_method']
            confidence = extraction['extraction_confidence']
            
            if method not in decoding_methods_success:
                decoding_methods_success[method] = {
                    'count': 0,
                    'high_confidence': 0,
                    'questions_found': 0
                }
            
            decoding_methods_success[method]['count'] += 1
            if confidence == 'HIGH':
                decoding_methods_success[method]['high_confidence'] += 1
            decoding_methods_success[method]['questions_found'] += len(extraction['questions_extracted'])
        
        if decoding_methods_success:
            print("ğŸ† æœ€é©ãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•:")
            
            # æˆåŠŸç‡ã§ã‚½ãƒ¼ãƒˆ
            sorted_methods = sorted(
                decoding_methods_success.items(), 
                key=lambda x: (x[1]['high_confidence'], x[1]['questions_found']), 
                reverse=True
            )
            
            best_method = sorted_methods[0][0] if sorted_methods else None
            
            for method, stats in sorted_methods:
                status = "ğŸ¥‡" if method == best_method else "ğŸ¥ˆ" if stats['high_confidence'] > 0 else "ğŸ¥‰"
                print(f"  {status} {method}: æˆåŠŸ{stats['count']}å›, é«˜ä¿¡é ¼{stats['high_confidence']}å›, å•é¡Œ{stats['questions_found']}å€‹")
            
            critical_findings.append(f"æœ€é©ãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•: {best_method}")
            
            # æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            if best_method:
                if 'gzip' in best_method:
                    recommended_approach = "gzipåœ§ç¸®è§£é™¤ + UTF-8ãƒ‡ã‚³ãƒ¼ãƒ‰"
                    critical_findings.append("gzipåœ§ç¸®ãŒåŸå› ")
                elif 'deflate' in best_method:
                    recommended_approach = "deflateåœ§ç¸®è§£é™¤ + UTF-8ãƒ‡ã‚³ãƒ¼ãƒ‰"
                    critical_findings.append("deflateåœ§ç¸®ãŒåŸå› ")
                elif 'latin1' in best_method:
                    recommended_approach = "æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¤‰æ›(latin1â†’utf-8)"
                    critical_findings.append("æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡ŒãŒåŸå› ")
                else:
                    recommended_approach = f"{best_method}ã«ã‚ˆã‚‹ç›´æ¥ãƒ‡ã‚³ãƒ¼ãƒ‰"
                    critical_findings.append("æ¨™æº–çš„ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œ")
                
                print(f"\nğŸ’¡ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {recommended_approach}")
        else:
            print("âŒ æˆåŠŸã—ãŸãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•ãªã—")
            critical_findings.append("å…¨ã¦ã®ãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•ãŒå¤±æ•—")
            recommended_approach = "ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã§ã®å‹•çš„å–å¾—ã‚’æ¤œè¨"
        
        # å•é¡Œå†…å®¹ã®ç¢ºèª
        total_questions = sum(len(ext['questions_extracted']) for ext in successful_extractions)
        total_choices = sum(len(ext['choices_extracted']) for ext in successful_extractions)
        
        if total_questions > 0:
            print(f"\nğŸ“ å•é¡Œå†…å®¹ç¢ºèª:")
            print(f"  æŠ½å‡ºå•é¡Œæ•°: {total_questions}å€‹")
            print(f"  æŠ½å‡ºé¸æŠè‚¢æ•°: {total_choices}å€‹")
            critical_findings.append(f"å•é¡Œå†…å®¹æŠ½å‡ºæˆåŠŸ: {total_questions}å•")
            
            # å•é¡Œã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            for extraction in successful_extractions[:2]:  # æœ€åˆã®2ã¤
                if extraction['questions_extracted']:
                    question = extraction['questions_extracted'][0]
                    print(f"  ğŸ“„ å•é¡Œä¾‹({extraction['decoding_method']}): {question['content'][:150]}...")
        else:
            print(f"\nâŒ å•é¡Œå†…å®¹ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            critical_findings.append("å•é¡Œå†…å®¹æŠ½å‡ºå¤±æ•—")
        
        self.analysis_results['critical_findings'] = critical_findings
        
        print()
        
        # æœ€çµ‚åˆ¤å®š
        if total_questions > 0:
            print("ğŸ† çµè«–: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡ŒãŒç‰¹å®šã•ã‚Œã€è§£æ±ºå¯èƒ½")
            print(f"  å¯¾ç­–: {recommended_approach}ã‚’å®Ÿè£…")
            credibility_status = "ENCODING_PROBLEM_SOLVED"
        elif successful_extractions:
            print("âš ï¸ çµè«–: éƒ¨åˆ†çš„ãªæˆåŠŸã€ã•ã‚‰ãªã‚‹èª¿æ•´ãŒå¿…è¦")
            print("  å¯¾ç­–: ãƒ‡ã‚³ãƒ¼ãƒ‰æ‰‹æ³•ã®æœ€é©åŒ–ãŒå¿…è¦")
            credibility_status = "PARTIAL_SUCCESS"
        else:
            print("ğŸš¨ çµè«–: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡ŒãŒæ·±åˆ»ã€ä»£æ›¿æ‰‹æ³•ãŒå¿…è¦")
            print("  å¯¾ç­–: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ã«ã‚ˆã‚‹å‹•çš„å–å¾—ã‚’æ¤œè¨")
            credibility_status = "ENCODING_PROBLEM_CRITICAL"
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_filename = f"production_content_encoding_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {report_filename}")
        print("\nğŸ”’ æœ¬ç•ªç’°å¢ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æå®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¥ æœ¬ç•ªç’°å¢ƒã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°åˆ†æ")
    print("ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿å•é¡Œã®åŸå› èª¿æŸ»ã¨è§£æ±ºç­–ã®ç‰¹å®š")
    print()
    
    analyzer = ProductionContentEncodingAnalysis()
    results = analyzer.run_comprehensive_encoding_analysis()
    
    return results

if __name__ == "__main__":
    main()