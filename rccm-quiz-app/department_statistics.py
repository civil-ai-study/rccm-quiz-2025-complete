"""
RCCMå­¦ç¿’ã‚¢ãƒ—ãƒª - éƒ¨é–€ãƒ»å•é¡Œç¨®åˆ¥åˆ¥çµ±è¨ˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³
è©³ç´°ãªå­¦ç¿’é€²æ—ã¨æˆç¸¾åˆ†ææ©Ÿèƒ½ã‚’æä¾›
"""

import statistics
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import List, Dict, Any, Tuple
import logging

# RCCMè¨­å®šã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import RCCMConfig

logger = logging.getLogger(__name__)

class DepartmentStatisticsAnalyzer:
    """éƒ¨é–€ãƒ»å•é¡Œç¨®åˆ¥åˆ¥çµ±è¨ˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.department_weights = {
            'road': 1.0,  # é“è·¯ï¼ˆåŸºæº–ï¼‰
            'civil_planning': 1.2,  # æ²³å·ãƒ»ç ‚é˜²ï¼ˆè¤‡é›‘ï¼‰
            'construction_env': 1.15,  # å»ºè¨­ç’°å¢ƒ
            'comprehensive': 1.3,  # ç·åˆæŠ€è¡“ç›£ç†ï¼ˆæœ€é›£é–¢ï¼‰
            'port_airport': 1.1,  # æ¸¯æ¹¾ãƒ»ç©ºæ¸¯
            'railway': 1.05,  # é‰„é“
            'urban_planning': 1.1,  # éƒ½å¸‚è¨ˆç”»
            'construction_mgmt': 1.1,  # å»ºè¨­ç®¡ç†
            'power_civil': 1.2,  # é›»åŠ›åœŸæœ¨
            'forestry': 1.0,  # æ£®æ—åœŸæœ¨
            'fisheries': 1.0,  # æ°´ç”£åœŸæœ¨
            'agriculture': 1.0  # è¾²æ¥­åœŸæœ¨
        }
    
    def _safe_mean(self, values: List[float]) -> float:
        """å®‰å…¨ãªå¹³å‡å€¤è¨ˆç®—"""
        try:
            return statistics.mean(values) if values else 0
        except (TypeError, ValueError, statistics.StatisticsError):
            return 0
    
    def generate_comprehensive_department_report(self, user_session: Dict) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãªéƒ¨é–€åˆ¥çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        history = user_session.get('history', [])
        
        if not history:
            return self._empty_report()
        
        # å„ç¨®çµ±è¨ˆåˆ†æã‚’å®Ÿè¡Œ
        department_analysis = self._analyze_department_performance(history)
        question_type_analysis = self._analyze_question_type_performance(history)
        cross_analysis = self._analyze_department_question_type_cross(history)
        time_series_analysis = self._analyze_time_series_trends(history)
        learning_efficiency = self._calculate_learning_efficiency(history)
        mastery_assessment = self._assess_mastery_levels(history)
        
        # å­¦ç¿’æ¨å¥¨ã‚’ç”Ÿæˆ
        recommendations = self._generate_learning_recommendations(
            department_analysis, question_type_analysis, cross_analysis
        )
        
        # ç·åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        comprehensive_report = {
            'overview': self._generate_overview_stats(history),
            'department_analysis': department_analysis,
            'question_type_analysis': question_type_analysis,
            'cross_analysis': cross_analysis,
            'time_series_analysis': time_series_analysis,
            'learning_efficiency': learning_efficiency,
            'mastery_assessment': mastery_assessment,
            'recommendations': recommendations,
            'generated_at': datetime.now().isoformat(),
            'total_questions_analyzed': len(history)
        }
        
        logger.info(f"éƒ¨é–€åˆ¥çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {len(history)}å•åˆ†æ")
        
        return comprehensive_report
    
    def _analyze_department_performance(self, history: List[Dict]) -> Dict[str, Any]:
        """éƒ¨é–€åˆ¥æˆç¸¾åˆ†æ"""
        department_stats = defaultdict(lambda: {
            'total_questions': 0,
            'correct_answers': 0,
            'total_time': 0,
            'recent_performance': [],
            'categories': set(),
            'question_types': defaultdict(lambda: {'total': 0, 'correct': 0}),
            'difficulty_distribution': defaultdict(int),
            'weekly_progress': defaultdict(lambda: {'total': 0, 'correct': 0})
        })
        
        # æœ€è¿‘30å•ã®ç¯„å›²ã‚’è¨­å®š
        recent_threshold = max(0, len(history) - 30)
        
        for i, entry in enumerate(history):
            department = entry.get('department', 'unknown')
            is_correct = entry.get('is_correct', False)
            elapsed_time = entry.get('elapsed', 0)
            category = entry.get('category', '')
            question_type = entry.get('question_type', 'unknown')
            difficulty = entry.get('difficulty', 'æ¨™æº–')
            date_str = entry.get('date', '')
            
            stats = department_stats[department]
            
            # åŸºæœ¬çµ±è¨ˆ
            stats['total_questions'] += 1
            if is_correct:
                stats['correct_answers'] += 1
            stats['total_time'] += elapsed_time
            stats['categories'].add(category)
            
            # å•é¡Œç¨®åˆ¥çµ±è¨ˆ
            stats['question_types'][question_type]['total'] += 1
            if is_correct:
                stats['question_types'][question_type]['correct'] += 1
            
            # é›£æ˜“åº¦åˆ†å¸ƒ
            stats['difficulty_distribution'][difficulty] += 1
            
            # æœ€è¿‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆæœ€æ–°30å•ï¼‰
            if i >= recent_threshold:
                stats['recent_performance'].append(is_correct)
            
            # é€±æ¬¡é€²æ—ï¼ˆæ—¥ä»˜ãŒã‚ã‚‹å ´åˆï¼‰
            if date_str:
                try:
                    date = datetime.fromisoformat(date_str.replace(' ', 'T'))
                    week_key = date.strftime('%Y-W%U')  # å¹´-é€±ç•ªå·
                    stats['weekly_progress'][week_key]['total'] += 1
                    if is_correct:
                        stats['weekly_progress'][week_key]['correct'] += 1
                except:
                    pass
        
        # åˆ†æçµæœã®è¨ˆç®—
        analysis = {}
        for department, stats in department_stats.items():
            if stats['total_questions'] > 0:
                accuracy = stats['correct_answers'] / stats['total_questions']
                avg_time = stats['total_time'] / stats['total_questions']
                recent_accuracy = (
                    sum(stats['recent_performance']) / len(stats['recent_performance'])
                    if stats['recent_performance'] else accuracy
                )
                
                # éƒ¨é–€é‡ã¿ã‚’è€ƒæ…®ã—ãŸèª¿æ•´æ¸ˆã¿æˆç¸¾
                dept_weight = self.department_weights.get(department, 1.0)
                weighted_accuracy = accuracy / dept_weight
                
                # å•é¡Œç¨®åˆ¥åˆ¥åˆ†æ
                type_analysis = {}
                for qtype, type_stats in stats['question_types'].items():
                    if type_stats['total'] > 0:
                        type_accuracy = type_stats['correct'] / type_stats['total']
                        type_analysis[qtype] = {
                            'accuracy': type_accuracy,
                            'total_questions': type_stats['total'],
                            'correct_answers': type_stats['correct']
                        }
                
                # é€±æ¬¡é€²æ—åˆ†æ
                weekly_trends = []
                for week, week_stats in sorted(stats['weekly_progress'].items()):
                    if week_stats['total'] > 0:
                        week_accuracy = week_stats['correct'] / week_stats['total']
                        weekly_trends.append({
                            'week': week,
                            'accuracy': week_accuracy,
                            'total_questions': week_stats['total']
                        })
                
                # éƒ¨é–€æƒ…å ±ã‚’å–å¾—
                dept_info = RCCMConfig.DEPARTMENTS.get(department, {
                    'name': department, 'description': 'ä¸æ˜ãªéƒ¨é–€'
                })
                
                analysis[department] = {
                    'name': dept_info.get('name', department),
                    'description': dept_info.get('description', ''),
                    'icon': dept_info.get('icon', 'ğŸ“‹'),
                    'color': dept_info.get('color', '#6c757d'),
                    'total_questions': stats['total_questions'],
                    'correct_answers': stats['correct_answers'],
                    'accuracy': accuracy,
                    'recent_accuracy': recent_accuracy,
                    'weighted_accuracy': weighted_accuracy,
                    'avg_time_per_question': avg_time,
                    'categories_covered': len(stats['categories']),
                    'question_type_analysis': type_analysis,
                    'difficulty_distribution': dict(stats['difficulty_distribution']),
                    'weekly_trends': weekly_trends,
                    'improvement_trend': recent_accuracy - accuracy,
                    'performance_grade': self._calculate_performance_grade(accuracy, dept_weight),
                    'study_recommendation': self._get_department_study_recommendation(
                        department, accuracy, type_analysis
                    )
                }
        
        return analysis
    
    def _analyze_question_type_performance(self, history: List[Dict]) -> Dict[str, Any]:
        """å•é¡Œç¨®åˆ¥åˆ¥æˆç¸¾åˆ†æï¼ˆ4-1åŸºç¤ vs 4-2å°‚é–€ï¼‰"""
        type_stats = defaultdict(lambda: {
            'total_questions': 0,
            'correct_answers': 0,
            'total_time': 0,
            'departments': defaultdict(lambda: {'total': 0, 'correct': 0}),
            'categories': defaultdict(lambda: {'total': 0, 'correct': 0}),
            'difficulty_distribution': defaultdict(int),
            'time_series': []
        })
        
        for entry in history:
            question_type = entry.get('question_type', 'unknown')
            department = entry.get('department', 'unknown')
            category = entry.get('category', 'unknown')
            difficulty = entry.get('difficulty', 'æ¨™æº–')
            is_correct = entry.get('is_correct', False)
            elapsed_time = entry.get('elapsed', 0)
            date_str = entry.get('date', '')
            
            stats = type_stats[question_type]
            
            # åŸºæœ¬çµ±è¨ˆ
            stats['total_questions'] += 1
            if is_correct:
                stats['correct_answers'] += 1
            stats['total_time'] += elapsed_time
            
            # éƒ¨é–€åˆ¥çµ±è¨ˆ
            stats['departments'][department]['total'] += 1
            if is_correct:
                stats['departments'][department]['correct'] += 1
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
            stats['categories'][category]['total'] += 1
            if is_correct:
                stats['categories'][category]['correct'] += 1
            
            # é›£æ˜“åº¦åˆ†å¸ƒ
            stats['difficulty_distribution'][difficulty] += 1
            
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
            if date_str:
                try:
                    date = datetime.fromisoformat(date_str.replace(' ', 'T'))
                    stats['time_series'].append({
                        'date': date.strftime('%Y-%m-%d'),
                        'is_correct': is_correct,
                        'elapsed_time': elapsed_time
                    })
                except:
                    pass
        
        # åˆ†æçµæœã®è¨ˆç®—
        analysis = {}
        for question_type, stats in type_stats.items():
            if stats['total_questions'] > 0:
                accuracy = stats['correct_answers'] / stats['total_questions']
                avg_time = stats['total_time'] / stats['total_questions']
                
                # éƒ¨é–€åˆ¥æˆç¸¾
                department_performance = {}
                for dept, dept_stats in stats['departments'].items():
                    if dept_stats['total'] > 0:
                        dept_accuracy = dept_stats['correct'] / dept_stats['total']
                        department_performance[dept] = {
                            'accuracy': dept_accuracy,
                            'total_questions': dept_stats['total'],
                            'department_name': RCCMConfig.DEPARTMENTS.get(dept, {}).get('name', dept)
                        }
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ¥æˆç¸¾
                category_performance = {}
                for cat, cat_stats in stats['categories'].items():
                    if cat_stats['total'] > 0:
                        cat_accuracy = cat_stats['correct'] / cat_stats['total']
                        category_performance[cat] = {
                            'accuracy': cat_accuracy,
                            'total_questions': cat_stats['total']
                        }
                
                # å•é¡Œç¨®åˆ¥æƒ…å ±ã‚’å–å¾—
                type_info = RCCMConfig.QUESTION_TYPES.get(question_type, {
                    'name': question_type, 'description': 'ä¸æ˜ãªå•é¡Œç¨®åˆ¥'
                })
                
                analysis[question_type] = {
                    'name': type_info.get('name', question_type),
                    'description': type_info.get('description', ''),
                    'total_questions': stats['total_questions'],
                    'correct_answers': stats['correct_answers'],
                    'accuracy': accuracy,
                    'avg_time_per_question': avg_time,
                    'department_performance': department_performance,
                    'category_performance': category_performance,
                    'difficulty_distribution': dict(stats['difficulty_distribution']),
                    'performance_grade': self._calculate_performance_grade(accuracy, 1.0),
                    'study_focus': self._get_question_type_study_focus(question_type, accuracy)
                }
        
        return analysis
    
    def _analyze_department_question_type_cross(self, history: List[Dict]) -> Dict[str, Any]:
        """éƒ¨é–€Ã—å•é¡Œç¨®åˆ¥ã®ã‚¯ãƒ­ã‚¹åˆ†æ"""
        cross_stats = defaultdict(lambda: defaultdict(lambda: {
            'total': 0, 'correct': 0, 'total_time': 0
        }))
        
        for entry in history:
            department = entry.get('department', 'unknown')
            question_type = entry.get('question_type', 'unknown')
            is_correct = entry.get('is_correct', False)
            elapsed_time = entry.get('elapsed', 0)
            
            stats = cross_stats[department][question_type]
            stats['total'] += 1
            if is_correct:
                stats['correct'] += 1
            stats['total_time'] += elapsed_time
        
        # ã‚¯ãƒ­ã‚¹åˆ†æçµæœ
        cross_analysis = {}
        correlations = []
        
        for department, dept_data in cross_stats.items():
            dept_analysis = {}
            for question_type, type_data in dept_data.items():
                if type_data['total'] > 0:
                    accuracy = type_data['correct'] / type_data['total']
                    avg_time = type_data['total_time'] / type_data['total']
                    
                    dept_analysis[question_type] = {
                        'accuracy': accuracy,
                        'total_questions': type_data['total'],
                        'avg_time': avg_time
                    }
            
            if dept_analysis:
                cross_analysis[department] = dept_analysis
                
                # åŸºç¤â†’å°‚é–€ã®ç›¸é–¢åˆ†æ
                if 'basic' in dept_analysis and 'specialist' in dept_analysis:
                    basic_acc = dept_analysis['basic']['accuracy']
                    specialist_acc = dept_analysis['specialist']['accuracy']
                    correlation = specialist_acc - basic_acc
                    
                    correlations.append({
                        'department': department,
                        'department_name': RCCMConfig.DEPARTMENTS.get(department, {}).get('name', department),
                        'basic_accuracy': basic_acc,
                        'specialist_accuracy': specialist_acc,
                        'correlation': correlation,
                        'foundation_strength': basic_acc >= 0.7,
                        'learning_efficiency': specialist_acc / basic_acc if basic_acc > 0 else 0
                    })
        
        return {
            'cross_performance': cross_analysis,
            'basic_specialist_correlations': correlations,
            'overall_correlation': self._safe_mean([c['correlation'] for c in correlations]) if correlations else 0
        }
    
    def _analyze_time_series_trends(self, history: List[Dict]) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—å­¦ç¿’å‚¾å‘åˆ†æ"""
        if len(history) < 10:
            return {'trend': 'insufficient_data'}
        
        # æ—¥ä»˜åˆ¥é›†è¨ˆ
        daily_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
        weekly_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
        monthly_stats = defaultdict(lambda: {'total': 0, 'correct': 0})
        
        for entry in history:
            date_str = entry.get('date', '')
            is_correct = entry.get('is_correct', False)
            
            if date_str:
                try:
                    date = datetime.fromisoformat(date_str.replace(' ', 'T'))
                    day_key = date.strftime('%Y-%m-%d')
                    week_key = date.strftime('%Y-W%U')
                    month_key = date.strftime('%Y-%m')
                    
                    daily_stats[day_key]['total'] += 1
                    weekly_stats[week_key]['total'] += 1
                    monthly_stats[month_key]['total'] += 1
                    
                    if is_correct:
                        daily_stats[day_key]['correct'] += 1
                        weekly_stats[week_key]['correct'] += 1
                        monthly_stats[month_key]['correct'] += 1
                except:
                    continue
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        def calculate_trend(stats_dict):
            if len(stats_dict) < 2:
                return []
            
            trend_data = []
            for period, stats in sorted(stats_dict.items()):
                if stats['total'] > 0:
                    accuracy = stats['correct'] / stats['total']
                    trend_data.append({
                        'period': period,
                        'accuracy': accuracy,
                        'total_questions': stats['total']
                    })
            return trend_data
        
        daily_trend = calculate_trend(daily_stats)
        weekly_trend = calculate_trend(weekly_stats)
        monthly_trend = calculate_trend(monthly_stats)
        
        # å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘ã®åˆ¤å®š
        if weekly_trend and len(weekly_trend) >= 2:
            recent_weeks = weekly_trend[-3:] if len(weekly_trend) >= 3 else weekly_trend
            recent_avg = self._safe_mean([w['accuracy'] for w in recent_weeks])
            early_weeks = weekly_trend[:3] if len(weekly_trend) >= 3 else weekly_trend[:-1]
            early_avg = self._safe_mean([w['accuracy'] for w in early_weeks]) if early_weeks else 0
            
            trend_direction = 'improving' if recent_avg > early_avg + 0.05 else \
                            'declining' if recent_avg < early_avg - 0.05 else 'stable'
        else:
            trend_direction = 'insufficient_data'
        
        return {
            'daily_trend': daily_trend,
            'weekly_trend': weekly_trend,
            'monthly_trend': monthly_trend,
            'trend_direction': trend_direction,
            'analysis_period_days': len(daily_stats),
            'total_study_days': len([d for d in daily_stats.values() if d['total'] > 0])
        }
    
    def _calculate_learning_efficiency(self, history: List[Dict]) -> Dict[str, Any]:
        """å­¦ç¿’åŠ¹ç‡ã®è¨ˆç®—"""
        if not history:
            return {}
        
        total_time = sum(entry.get('elapsed', 0) for entry in history)
        total_questions = len(history)
        correct_answers = sum(1 for entry in history if entry.get('is_correct', False))
        
        # åŸºæœ¬åŠ¹ç‡æŒ‡æ¨™
        avg_time_per_question = total_time / total_questions if total_questions > 0 else 0
        accuracy_rate = correct_answers / total_questions if total_questions > 0 else 0
        efficiency_score = correct_answers / (total_time / 60) if total_time > 0 else 0  # æ­£ç­”æ•°/åˆ†
        
        # éƒ¨é–€åˆ¥åŠ¹ç‡
        department_efficiency = {}
        dept_stats = defaultdict(lambda: {'time': 0, 'questions': 0, 'correct': 0})
        
        for entry in history:
            department = entry.get('department', 'unknown')
            elapsed = entry.get('elapsed', 0)
            is_correct = entry.get('is_correct', False)
            
            dept_stats[department]['time'] += elapsed
            dept_stats[department]['questions'] += 1
            if is_correct:
                dept_stats[department]['correct'] += 1
        
        for department, stats in dept_stats.items():
            if stats['questions'] > 0 and stats['time'] > 0:
                dept_efficiency = stats['correct'] / (stats['time'] / 60)
                department_efficiency[department] = {
                    'efficiency_score': dept_efficiency,
                    'avg_time': stats['time'] / stats['questions'],
                    'accuracy': stats['correct'] / stats['questions']
                }
        
        return {
            'overall_efficiency_score': efficiency_score,
            'avg_time_per_question': avg_time_per_question,
            'accuracy_rate': accuracy_rate,
            'department_efficiency': department_efficiency,
            'efficiency_grade': self._grade_efficiency(efficiency_score),
            'time_management_advice': self._get_time_management_advice(avg_time_per_question)
        }
    
    def _assess_mastery_levels(self, history: List[Dict]) -> Dict[str, Any]:
        """ç¿’ç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡"""
        # éƒ¨é–€åˆ¥ç¿’ç†Ÿåº¦
        department_mastery = {}
        question_type_mastery = {}
        
        # éƒ¨é–€åˆ¥ç¿’ç†Ÿåº¦è¨ˆç®—
        dept_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'recent_correct': 0, 'recent_total': 0})
        
        recent_threshold = max(0, len(history) - 20)  # æœ€è¿‘20å•
        
        for i, entry in enumerate(history):
            department = entry.get('department', 'unknown')
            question_type = entry.get('question_type', 'unknown')
            is_correct = entry.get('is_correct', False)
            
            # éƒ¨é–€åˆ¥çµ±è¨ˆ
            dept_stats[department]['total'] += 1
            if is_correct:
                dept_stats[department]['correct'] += 1
            
            # æœ€è¿‘ã®æˆç¸¾
            if i >= recent_threshold:
                dept_stats[department]['recent_total'] += 1
                if is_correct:
                    dept_stats[department]['recent_correct'] += 1
        
        for department, stats in dept_stats.items():
            if stats['total'] >= 5:  # æœ€ä½5å•ä»¥ä¸Š
                overall_accuracy = stats['correct'] / stats['total']
                recent_accuracy = (stats['recent_correct'] / stats['recent_total'] 
                                 if stats['recent_total'] > 0 else overall_accuracy)
                
                # ç¿’ç†Ÿåº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š
                mastery_level = self._determine_mastery_level(overall_accuracy, recent_accuracy, stats['total'])
                
                department_mastery[department] = {
                    'mastery_level': mastery_level,
                    'overall_accuracy': overall_accuracy,
                    'recent_accuracy': recent_accuracy,
                    'total_questions': stats['total'],
                    'improvement_trend': recent_accuracy - overall_accuracy,
                    'department_name': RCCMConfig.DEPARTMENTS.get(department, {}).get('name', department)
                }
        
        return {
            'department_mastery': department_mastery,
            'question_type_mastery': question_type_mastery,
            'overall_mastery_summary': self._generate_mastery_summary(department_mastery)
        }
    
    def _generate_learning_recommendations(self, dept_analysis: Dict, type_analysis: Dict, cross_analysis: Dict) -> Dict[str, Any]:
        """å­¦ç¿’æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = {
            'priority_departments': [],
            'priority_question_types': [],
            'study_strategy': [],
            'time_allocation': {},
            'next_steps': []
        }
        
        # å„ªå…ˆéƒ¨é–€ã®ç‰¹å®šï¼ˆæˆç¸¾ã®ä½ã„é †ï¼‰
        dept_priorities = []
        for dept_id, dept_data in dept_analysis.items():
            if dept_data['total_questions'] >= 3:
                priority_score = (1 - dept_data['accuracy']) * dept_data['total_questions']
                dept_priorities.append({
                    'department': dept_id,
                    'name': dept_data['name'],
                    'priority_score': priority_score,
                    'accuracy': dept_data['accuracy'],
                    'recommendation': dept_data['study_recommendation']
                })
        
        dept_priorities.sort(key=lambda x: x['priority_score'], reverse=True)
        recommendations['priority_departments'] = dept_priorities[:3]
        
        # å•é¡Œç¨®åˆ¥ã®å„ªå…ˆåº¦
        for type_id, type_data in type_analysis.items():
            if type_data['total_questions'] >= 3:
                recommendations['priority_question_types'].append({
                    'question_type': type_id,
                    'name': type_data['name'],
                    'accuracy': type_data['accuracy'],
                    'focus': type_data['study_focus']
                })
        
        # å­¦ç¿’æˆ¦ç•¥
        if dept_priorities:
            lowest_dept = dept_priorities[0]
            if lowest_dept['accuracy'] < 0.5:
                recommendations['study_strategy'].append(
                    f"{lowest_dept['name']}ã®åŸºç¤ã‹ã‚‰é›†ä¸­çš„ã«å¾©ç¿’ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
                )
            elif lowest_dept['accuracy'] < 0.7:
                recommendations['study_strategy'].append(
                    f"{lowest_dept['name']}ã®å¿œç”¨å•é¡Œã§å®ŸåŠ›ã‚’å‘ä¸Šã•ã›ã¾ã—ã‚‡ã†ã€‚"
                )
        
        # åŸºç¤â†’å°‚é–€ã®ç›¸é–¢ã‹ã‚‰æ¨å¥¨
        correlations = cross_analysis.get('basic_specialist_correlations', [])
        for corr in correlations:
            if not corr['foundation_strength']:
                recommendations['study_strategy'].append(
                    f"{corr['department_name']}ã§ã¯åŸºç¤ï¼ˆ4-1ï¼‰ã®ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ã§å°‚é–€ï¼ˆ4-2ï¼‰ã®æˆç¸¾å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ã€‚"
                )
        
        return recommendations
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    
    def _empty_report(self) -> Dict[str, Any]:
        """ç©ºã®ãƒ¬ãƒãƒ¼ãƒˆã‚’è¿”ã™"""
        return {
            'overview': {
                'total_questions': 0, 
                'correct_answers': 0,
                'overall_accuracy': 0.0,
                'departments_studied': 0,
                'department_coverage': 0.0,
                'question_types_studied': 0,
                'performance_summary': 'å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å•é¡Œã«æŒ‘æˆ¦ã—ã¦ãã ã•ã„ã€‚'
            },
            'department_analysis': {},
            'question_type_analysis': {},
            'cross_analysis': {},
            'time_series_analysis': {'trend': 'no_data'},
            'learning_efficiency': {},
            'mastery_assessment': {},
            'recommendations': {
                'priority_departments': [],
                'priority_question_types': [],
                'study_strategy': ['ã¾ãšã¯èˆˆå‘³ã®ã‚ã‚‹éƒ¨é–€ã‹ã‚‰å­¦ç¿’ã‚’å§‹ã‚ã¾ã—ã‚‡ã†ã€‚'],
                'time_allocation': {},
                'next_steps': []
            },
            'generated_at': datetime.now().isoformat(),
            'total_questions_analyzed': 0
        }
    
    def _generate_overview_stats(self, history: List[Dict]) -> Dict[str, Any]:
        """æ¦‚è¦çµ±è¨ˆã®ç”Ÿæˆ"""
        total_questions = len(history)
        correct_answers = sum(1 for h in history if h.get('is_correct', False))
        accuracy = correct_answers / total_questions if total_questions > 0 else 0
        
        # éƒ¨é–€ã‚«ãƒãƒ¬ãƒƒã‚¸
        departments_studied = len(set(h.get('department') for h in history if h.get('department')))
        total_departments = len(RCCMConfig.DEPARTMENTS)
        
        # å•é¡Œç¨®åˆ¥ã‚«ãƒãƒ¬ãƒƒã‚¸
        types_studied = len(set(h.get('question_type') for h in history if h.get('question_type')))
        
        return {
            'total_questions': total_questions,
            'correct_answers': correct_answers,
            'overall_accuracy': accuracy,
            'departments_studied': departments_studied,
            'department_coverage': departments_studied / total_departments,
            'question_types_studied': types_studied,
            'performance_summary': self._get_performance_summary(accuracy)
        }
    
    def _calculate_performance_grade(self, accuracy: float, weight: float) -> str:
        """æˆç¸¾ã‚°ãƒ¬ãƒ¼ãƒ‰ã®è¨ˆç®—"""
        adjusted_accuracy = accuracy / weight
        
        if adjusted_accuracy >= 0.9:
            return 'A+'
        elif adjusted_accuracy >= 0.8:
            return 'A'
        elif adjusted_accuracy >= 0.7:
            return 'B+'
        elif adjusted_accuracy >= 0.6:
            return 'B'
        elif adjusted_accuracy >= 0.5:
            return 'C+'
        elif adjusted_accuracy >= 0.4:
            return 'C'
        else:
            return 'D'
    
    def _get_department_study_recommendation(self, department: str, accuracy: float, type_analysis: Dict) -> str:
        """éƒ¨é–€åˆ¥å­¦ç¿’æ¨å¥¨"""
        if accuracy < 0.5:
            return f"åŸºç¤ã‹ã‚‰å¾¹åº•çš„ã«å¾©ç¿’ã—ã¾ã—ã‚‡ã†ã€‚"
        elif accuracy < 0.7:
            return f"å¿œç”¨å•é¡Œã§å®ŸåŠ›ã‚’å‘ä¸Šã•ã›ã¾ã—ã‚‡ã†ã€‚"
        elif accuracy < 0.85:
            return f"é«˜é›£æ˜“åº¦å•é¡Œã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¾ã—ã‚‡ã†ã€‚"
        else:
            return f"å„ªç§€ãªæˆç¸¾ã§ã™ã€‚ä»–éƒ¨é–€ã¸ã®å±•é–‹ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
    
    def _get_question_type_study_focus(self, question_type: str, accuracy: float) -> str:
        """å•é¡Œç¨®åˆ¥åˆ¥å­¦ç¿’é‡ç‚¹"""
        if question_type == 'basic':
            if accuracy < 0.7:
                return "åŸºç¤çŸ¥è­˜ã®å¾¹åº•ç†è§£ãŒå¿…è¦ã§ã™ã€‚"
            else:
                return "åŸºç¤ã¯è‰¯å¥½ã§ã™ã€‚å°‚é–€åˆ†é‡ã¸ã®å¿œç”¨ã‚’é€²ã‚ã¾ã—ã‚‡ã†ã€‚"
        elif question_type == 'specialist':
            if accuracy < 0.6:
                return "å°‚é–€åˆ†é‡ã®åŸºç¤ç†è§£ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚"
            else:
                return "å°‚é–€çŸ¥è­˜ã‚’å®Ÿè·µã«æ´»ç”¨ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã§ã™ã€‚"
        else:
            return "ç¶™ç¶šçš„ãªå­¦ç¿’ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚"
    
    def _determine_mastery_level(self, overall_accuracy: float, recent_accuracy: float, total_questions: int) -> str:
        """ç¿’ç†Ÿåº¦ãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š"""
        if total_questions < 10:
            return "åˆç´š"
        elif overall_accuracy >= 0.8 and recent_accuracy >= 0.8:
            return "ä¸Šç´š"
        elif overall_accuracy >= 0.6 and recent_accuracy >= 0.6:
            return "ä¸­ç´š"
        else:
            return "åˆç´š"
    
    def _generate_mastery_summary(self, department_mastery: Dict) -> Dict[str, Any]:
        """ç¿’ç†Ÿåº¦ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        if not department_mastery:
            return {'total_departments': 0, 'mastery_distribution': {}}
        
        mastery_counts = Counter(data['mastery_level'] for data in department_mastery.values())
        
        return {
            'total_departments': len(department_mastery),
            'mastery_distribution': dict(mastery_counts),
            'advanced_departments': [dept for dept, data in department_mastery.items() 
                                   if data['mastery_level'] == 'ä¸Šç´š'],
            'improvement_needed': [dept for dept, data in department_mastery.items() 
                                 if data['mastery_level'] == 'åˆç´š']
        }
    
    def _grade_efficiency(self, efficiency_score: float) -> str:
        """åŠ¹ç‡ã‚°ãƒ¬ãƒ¼ãƒ‰ã®åˆ¤å®š"""
        if efficiency_score >= 2.0:
            return "éå¸¸ã«é«˜åŠ¹ç‡"
        elif efficiency_score >= 1.5:
            return "é«˜åŠ¹ç‡"
        elif efficiency_score >= 1.0:
            return "æ¨™æº–"
        elif efficiency_score >= 0.5:
            return "è¦æ”¹å–„"
        else:
            return "ä½åŠ¹ç‡"
    
    def _get_time_management_advice(self, avg_time: float) -> str:
        """æ™‚é–“ç®¡ç†ã‚¢ãƒ‰ãƒã‚¤ã‚¹"""
        if avg_time < 30:
            return "å›ç­”ãŒé€Ÿã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã˜ã£ãã‚Šè€ƒãˆã¦è§£ç­”ã—ã¾ã—ã‚‡ã†ã€‚"
        elif avg_time < 60:
            return "é©åˆ‡ãªå›ç­”æ™‚é–“ã§ã™ã€‚ã“ã®èª¿å­ã‚’ç¶­æŒã—ã¾ã—ã‚‡ã†ã€‚"
        elif avg_time < 90:
            return "ã‚„ã‚„æ™‚é–“ãŒã‹ã‹ã£ã¦ã„ã¾ã™ã€‚åŠ¹ç‡çš„ãªè§£æ³•ã‚’èº«ã«ã¤ã‘ã¾ã—ã‚‡ã†ã€‚"
        else:
            return "æ™‚é–“ãŒã‹ã‹ã‚Šã™ãã¦ã„ã¾ã™ã€‚åŸºç¤çŸ¥è­˜ã®å®šç€ã‚’å›³ã‚Šã¾ã—ã‚‡ã†ã€‚"
    
    def _get_performance_summary(self, accuracy: float) -> str:
        """æˆç¸¾ã‚µãƒãƒªãƒ¼"""
        if accuracy >= 0.85:
            return "å„ªç§€ãªæˆç¸¾ã§ã™ã€‚"
        elif accuracy >= 0.7:
            return "è‰¯å¥½ãªæˆç¸¾ã§ã™ã€‚"
        elif accuracy >= 0.55:
            return "æ¨™æº–çš„ãªæˆç¸¾ã§ã™ã€‚"
        else:
            return "æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
department_statistics = DepartmentStatisticsAnalyzer()