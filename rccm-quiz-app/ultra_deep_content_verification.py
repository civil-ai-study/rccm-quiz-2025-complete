#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¥ ULTRA DEEP CONTENT VERIFICATION
ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®æ··åœ¨å•é¡Œã‚’æœ¬ç•ªç’°å¢ƒã§å®Ÿéš›ã®å•é¡Œå†…å®¹ã‚’è©³ç´°åˆ†æã—ã¦100%ç¢ºèª

é‡è¦å•é¡Œã®å®Œå…¨è§£æ±ºç¢ºèª:
1. 4-1åŸºç¤ç§‘ç›®ã¨4-2å°‚é–€ç§‘ç›®ã®æ··åœ¨
2. å¹´åº¦ã®æ··åœ¨  
3. ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®æ··åœ¨
4. å»ºè¨­ç’°å¢ƒéƒ¨é–€ã§ã®é‹¼æ§‹é€ ãƒ»ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆå•é¡Œæ··å…¥

æœ¬ç•ªç’°å¢ƒ: https://rccm-quiz-2025.onrender.com
"""

import requests
import json
import time
from datetime import datetime
import re
import urllib.parse
from typing import Dict, List, Optional, Tuple
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class UltraDeepContentVerification:
    def __init__(self):
        self.base_url = "https://rccm-quiz-2025.onrender.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        })
        
        # é‡ç‚¹æ¤œè¨¼å¯¾è±¡ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šå•é¡Œã®ç„¦ç‚¹ï¼‰
        self.critical_departments = [
            'å»ºè¨­ç’°å¢ƒ',  # ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®å•é¡Œéƒ¨é–€
            'é‹¼æ§‹é€ ãƒ»ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ',  # æ··å…¥ãŒå ±å‘Šã•ã‚ŒãŸéƒ¨é–€
            'é“è·¯', 'æ²³å·ãƒ»ç ‚é˜²', 'éƒ½å¸‚è¨ˆç”»', 'é€ åœ’', 
            'åœŸè³ªãƒ»åŸºç¤', 'æ–½å·¥è¨ˆç”»', 'ä¸Šä¸‹æ°´é“', 
            'æ£®æ—åœŸæœ¨', 'è¾²æ¥­åœŸæœ¨', 'ãƒˆãƒ³ãƒãƒ«'
        ]
        
        # é‡ç‚¹æ¤œè¨¼å¹´åº¦
        self.critical_years = [2019, 2018, 2017, 2016, 2015, 2014, 2013, 2012, 2011, 2010, 2009, 2008]
        self.question_counts = [10, 20, 30]
        
        # è©³ç´°åˆ†æçµæœ
        self.detailed_results = {
            'timestamp': datetime.now().isoformat(),
            'verification_type': 'ULTRA_DEEP_CONTENT_VERIFICATION',
            'target_url': self.base_url,
            'mixing_issues_found': [],
            'content_analysis': {},
            'category_violations': {},
            'year_violations': {},
            'basic_specialist_mixing': {},
            'critical_findings': []
        }

    def extract_question_content_deep(self, html_content: str) -> Dict:
        """å®Ÿéš›ã®å•é¡Œå†…å®¹ã®è¶…è©³ç´°æŠ½å‡ºã¨åˆ†æ"""
        try:
            content_analysis = {
                'raw_html_length': len(html_content),
                'questions_extracted': [],
                'problem_types_detected': [],
                'year_indicators': [],
                'category_keywords': [],
                'basic_subject_indicators': [],
                'specialist_subject_indicators': [],
                'detailed_content_blocks': []
            }
            
            # HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ç´”ç²‹ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            text_content = re.sub(r'<[^>]+>', ' ', html_content)
            text_content = re.sub(r'\s+', ' ', text_content).strip()
            
            content_analysis['detailed_content_blocks'].append({
                'method': 'full_text_extraction',
                'content': text_content[:1000],  # æœ€åˆã®1000æ–‡å­—
                'length': len(text_content)
            })
            
            # å…¨æ–‡ã‹ã‚‰ã®å•é¡Œæ–‡æŠ½å‡º
            full_text = text_content
            
            # å•é¡Œç•ªå·ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            question_number_patterns = [
                r'å•é¡Œ\s*(\d+)\.?\s*(.*?)(?=å•é¡Œ\s*\d+|$)',
                r'å•\s*(\d+)\.?\s*(.*?)(?=å•\s*\d+|$)', 
                r'Question\s*(\d+)\.?\s*(.*?)(?=Question\s*\d+|$)',
                r'(\d+)\.?\s*([^0-9]+.*?)(?=\d+\.|$)'
            ]
            
            for pattern in question_number_patterns:
                matches = re.findall(pattern, full_text, re.DOTALL | re.IGNORECASE)
                for match in matches:
                    if len(match) >= 2 and len(match[1].strip()) > 20:
                        content_analysis['questions_extracted'].append({
                            'number': match[0],
                            'content': match[1].strip()[:300],
                            'pattern': pattern
                        })
            
            # å¹´åº¦æŒ‡æ¨™ã®æ¤œå‡º
            year_patterns = [
                r'20(\d{2})å¹´',
                r'å¹³æˆ(\d{1,2})å¹´',
                r'ä»¤å’Œ(\d{1,2})å¹´',
                r'H(\d{1,2})',
                r'R(\d{1,2})'
            ]
            
            for pattern in year_patterns:
                matches = re.findall(pattern, full_text)
                content_analysis['year_indicators'].extend(matches)
            
            # 4-1åŸºç¤ç§‘ç›®ã®æŒ‡æ¨™æ¤œå‡º
            basic_subject_patterns = [
                r'åŸºç¤ç§‘ç›®',
                r'4-1',
                r'å››-ä¸€',
                r'æ•°å­¦',
                r'ç‰©ç†',
                r'åŒ–å­¦',
                r'åŠ›å­¦ã®åŸºç¤',
                r'ææ–™åŠ›å­¦',
                r'æ§‹é€ åŠ›å­¦',
                r'æ°´ç†å­¦',
                r'åœŸè³ªåŠ›å­¦',
                r'æ¸¬é‡',
                r'æƒ…å ±æŠ€è¡“'
            ]
            
            for pattern in basic_subject_patterns:
                if re.search(pattern, full_text, re.IGNORECASE):
                    content_analysis['basic_subject_indicators'].append(pattern)
            
            # 4-2å°‚é–€ç§‘ç›®ã®æŒ‡æ¨™æ¤œå‡º  
            specialist_patterns = [
                r'å°‚é–€ç§‘ç›®',
                r'4-2',
                r'å››-äºŒ'
            ]
            
            for pattern in specialist_patterns:
                if re.search(pattern, full_text, re.IGNORECASE):
                    content_analysis['specialist_subject_indicators'].append(pattern)
            
            return content_analysis
            
        except Exception as e:
            logger.error(f"å•é¡Œå†…å®¹æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {'error': str(e)}

    def analyze_category_mixing(self, content: str, expected_department: str) -> Dict:
        """ã‚«ãƒ†ã‚´ãƒªãƒ¼æ··åœ¨ã®è©³ç´°åˆ†æ"""
        
        # éƒ¨é–€åˆ¥å°‚é–€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚ˆã‚Šè©³ç´°ï¼‰
        department_keywords = {
            'å»ºè¨­ç’°å¢ƒ': {
                'primary': ['ç’°å¢ƒ', 'é¨’éŸ³', 'æŒ¯å‹•', 'å¤§æ°—æ±šæŸ“', 'æ°´è³ªæ±šæ¿', 'åœŸå£Œæ±šæŸ“', 'ã‚¢ã‚»ã‚¹ãƒ¡ãƒ³ãƒˆ'],
                'secondary': ['CO2', 'NOx', 'SOx', 'PM2.5', 'ç’°å¢ƒåŸºæº–', 'ç’°å¢ƒå½±éŸ¿è©•ä¾¡', 'EIA'],
                'technical': ['ãƒ‡ã‚·ãƒ™ãƒ«', 'dB', 'ppm', 'mg/L', 'Î¼g/mÂ³', 'ç’°å¢ƒåŸºæœ¬æ³•']
            },
            'é‹¼æ§‹é€ ãƒ»ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ': {
                'primary': ['é‹¼æ§‹é€ ', 'ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ', 'é‰„ç­‹', 'é‰„éª¨', 'PC', 'RC', 'SRC'],
                'secondary': ['åœ§ç¸®å¼·åº¦', 'å¼•å¼µå¼·åº¦', 'ãƒ¤ãƒ³ã‚°ç‡', 'å¼¾æ€§ä¿‚æ•°', 'ãƒã‚¢ã‚½ãƒ³æ¯”'],
                'technical': ['MPa', 'N/mmÂ²', 'kN/mÂ²', 'Ïƒ', 'Ï„', 'Îµ']
            },
            'é“è·¯': {
                'primary': ['é“è·¯', 'èˆ—è£…', 'äº¤é€š', 'ã‚¢ã‚¹ãƒ•ã‚¡ãƒ«ãƒˆ', 'è»Šé“', 'æ­©é“'],
                'secondary': ['äº¤å·®ç‚¹', 'ä¿¡å·', 'æ¨™è­˜', 'æ¨ªæ–­æ­©é“', 'ä¸­å¤®åˆ†é›¢å¸¯'],
                'technical': ['CBR', 'äº¤é€šé‡', 'è»¸é‡', 'TA', 'CBRè©¦é¨“']
            },
            'æ²³å·ãƒ»ç ‚é˜²': {
                'primary': ['æ²³å·', 'ç ‚é˜²', 'æ²»æ°´', 'å ¤é˜²', 'è­·å²¸', 'æµåŸŸ'],
                'secondary': ['æ´ªæ°´', 'ãƒ€ãƒ ', 'å °', 'æ°´é–€', 'æ¨‹é–€', 'æ’æ°´æ©Ÿå ´'],
                'technical': ['æµé‡', 'æµé€Ÿ', 'æ°´ä½', 'HWL', 'LWL', 'mÂ³/s']
            },
            'éƒ½å¸‚è¨ˆç”»': {
                'primary': ['éƒ½å¸‚è¨ˆç”»', 'å¸‚è¡—åœ°', 'åŒºåŸŸ', 'åŒºç”»', 'åœŸåœ°åˆ©ç”¨'],
                'secondary': ['ã‚¾ãƒ¼ãƒ‹ãƒ³ã‚°', 'ç”¨é€”åœ°åŸŸ', 'å»ºãºã„ç‡', 'å®¹ç©ç‡'],
                'technical': ['éƒ½å¸‚è¨ˆç”»æ³•', 'å»ºç¯‰åŸºæº–æ³•', 'é–‹ç™ºè¡Œç‚º']
            },
            'é€ åœ’': {
                'primary': ['é€ åœ’', 'ç·‘åœ°', 'å…¬åœ’', 'æ¤æ ½', 'æ¨¹æœ¨', 'åº­åœ’'],
                'secondary': ['æ™¯è¦³', 'ç·‘åŒ–', 'èŠç”Ÿ', 'èŠ±å£‡', 'éŠå…·'],
                'technical': ['æ¤ç”Ÿ', 'æ¨¹ç¨®', 'å‰ªå®š', 'æ–½è‚¥', 'ç—…è™«å®³']
            },
            'åœŸè³ªãƒ»åŸºç¤': {
                'primary': ['åœŸè³ª', 'åŸºç¤', 'åœ°ç›¤', 'æ”¯æŒåŠ›', 'Nå€¤', 'ã›ã‚“æ–­'],
                'secondary': ['åœ§å¯†', 'æ¶²çŠ¶åŒ–', 'æ²ˆä¸‹', 'æ­åŸºç¤', 'ç›´æ¥åŸºç¤'],
                'technical': ['kN/mÂ²', 'kPa', 'Ï†', 'c', 'SPT', 'CPT']
            },
            'æ–½å·¥è¨ˆç”»': {
                'primary': ['æ–½å·¥', 'å·¥ç¨‹', 'ç®¡ç†', 'å“è³ªç®¡ç†', 'å®‰å…¨ç®¡ç†'],
                'secondary': ['å·¥äº‹', 'æ–½å·¥æ³•', 'æ©Ÿæ¢°', 'ä»®è¨­', 'è¶³å ´'],
                'technical': ['å·¥ç¨‹è¡¨', 'PERT', 'CPM', 'ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ']
            },
            'ä¸Šä¸‹æ°´é“': {
                'primary': ['ä¸Šæ°´é“', 'ä¸‹æ°´é“', 'çµ¦æ°´', 'æ’æ°´', 'æµ„æ°´'],
                'secondary': ['é…æ°´', 'é€æ°´', 'å–æ°´', 'æµ„åŒ–', 'å‡¦ç†'],
                'technical': ['BOD', 'COD', 'SS', 'DO', 'pH', 'mg/L']
            },
            'æ£®æ—åœŸæœ¨': {
                'primary': ['æ£®æ—', 'æ—é“', 'æ²»å±±', 'æœ¨æ', 'é–“ä¼'],
                'secondary': ['é€ æ—', 'è‚²æ—', 'ä¼æ¡', 'æ¬å‡º', 'æœ¨æ©‹'],
                'technical': ['ç«‹æœ¨', 'mÂ³', 'æç©', 'è“„ç©', 'æˆé•·é‡']
            },
            'è¾²æ¥­åœŸæœ¨': {
                'primary': ['è¾²æ¥­', 'çŒæ¼‘', 'è¾²åœ°', 'æ°´åˆ©', 'æ’æ°´è·¯'],
                'secondary': ['åœƒå ´', 'ç”¨æ°´', 'ç”°ç•‘', 'è¾²é“', 'æš—æ¸ '],
                'technical': ['å–æ°´é‡', 'ç”¨æ°´é‡', 'L/s', 'mm/day', 'æœ‰åŠ¹é›¨é‡']
            },
            'ãƒˆãƒ³ãƒãƒ«': {
                'primary': ['ãƒˆãƒ³ãƒãƒ«', 'æ˜å‰Š', 'æ”¯ä¿', 'è¦†å·¥', 'NATM'],
                'secondary': ['ã‚·ãƒ¼ãƒ«ãƒ‰', 'å±±å²³å·¥æ³•', 'TBM', 'ãƒ­ãƒƒã‚¯ãƒœãƒ«ãƒˆ'],
                'technical': ['åœ°å±±', 'åœŸè¢«ã‚Š', 'å†…ç©º', 'å‘å£', 'kPa']
            }
        }
        
        mixing_analysis = {
            'expected_department': expected_department,
            'found_departments': [],
            'mixing_violations': [],
            'keyword_matches': {
                'expected': [],
                'unexpected': []
            },
            'confidence_score': 0.0
        }
        
        content_lower = content.lower()
        
        # æœŸå¾…ã•ã‚Œã‚‹éƒ¨é–€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        expected_keywords = department_keywords.get(expected_department, {})
        expected_matches = 0
        
        for category, keywords in expected_keywords.items():
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    mixing_analysis['keyword_matches']['expected'].append({
                        'keyword': keyword,
                        'category': category,
                        'department': expected_department
                    })
                    expected_matches += 1
        
        # ä»–éƒ¨é–€ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆæ··åœ¨æ¤œå‡ºï¼‰
        unexpected_matches = 0
        for dept_name, dept_keywords in department_keywords.items():
            if dept_name != expected_department:
                for category, keywords in dept_keywords.items():
                    for keyword in keywords:
                        if keyword.lower() in content_lower:
                            mixing_analysis['keyword_matches']['unexpected'].append({
                                'keyword': keyword,
                                'category': category,
                                'department': dept_name
                            })
                            unexpected_matches += 1
                            
                            if dept_name not in mixing_analysis['found_departments']:
                                mixing_analysis['found_departments'].append(dept_name)
        
        # æ··åœ¨é•åã®åˆ¤å®š
        if unexpected_matches > 0:
            for dept in mixing_analysis['found_departments']:
                mixing_analysis['mixing_violations'].append({
                    'violation_type': 'CATEGORY_MIXING',
                    'expected_department': expected_department,
                    'found_department': dept,
                    'severity': 'HIGH' if dept in ['é‹¼æ§‹é€ ãƒ»ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ', 'å»ºè¨­ç’°å¢ƒ'] else 'MEDIUM'
                })
        
        # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        total_matches = expected_matches + unexpected_matches
        if total_matches > 0:
            mixing_analysis['confidence_score'] = expected_matches / total_matches
        else:
            mixing_analysis['confidence_score'] = 0.0
        
        return mixing_analysis

    def verify_single_exam_session(self, department: str, year: int, question_count: int) -> Dict:
        """å˜ä¸€è©¦é¨“ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è©³ç´°æ¤œè¨¼"""
        
        print(f"  ğŸ” è©³ç´°æ¤œè¨¼: {department} {year}å¹´åº¦ {question_count}å•")
        
        session_result = {
            'department': department,
            'year': year,
            'question_count': question_count,
            'timestamp': datetime.now().isoformat(),
            'connection_successful': False,
            'content_extracted': False,
            'mixing_analysis': {},
            'violations_found': [],
            'raw_content_sample': '',
            'detailed_findings': []
        }
        
        try:
            # è©¦é¨“é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            data = {
                'questions': str(question_count),
                'year': str(year),
                'category': department
            }
            
            url = f"{self.base_url}/start_exam/specialist"
            
            print(f"    ğŸ“¡ ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {url}")
            print(f"    ğŸ“Š ãƒ‡ãƒ¼ã‚¿: {data}")
            
            response = self.session.post(url, data=data, timeout=60, allow_redirects=True)
            
            if response.status_code == 200:
                session_result['connection_successful'] = True
                html_content = response.text
                session_result['raw_content_sample'] = html_content[:1000]  # æœ€åˆã®1000æ–‡å­—
                
                print(f"    âœ… æ¥ç¶šæˆåŠŸ (HTMLé•·: {len(html_content)}æ–‡å­—)")
                
                # è©³ç´°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º
                content_analysis = self.extract_question_content_deep(html_content)
                session_result['content_analysis'] = content_analysis
                
                if content_analysis.get('questions_extracted') or content_analysis.get('detailed_content_blocks'):
                    session_result['content_extracted'] = True
                    
                    # ã‚«ãƒ†ã‚´ãƒªãƒ¼æ··åœ¨åˆ†æ
                    full_content = ' '.join([
                        block.get('content', '') for block in content_analysis.get('detailed_content_blocks', [])
                    ])
                    
                    if full_content:
                        mixing_analysis = self.analyze_category_mixing(full_content, department)
                        session_result['mixing_analysis'] = mixing_analysis
                        
                        # é•åæ¤œå‡º
                        if mixing_analysis['mixing_violations']:
                            session_result['violations_found'] = mixing_analysis['mixing_violations']
                            print(f"    ğŸš¨ ã‚«ãƒ†ã‚´ãƒªãƒ¼æ··åœ¨æ¤œå‡º: {len(mixing_analysis['mixing_violations'])}ä»¶")
                            
                            # é‡è¦ãªé•åã‚’è¨˜éŒ²
                            for violation in mixing_analysis['mixing_violations']:
                                if violation['expected_department'] == 'å»ºè¨­ç’°å¢ƒ' and violation['found_department'] == 'é‹¼æ§‹é€ ãƒ»ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ':
                                    session_result['detailed_findings'].append({
                                        'finding_type': 'CRITICAL_USER_REPORTED_ISSUE',
                                        'description': f"å»ºè¨­ç’°å¢ƒéƒ¨é–€ã§é‹¼æ§‹é€ ãƒ»ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆå•é¡Œæ··å…¥æ¤œå‡º",
                                        'evidence': mixing_analysis['keyword_matches']['unexpected']
                                    })
                        else:
                            print(f"    âœ… ã‚«ãƒ†ã‚´ãƒªãƒ¼æ··åœ¨ãªã— (ä¿¡é ¼åº¦: {mixing_analysis['confidence_score']:.2f})")
                    
                    # 4-1/4-2æ··åœ¨ãƒã‚§ãƒƒã‚¯
                    basic_indicators = content_analysis.get('basic_subject_indicators', [])
                    specialist_indicators = content_analysis.get('specialist_subject_indicators', [])
                    
                    if basic_indicators and specialist_indicators:
                        session_result['violations_found'].append({
                            'violation_type': 'BASIC_SPECIALIST_MIXING',
                            'basic_indicators': basic_indicators,
                            'specialist_indicators': specialist_indicators,
                            'severity': 'CRITICAL'
                        })
                        print(f"    ğŸš¨ 4-1/4-2æ··åœ¨æ¤œå‡º")
                    
                    # å¹´åº¦æ··åœ¨ãƒã‚§ãƒƒã‚¯  
                    year_indicators = content_analysis.get('year_indicators', [])
                    if year_indicators:
                        expected_year_str = str(year)[-2:]  # ä¸‹2æ¡
                        unexpected_years = [y for y in year_indicators if y != expected_year_str]
                        if unexpected_years:
                            session_result['violations_found'].append({
                                'violation_type': 'YEAR_MIXING',
                                'expected_year': year,
                                'found_years': unexpected_years,
                                'severity': 'HIGH'
                            })
                            print(f"    ğŸš¨ å¹´åº¦æ··åœ¨æ¤œå‡º: {unexpected_years}")
                else:
                    print(f"    âš ï¸ å•é¡Œå†…å®¹æŠ½å‡ºå¤±æ•—")
            else:
                print(f"    âŒ æ¥ç¶šå¤±æ•—: HTTP {response.status_code}")
                session_result['error'] = f"HTTP {response.status_code}"
                
        except Exception as e:
            print(f"    ğŸ’¥ ä¾‹å¤–ç™ºç”Ÿ: {str(e)}")
            session_result['error'] = str(e)
        
        return session_result

    def run_comprehensive_mixing_verification(self):
        """åŒ…æ‹¬çš„æ··åœ¨å•é¡Œæ¤œè¨¼ã®å®Ÿè¡Œ"""
        print("ğŸ”¥ ULTRA DEEP CONTENT VERIFICATION é–‹å§‹")
        print("=" * 80)
        print("ç›®çš„: ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®æ··åœ¨å•é¡Œã‚’æœ¬ç•ªç’°å¢ƒã§å®Ÿéš›ç¢ºèª")
        print("ç„¦ç‚¹: 4-1/4-2æ··åœ¨ã€å¹´åº¦æ··åœ¨ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼æ··åœ¨ã€å»ºè¨­ç’°å¢ƒÃ—é‹¼æ§‹é€ æ··å…¥")
        print("=" * 80)
        
        start_time = time.time()
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–
        print("ğŸ”„ æœ¬ç•ªç’°å¢ƒã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–...")
        try:
            init_response = self.session.get(self.base_url, timeout=30)
            if init_response.status_code == 200:
                print("âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–æˆåŠŸ")
            else:
                print(f"âŒ ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–å¤±æ•—: HTTP {init_response.status_code}")
                return None
        except Exception as e:
            print(f"ğŸ’¥ ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None
        
        # é‡ç‚¹æ¤œè¨¼å®Ÿè¡Œ
        total_tests = 0
        critical_violations = []
        department_results = {}
        
        for department in self.critical_departments:
            print(f"\nğŸ¢ ã€{department}éƒ¨é–€ã€‘è©³ç´°æ··åœ¨æ¤œè¨¼")
            print("-" * 50)
            
            department_results[department] = {
                'tests_conducted': 0,
                'violations_found': 0,
                'critical_issues': [],
                'session_results': []
            }
            
            # å„å¹´åº¦ãƒ»å•é¡Œæ•°ã§ã®æ¤œè¨¼
            for year in self.critical_years[:3]:  # æœ€æ–°3å¹´åˆ†ã‚’é‡ç‚¹çš„ã«
                for question_count in self.question_counts:
                    total_tests += 1
                    department_results[department]['tests_conducted'] += 1
                    
                    session_result = self.verify_single_exam_session(department, year, question_count)
                    department_results[department]['session_results'].append(session_result)
                    
                    # é•åãƒã‚§ãƒƒã‚¯
                    if session_result.get('violations_found'):
                        department_results[department]['violations_found'] += len(session_result['violations_found'])
                        
                        for violation in session_result['violations_found']:
                            if violation.get('severity') == 'CRITICAL':
                                critical_violations.append({
                                    'department': department,
                                    'year': year,
                                    'question_count': question_count,
                                    'violation': violation
                                })
                                department_results[department]['critical_issues'].append(violation)
                    
                    # ç‰¹åˆ¥ãªç„¦ç‚¹: å»ºè¨­ç’°å¢ƒÃ—é‹¼æ§‹é€ æ··å…¥
                    if department == 'å»ºè¨­ç’°å¢ƒ':
                        detailed_findings = session_result.get('detailed_findings', [])
                        for finding in detailed_findings:
                            if finding.get('finding_type') == 'CRITICAL_USER_REPORTED_ISSUE':
                                critical_violations.append({
                                    'department': department,
                                    'year': year,
                                    'question_count': question_count,
                                    'finding': finding
                                })
                    
                    time.sleep(0.5)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            
            # éƒ¨é–€åˆ¥çµæœã‚µãƒãƒªãƒ¼
            violation_rate = (department_results[department]['violations_found'] / 
                            department_results[department]['tests_conducted']) * 100 if department_results[department]['tests_conducted'] > 0 else 0
            
            print(f"  ğŸ“Š {department}çµæœ:")
            print(f"    å®Ÿæ–½ãƒ†ã‚¹ãƒˆ: {department_results[department]['tests_conducted']}")
            print(f"    é•åæ¤œå‡º: {department_results[department]['violations_found']}")
            print(f"    é•åç‡: {violation_rate:.1f}%")
            print(f"    é‡è¦å•é¡Œ: {len(department_results[department]['critical_issues'])}")
        
        end_time = time.time()
        duration = end_time - start_time
        
        # æœ€çµ‚çµæœã¨åˆ¤å®š
        self.detailed_results.update({
            'total_tests_conducted': total_tests,
            'total_violations': sum(dept['violations_found'] for dept in department_results.values()),
            'critical_violations': critical_violations,
            'department_results': department_results,
            'verification_duration': duration
        })
        
        self.generate_mixing_verification_report()
        
        return self.detailed_results

    def generate_mixing_verification_report(self):
        """æ··åœ¨å•é¡Œæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "=" * 80)
        print("ğŸ”¥ ULTRA DEEP CONTENT VERIFICATION çµæœ")
        print("=" * 80)
        
        total_violations = self.detailed_results['total_violations']
        critical_violations = len(self.detailed_results['critical_violations'])
        
        print(f"ğŸ“Š ç·åˆçµæœ:")
        print(f"  å®Ÿæ–½ãƒ†ã‚¹ãƒˆç·æ•°: {self.detailed_results['total_tests_conducted']}")
        print(f"  æ¤œå‡ºé•åç·æ•°: {total_violations}")
        print(f"  é‡è¦é•åæ•°: {critical_violations}")
        print(f"  å®Ÿè¡Œæ™‚é–“: {self.detailed_results['verification_duration']:.1f}ç§’")
        print()
        
        # é‡è¦é•åã®è©³ç´°
        if critical_violations > 0:
            print("ğŸš¨ é‡è¦é•åè©³ç´°:")
            for i, violation in enumerate(self.detailed_results['critical_violations'][:5], 1):
                print(f"  {i}. {violation['department']} {violation['year']}å¹´åº¦ {violation['question_count']}å•")
                if 'violation' in violation:
                    print(f"     ã‚¿ã‚¤ãƒ—: {violation['violation']['violation_type']}")
                    print(f"     é‡è¦åº¦: {violation['violation']['severity']}")
                if 'finding' in violation:
                    print(f"     ç™ºè¦‹: {violation['finding']['description']}")
            print()
        
        # éƒ¨é–€åˆ¥çµæœ
        print("ğŸ¢ éƒ¨é–€åˆ¥çµæœ:")
        for dept, results in self.detailed_results['department_results'].items():
            violation_rate = (results['violations_found'] / results['tests_conducted']) * 100 if results['tests_conducted'] > 0 else 0
            status = "ğŸš¨" if results['critical_issues'] else "âš ï¸" if results['violations_found'] > 0 else "âœ…"
            print(f"  {status} {dept}: {violation_rate:.1f}% é•åç‡ ({results['violations_found']}/{results['tests_conducted']})")
        
        print()
        
        # æœ€çµ‚åˆ¤å®š
        if critical_violations == 0 and total_violations == 0:
            print("ğŸ† åˆ¤å®š: PERFECT - æ··åœ¨å•é¡Œã¯å®Œå…¨ã«è§£æ±ºæ¸ˆã¿")
            credibility_status = "PERFECT"
        elif critical_violations == 0 and total_violations <= 3:
            print("âœ… åˆ¤å®š: GOOD - è»½å¾®ãªå•é¡Œã®ã¿æ¤œå‡º")
            credibility_status = "GOOD"
        elif critical_violations <= 2:
            print("âš ï¸ åˆ¤å®š: NEEDS ATTENTION - é‡è¦å•é¡Œã‚ã‚Š")
            credibility_status = "NEEDS_ATTENTION"
        else:
            print("ğŸš¨ åˆ¤å®š: CRITICAL - æ·±åˆ»ãªæ··åœ¨å•é¡Œã‚ã‚Š")
            credibility_status = "CRITICAL"
        
        # ç‰¹åˆ¥å ±å‘Š: ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šå•é¡Œ
        user_reported_issues = [v for v in self.detailed_results['critical_violations'] 
                              if 'finding' in v and v['finding'].get('finding_type') == 'CRITICAL_USER_REPORTED_ISSUE']
        
        if user_reported_issues:
            print(f"\nğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šå•é¡Œã®çŠ¶æ³:")
            print(f"  å»ºè¨­ç’°å¢ƒÃ—é‹¼æ§‹é€ æ··å…¥: {len(user_reported_issues)}ä»¶æ¤œå‡º")
            print("  âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®å•é¡ŒãŒæœ¬ç•ªç’°å¢ƒã§ç¢ºèªã•ã‚Œã¾ã—ãŸ")
        else:
            print(f"\nğŸ¯ ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šå•é¡Œã®çŠ¶æ³:")
            print("  å»ºè¨­ç’°å¢ƒÃ—é‹¼æ§‹é€ æ··å…¥: 0ä»¶")
            print("  âœ… ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šã®å•é¡Œã¯æœ¬ç•ªç’°å¢ƒã§ç¢ºèªã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_filename = f"ultra_deep_content_verification_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(self.detailed_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_filename}")
        print("\nğŸ”’ ULTRA DEEP CONTENT VERIFICATION å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ”¥ ULTRA DEEP CONTENT VERIFICATION")
    print("æœ¬ç•ªç’°å¢ƒã§ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å ±å‘Šæ··åœ¨å•é¡Œã®å®Œå…¨ç¢ºèª")
    print()
    
    verifier = UltraDeepContentVerification()
    results = verifier.run_comprehensive_mixing_verification()
    
    return results

if __name__ == "__main__":
    main()